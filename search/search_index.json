{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":[" "]},"docs":[{"location":"","title":"\u015arodowiska uruchomieniowe AutoML - PJATK 2025L","text":"<p>Repozytorium zawiera materia\u0142y dotycz\u0105ce realizacji przedmiotu SUML na PJATK - semestr letni 2025.</p>"},{"location":"#tematyka-zajec","title":"Tematyka zaj\u0119\u0107","text":"<p>Celem kursu jest zdobycie praktycznych umiej\u0119tno\u015bci w zakresie budowy, wdra\u017cania i zarz\u0105dzania aplikacjami wykorzystuj\u0105cymi uczenie maszynowe (ML) i generatywn\u0105 sztuczn\u0105 inteligencj\u0119 (GenAI). Studenci naucz\u0105 si\u0119 konfigurowa\u0107 \u015brodowiska programistyczne oraz integrowa\u0107 modele ML w aplikacjach webowych. Kluczowe aspekty kursu obejmuj\u0105:</p> <ul> <li>Pozyskiwanie i przygotowanie danych do treningu modeli ML,</li> <li>Trenowanie, ewaluacj\u0119 i wdra\u017canie modeli,</li> <li>Tworzenie interfejs\u00f3w u\u017cytkownika do interakcji z modelami,</li> <li>Uruchamianie aplikacji w \u015brodowisku chmurowym,</li> <li>Automatyzacj\u0119 proces\u00f3w wdra\u017cania (CI/CD),</li> <li>Korzystanie z AutoML i gotowych modeli ML/GenAI (np. Hugging Face, OpenAI).</li> </ul>"},{"location":"#organizacja-zajec","title":"Organizacja zaj\u0119\u0107","text":"<p>Studenci pracuj\u0105 w zespo\u0142ach dwu- lub trzyosobowych. Celem projektu jest stworzenie aplikacji dzia\u0142aj\u0105cej w \u015brodowisku chmurowym, zgodnie z dobrymi praktykami in\u017cynierii oprogramowania. Wymagania dotycz\u0105ce aplikacji:</p> <ul> <li>Powinna posiada\u0107 prosty interfejs u\u017cytkownika,</li> <li>Wykorzystywa\u0107 model ML lub GenAI w praktyczny spos\u00f3b.</li> </ul> <p>Przyk\u0142ady projekt\u00f3w:</p> <ol> <li>W\u0142asny model ML wytrenowany na publicznie dost\u0119pnych danych (np. z platformy Kaggle) \u2013 aplikacja zbiera dane wej\u015bciowe i dokonuje predykcji.</li> <li>Wykorzystanie gotowego modelu z Hugging Face \u2013 aplikacja integruje model (np. analiza sentymentu, t\u0142umaczenie, klasyfikacja obraz\u00f3w, generowanie tre\u015bci) i udost\u0119pnia go u\u017cytkownikom.</li> </ol> <p>W trakcie zaj\u0119\u0107 b\u0119d\u0105 przedstawiane nowe technologie wraz z prostymi przyk\u0142adami. W trakcie zaj\u0119\u0107 b\u0119d\u0105 do rozwi\u0105zania przez student\u00f3w zadania wykorzystuj\u0105ce przedstawian\u0105 technologi\u0119.</p>"},{"location":"#terminy-zajec-i-zadania","title":"Terminy zaj\u0119\u0107 i zadania","text":"Termin Numer zaj\u0119\u0107 \u0106wiczenia Zakres zaj\u0119\u0107 Checkpoint projektu 08.03. 1 Wprowadzenie, backlog, user stories - Om\u00f3wienie formatu zaj\u0119\u0107, podzia\u0142 na zespo\u0142y, wyb\u00f3r tematu projektu  - Wprowadzenie do backlogu i user stories (np. w Azure DevOps) - 22.03. 2 \u015arodowisko programistyczne - Prioryteryzacja backlogu i sprinty  - Przygotowanie \u015brodowiska programistycznego w Pythonie: pyenv, conda  - Tworzenie pierwszej aplikacji w Streamlit bazuj\u0105cej na modelu z Hugging Face Temat, backlog i repozytorium kodu dla projektu 05.04. 3 Interfejs u\u017cytkownika dla aplikacji ML - Trenowanie w\u0142asnej sieci neuronowej  - Tworzenie aplikacji w Gradio na podstawie w\u0142asnego modelu klasyfikacji obraz\u00f3w Dzia\u0142aj\u0105ca pierwsza wersja aplikacji (Streamlit/Gradio) 26.04. 4 Docker - Wprowadzenie do Dockera: pisanie Dockerfile, uruchamianie aplikacji w kontenerze Pod\u0142\u0105czenie modelu AI do aplikacji 17.05. 5 CI/CD - Wprowadzenie do CI/CD  - Konfiguracja GitHub Actions Konteneryzcja aplikacji (docker) 31.05. 6 Uruchamianie aplikacji w chmurze - Uruchamianie przyk\u0142adowej aplikacji (kontenera) na Azure Stworzenie pipeline'\u00f3w CI/CD 14.06. 7 Narz\u0119dzia ML - Wprowadzenie do MLflow  - Wprowadzenie do AutoML w Pythonie  - Wprowadzenie do kwantyzacji sieci neuronowych  - Uruchamianie modeli LLM z ollama Aplikacja dzia\u0142aj\u0105ca w \u015brodowisku chmurowym 28.06. 8 Prezentacje i ocena - Prezentacja projekt\u00f3w, ocena i feedback Prezentacja"},{"location":"#zasady-oceniania","title":"Zasady oceniania","text":"<p>Ocena z przedmiotu sk\u0142ada si\u0119 z trzech element\u00f3w: regularnej pracy w czasie semestru, ko\u0144cowego rozwi\u0105zania oraz ko\u0144cowej prezentacji.</p> <p>W semestrze mo\u017cna zdoby\u0107 maksymalnie 100 pkt, w tym:</p> <ul> <li>Checkpointy projektu (60 pkt) \u2013 10 pkt za ka\u017cdy checkpoint.<ul> <li>Na oddaniu checkpointu powinien by\u0107 obecny ca\u0142y zesp\u00f3\u0142.</li> <li>Oddanie checkpointu nast\u0119puje tylko w trakcie zaj\u0119\u0107 (z wyj\u0105tkiem nieobecno\u015bci z powodu choroby lub jednorazowej nieobecno\u015bci, jak w zasadach dodatkowych).</li> <li>Ka\u017cdy kolejny checkpoint projektu mo\u017ce zosta\u0107 przyj\u0119ty tylko po zatwierdzeniu (oddaniu) poprzedniego.</li> </ul> </li> <li>Ko\u0144cowe rozwi\u0105zanie (20 pkt) \u2013 ocena spe\u0142nienia wymaga\u0144 aplikacji, jako\u015bci kodu (czytelno\u015b\u0107, komentarze, struktura repozytorium, poprawno\u015b\u0107 commit\u00f3w, branche, itp.).</li> <li>Ko\u0144cowa prezentacja (20 pkt) \u2013 ocena klarowno\u015bci prezentacji, przedstawienia funkcjonalno\u015bci oraz odpowiedzi na pytania z sali (przewidziane s\u0105 dodatkowe punkty za zadawanie pyta\u0144 innym zespo\u0142om). Brak ko\u0144cowej prezentacji projektu skutkuje niemo\u017cliwo\u015bci\u0105 zaliczenia projektu.</li> </ul> <p>Zasady dodatkowe:</p> <ul> <li>Dopuszczalna jest jedna nieusprawiedliwiona nieobecno\u015b\u0107 \u2013 ka\u017cda kolejna obni\u017ca ocen\u0119 o 10 pkt.</li> <li>Oddanie checkpointu odbywa si\u0119 podczas zaj\u0119\u0107 (kilka minut dla ka\u017cdego zespo\u0142u na koniec zaj\u0119\u0107).</li> <li>Op\u00f3\u017anienia w oddaniu checkpointu skutkuj\u0105 obni\u017ceniem punktacji o 5 pkt za ka\u017cdy rozpocz\u0119ty tydzie\u0144 op\u00f3\u017anienia (liczone do momentu przes\u0142ania rozwi\u0105zania poprzez Teamsy - punkty zostaj\u0105 przyznane dopiero po prezentacji checkpointu).</li> </ul> <p>Ko\u0144cowe oceny s\u0105 przyznawane na podstawie sumy zdobytych punkt\u00f3w:</p> Punkty Ocena 90 - 100 5 80 - 89 4.5 70 - 79 4 60 - 69 3.5 50 - 59 3 &lt; 50 2"},{"location":"01-user-stories/","title":"Organizacja pracy w projekcie","text":""},{"location":"01-user-stories/#backlog","title":"Backlog","text":""},{"location":"01-user-stories/#czym-jest-backlog","title":"Czym jest backlog?","text":"<p>Backlog to lista wszystkich funkcjonalno\u015bci, zada\u0144 i usprawnie\u0144, kt\u00f3re maj\u0105 zosta\u0107 wykonane w projekcie.</p> <p>W Azure DevOps backlog to po prostu zbi\u00f3r Work Items (Epic, Feature, User Story, Task), kt\u00f3re opisuj\u0105 wymagania i prace do wykonania w projekcie.</p> <p>Backlog mo\u017cna podzieli\u0107 na dwa poziomy:</p> <ul> <li>Product Backlog \u2013 wszystkie wymagania dla ca\u0142ego produktu (d\u0142ugoterminowe).</li> <li>Sprint Backlog \u2013 zestaw zada\u0144 wybranych do realizacji w jednym sprincie (kr\u00f3tkoterminowe).</li> </ul>"},{"location":"01-user-stories/#po-co-jest-backlog","title":"Po co jest backlog?","text":"<ul> <li>Organizacja pracy</li> <li>Priorytetyzacja</li> <li>\u015aledzenie post\u0119pu</li> <li>Komunikacja w zespole</li> <li>Elastyczno\u015b\u0107</li> </ul>"},{"location":"01-user-stories/#backlog-na-podstawie-azure-devops","title":"Backlog na podstawie Azure DevOps","text":"<p>Lista element\u00f3w z polami:</p> <ul> <li>Typ (Epis, Feature, User story, Task)</li> <li>Tytu\u0142</li> <li>Opis</li> <li>Kryteria akceptacji</li> <li>Status</li> <li>Priorytet</li> <li>Estymacja czasu (tzw. Story Points)</li> <li>Osoba odpowiedzialna</li> <li>Linki do innych element\u00f3w backlogu</li> <li>Linki do ga\u0142\u0119zi (branch) w repozytorium</li> <li>Komentarze/dyskusja</li> </ul>"},{"location":"01-user-stories/#typy-elementow-w-backlogu","title":"Typy element\u00f3w w backlogu","text":"<ol> <li> <p>Epic (Epik)</p> <p>Najwi\u0119kszy poziom abstrakcji, reprezentuje szeroki obszar funkcjonalny lub cel biznesowy.</p> <p>Przyk\u0142ad:</p> <p>\ud83d\udccc Tytu\u0142: Ulepszenie bezpiecze\u0144stwa aplikacji</p> <p>\ud83d\udccc Opis: W celu zwi\u0119kszenia ochrony danych u\u017cytkownik\u00f3w i zgodno\u015bci z RODO dodajemy szyfrowanie hase\u0142 oraz logowanie dwusk\u0142adnikowe.</p> <p>\ud83d\udccc Kryteria akceptacji:</p> <p>\u2705 Has\u0142a przechowywane w formie zaszyfrowanej</p> <p>\u2705 Obs\u0142uga uwierzytelniania dwuetapowego</p> <p>\u2705 Mo\u017cliwo\u015b\u0107 zmiany has\u0142a przez u\u017cytkownika</p> </li> <li> <p>Feature (Funkcja)</p> <p>\u015aredni poziom abstrakcji. Feature reprezentuje wi\u0119ksz\u0105 funkcjonalno\u015b\u0107 w ramach Epic i sk\u0142ada si\u0119 z User Stories.</p> <p>Przyk\u0142ad:</p> <p>\ud83d\udccc Tytu\u0142: Obs\u0142uga dwusk\u0142adnikowego uwierzytelniania</p> <p>\ud83d\udccc Opis: Aby zwi\u0119kszy\u0107 bezpiecze\u0144stwo u\u017cytkownik\u00f3w, dodajemy mo\u017cliwo\u015b\u0107 aktywacji logowania dwuetapowego przy u\u017cyciu kod\u00f3w SMS lub aplikacji Authenticator.</p> <p>\ud83d\udccc Kryteria akceptacji:</p> <p>\u2705 U\u017cytkownik mo\u017ce w\u0142\u0105czy\u0107 dwusk\u0142adnikowe uwierzytelnianie w ustawieniach</p> <p>\u2705 Kod weryfikacyjny jest wysy\u0142any na telefon lub generowany przez aplikacj\u0119</p> <p>\u2705 Po trzech b\u0142\u0119dnych pr\u00f3bach logowania konto jest blokowane</p> </li> <li> <p>User Story (Historia u\u017cytkownika)</p> <p>Reprezentuje konkretne wymaganie u\u017cytkownika \u2013 jest to najmniejsza jednostka backlogu zawieraj\u0105ca pojedyncz\u0105 funkcjonalno\u015b\u0107. Powinna by\u0107 na tyle ma\u0142a, aby mog\u0142a zosta\u0107 uko\u0144czona w jednym sprincie. Cz\u0119sto jej format to: Jako [kto\u015b] chc\u0119 [co\u015b], aby [co\u015b].</p> <p>Przyk\u0142ad:</p> <p>\ud83d\udccc Tytu\u0142: Jako u\u017cytkownik chc\u0119 w\u0142\u0105czy\u0107 logowanie dwuetapowe, aby zwi\u0119kszy\u0107 bezpiecze\u0144stwo mojego konta</p> <p>\ud83d\udccc Opis: Po aktywacji u\u017cytkownik musi potwierdzi\u0107 to\u017csamo\u015b\u0107 przy logowaniu, wpisuj\u0105c kod SMS lub z aplikacji Authenticator.</p> <p>\ud83d\udccc Kryteria akceptacji:</p> <p>\u2705 W ustawieniach dost\u0119pna jest opcja \u201eW\u0142\u0105cz dwusk\u0142adnikowe uwierzytelnianie\u201d</p> <p>\u2705 Po aktywacji u\u017cytkownik musi wybra\u0107 metod\u0119: SMS lub Authenticator</p> <p>\u2705 Przy ka\u017cdym logowaniu u\u017cytkownik musi poda\u0107 kod</p> </li> <li> <p>Task (Zadanie)</p> <p>Reprezentuje konkretn\u0105 prac\u0119 do wykonania w ramach User Story. Mo\u017ce to by\u0107 kodowanie, testowanie, projektowanie UI itp.</p> <p>Przyk\u0142ad:</p> <p>\ud83d\udccc Tytu\u0142: Zaimplementowa\u0107 mechanizm wysy\u0142ania kod\u00f3w SMS</p> <p>\ud83d\udccc Opis: Utworzy\u0107 API do wysy\u0142ki kod\u00f3w na telefon u\u017cytkownika przy logowaniu. Skorzysta\u0107 z zewn\u0119trznego dostawcy us\u0142ug SMS.</p> <p>\ud83d\udccc Status: W toku</p> <p>\ud83d\udccc Przypisany do: Jan Kowalski</p> </li> </ol>"},{"location":"01-user-stories/#przykadowy-backlog","title":"Przyk\u0142adowy backlog","text":"<p>Proces w Azure DevOps: <code>Agile</code></p> <p></p>"},{"location":"01-user-stories/#jak-pisac-dobre-user-story","title":"Jak pisa\u0107 dobre User Story?","text":"<p>Zasada INVEST to \u015bwietna praktyka, kt\u00f3ra pomaga w tworzeniu dobrych User Stories w backlogu.</p> <ol> <li> <p>I \u2013 Independent (Niezale\u017cna)</p> <ul> <li> <p>Historia u\u017cytkownika powinna by\u0107 mo\u017cliwie samodzielna, aby mo\u017cna by\u0142o j\u0105 realizowa\u0107 bez zale\u017cno\u015bci od innych zada\u0144.</p> </li> <li> <p>Je\u015bli jedna User Story blokuje inn\u0105, mo\u017ce to powodowa\u0107 op\u00f3\u017anienia w realizacji sprintu.</p> </li> <li> <p>\u274c Z\u0142e: \u201eU\u017cytkownik mo\u017ce zresetowa\u0107 has\u0142o\u201d (je\u015bli logowanie nie jest gotowe, nie mo\u017cna tego wdro\u017cy\u0107).</p> </li> <li> <p>\u2705 Dobre: \u201eU\u017cytkownik mo\u017ce otrzyma\u0107 e-mail z linkiem do resetu has\u0142a\u201d (to mo\u017cna wdro\u017cy\u0107 osobno).</p> </li> </ul> </li> <li> <p>N \u2013 Negotiable (Negocjowalna)</p> <ul> <li>User Story nie jest sztywn\u0105 specyfikacj\u0105, ale punktem do rozm\u00f3w mi\u0119dzy zespo\u0142em a Product Ownerem.</li> </ul> <p>Szczeg\u00f3\u0142y mog\u0105 si\u0119 zmienia\u0107 w trakcie planowania.</p> <ul> <li>\u2705 Dobre: \u201eU\u017cytkownik powinien widzie\u0107 list\u0119 swoich zada\u0144\u201d \u2013 spos\u00f3b realizacji (np. jako tabela czy lista kart) mo\u017ce by\u0107 ustalony  p\u00f3\u017aniej.</li> </ul> </li> <li> <p>V \u2013 Valuable (Warto\u015bciowa)</p> <ul> <li> <p>Ka\u017cda historia powinna przynosi\u0107 warto\u015b\u0107 dla u\u017cytkownika lub klienta.</p> </li> <li> <p>Je\u015bli User Story nie daje warto\u015bci, nie powinna znale\u017a\u0107 si\u0119 w backlogu.</p> </li> <li> <p>\u274c Z\u0142e: \u201eZmienimy kolor przycisku na niebieski\u201d \u2013 je\u015bli to nie wp\u0142ywa na UX, nie ma warto\u015bci.</p> </li> <li> <p>\u2705 Dobre: \u201eU\u017cytkownik mo\u017ce filtrowa\u0107 swoje zadania po statusie\u201d \u2013 daje realn\u0105 warto\u015b\u0107.</p> </li> </ul> </li> <li> <p>E \u2013 Estimable (Estymowalna)</p> <ul> <li> <p>Powinna by\u0107 na tyle szczeg\u00f3\u0142owa, aby da\u0142o si\u0119 oszacowa\u0107 czas pracy.</p> </li> <li> <p>Je\u015bli nie mo\u017cna jej oszacowa\u0107, to prawdopodobnie jest za du\u017ca i trzeba j\u0105 podzieli\u0107 na mniejsze cz\u0119\u015bci.</p> </li> <li> <p>\u274c Z\u0142e: \u201eDodanie nowego systemu raportowania\u201d \u2013 zbyt szerokie, trudne do oszacowania.</p> </li> <li> <p>\u2705 Dobre: \u201eU\u017cytkownik mo\u017ce wygenerowa\u0107 raport PDF z zadaniami\u201d \u2013 konkretny zakres.</p> </li> </ul> </li> <li> <p>S \u2013 Small (Ma\u0142a)</p> <ul> <li>Powinna by\u0107 na tyle ma\u0142a, \u017ceby mo\u017cna by\u0142o j\u0105 uko\u0144czy\u0107 w jednym sprincie (najlepiej w ci\u0105gu 1-3 dni).</li> </ul> <p>Je\u015bli historia jest zbyt du\u017ca, podziel j\u0105 na mniejsze User Stories.</p> <ul> <li> <p>\u274c Z\u0142e: \u201eJako u\u017cytkownik chc\u0119 pe\u0142n\u0105 personalizacj\u0119 ustawie\u0144 aplikacji\u201d \u2013 za du\u017ce.</p> </li> <li> <p>\u2705 Dobre: \u201eJako u\u017cytkownik chc\u0119 zmieni\u0107 j\u0119zyk aplikacji w ustawieniach\u201d \u2013 ma\u0142e i konkretne.</p> </li> </ul> </li> <li> <p>T \u2013 Testable (Testowalna)</p> <ul> <li>Powinna mie\u0107 jasne kryteria akceptacji, kt\u00f3re pozwalaj\u0105 sprawdzi\u0107, czy dzia\u0142a poprawnie.</li> </ul> <p>Je\u015bli nie mo\u017cna przetestowa\u0107 historii, to jest \u017ale sformu\u0142owana.</p> <ul> <li> <p>\u274c Z\u0142e: \u201eU\u017cytkownik powinien mie\u0107 lepsze do\u015bwiadczenie logowania\u201d \u2013 niejasne, nie da si\u0119 przetestowa\u0107.</p> </li> <li> <p>\u2705 Dobre: \u201ePo trzech b\u0142\u0119dnych pr\u00f3bach logowania u\u017cytkownik widzi komunikat o blokadzie\u201d \u2013 mo\u017cna sprawdzi\u0107.</p> </li> </ul> </li> </ol>"},{"location":"01-user-stories/#przykad-dobrej-user-story-z-invest","title":"Przyk\u0142ad dobrej User Story z INVEST","text":"<p>Tytu\u0142: Jako u\u017cytkownik chc\u0119 otrzymywa\u0107 e-mail z linkiem do resetu has\u0142a, aby m\u00f3c odzyska\u0107 dost\u0119p do konta.</p> <p>\u2705 Independent (Niezale\u017cna) \u2013 Mo\u017cna wdro\u017cy\u0107 reset has\u0142a osobno, bez czekania na inne funkcje.</p> <p>\u2705 Negotiable (Negocjowalna) \u2013 Mo\u017cemy negocjowa\u0107 szczeg\u00f3\u0142y implementacji, np. spos\u00f3b generowania linku do resetu has\u0142a (czas trwania wa\u017cno\u015bci linku, liczba pr\u00f3b, itp.).</p> <p>\u2705 Valuable (Warto\u015bciowa) \u2013 U\u017cytkownik nie straci konta, je\u015bli zapomni has\u0142a.</p> <p>\u2705 Estimable (Estymowalna) \u2013 Mo\u017cemy oszacowa\u0107 prac\u0119 (np. 3 dni).</p> <p>\u2705 Small (Ma\u0142a) \u2013 Jest to jedno konkretne zadanie.</p> <p>\u2705 Testable (Testowalna) \u2013 Mo\u017cemy sprawdzi\u0107, czy e-mail faktycznie przychodzi.</p>"},{"location":"01-user-stories/homework/","title":"Praca domowa","text":"<p>\ud83d\udccc Zadanie 1 \u2013 Temat projektu, backlog i repozytorium kodu</p> <p>\ud83c\udfaf Cel zadania:</p> <p>Celem zadania jest przygotowanie repozytorium kodu oraz backlogu projektu, podzielonego na Feature\u2019y i User Stories zgodnie z podej\u015bciem INVEST.</p> <p>\ud83d\udcdd Elementy do przygotowania:</p> <ol> <li> <p>Grupa projektowa</p> <ul> <li>Uzupe\u0142nijcie plik Excel (teams.xlsx) w plikach zespo\u0142u (imi\u0119, nazwisko, nazwa zespo\u0142u).</li> </ul> </li> <li> <p>Repozytorium kodu</p> <ul> <li> <p>Stw\u00f3rzcie repozytorium w wybranym narz\u0119dziu (GitHub, Azure DevOps).</p> </li> <li> <p>Je\u015bli repozytorium jest prywatne, wy\u015blijcie zaproszenia do cz\u0142onk\u00f3w zespo\u0142u oraz do prowadz\u0105cego.</p> </li> <li> <p>Dodajcie link do repozytorium w pliku Excel (teams.xlsx).</p> </li> </ul> </li> <li> <p>Backlog projektu</p> <ul> <li> <p>Stw\u00f3rzcie backlog zawieraj\u0105cy:</p> <ul> <li> <p>Minimum 2 Feature\u2019y (np. Implementacja UI, Modu\u0142 AI, Wdro\u017cenie).</p> </li> <li> <p>User Stories wraz z kryteriami akceptacji (zgodnie z INVEST).</p> </li> </ul> </li> <li> <p>Mo\u017cecie u\u017cy\u0107 dowolnego narz\u0119dzia (Azure DevOps, Jira, GitHub Projects).</p> </li> </ul> </li> </ol> <p>\ud83d\udce4 Elementy do przes\u0142ania:</p> <ul> <li>Kr\u00f3tki raport (jako zrzut ekranu) \u2013 powinna by\u0107 widoczna hierarchia backlogu: Feature\u2019y i User Stories.</li> </ul> <p>\ud83c\udf99\ufe0f Elementy do pokazania na zaj\u0119ciach:</p> <ul> <li> <p>Backlog projektu w wybranym narz\u0119dziu (Feature\u2019y, User Stories, kryteria akceptacji).</p> </li> <li> <p>Om\u00f3wienie jednej User Story w kontek\u015bcie podej\u015bcia INVEST (grupa sama wybiera, kt\u00f3r\u0105 User Story om\u00f3wi).</p> </li> </ul>"},{"location":"02-environment/","title":"\u015arodowisko programistyczne","text":""},{"location":"02-environment/#organizacja-pracy-w-projekcie-czesc-2","title":"Organizacja pracy w projekcie (cz\u0119\u015b\u0107 2)","text":""},{"location":"02-environment/#priorytetyzacja-backlogu-metoda-moscow","title":"Priorytetyzacja backlogu \u2013 metoda MoSCoW","text":"<p>Metoda MoSCoW pomaga zespo\u0142om okre\u015bli\u0107 priorytet zada\u0144, dziel\u0105c je na cztery kategorie:</p> <ul> <li> <p>Must have (M) \u2013 absolutnie kluczowe wymagania, bez kt\u00f3rych projekt nie mo\u017ce zosta\u0107 uko\u0144czony.</p> <p>Przyk\u0142ad: \u201eSystem musi umo\u017cliwia\u0107 logowanie u\u017cytkownik\u00f3w.\u201d</p> </li> <li> <p>Should have (S) \u2013 bardzo wa\u017cne, ale nie krytyczne; mog\u0105 by\u0107 zrealizowane p\u00f3\u017aniej, je\u015bli zabraknie czasu.</p> <p>Przyk\u0142ad: \u201eAplikacja powinna mie\u0107 mo\u017cliwo\u015b\u0107 logowania przez Google, ale mo\u017ce to by\u0107 dodane po premierze.\u201d</p> </li> <li> <p>Could have (C) \u2013 mile widziane dodatki, ale nie wp\u0142ywaj\u0105 znacz\u0105co na g\u0142\u00f3wne funkcjonalno\u015bci.</p> <p>Przyk\u0142ad: \u201eDodanie ciemnego motywu w interfejsie u\u017cytkownika.\u201d</p> </li> <li> <p>Won't have (W) \u2013 funkcje, kt\u00f3re nie b\u0119d\u0105 realizowane w tym sprincie / wersji, ale mog\u0105 by\u0107 brane pod uwag\u0119 w przysz\u0142o\u015bci.</p> <p>Przyk\u0142ad: \u201eObs\u0142uga wielu j\u0119zyk\u00f3w w pierwszej wersji aplikacji.\u201d</p> </li> </ul>"},{"location":"02-environment/#story-points-i-estymacja-pracy-w-sprincie","title":"Story Points i estymacja pracy w sprincie","text":"<p>Story Points to jednostka miary z\u0142o\u017cono\u015bci i wysi\u0142ku wymaganego do wykonania zadania. Nie jest to liczba godzin, lecz wzgl\u0119dna trudno\u015b\u0107.</p>"},{"location":"02-environment/#jak-przydzielac-zadania","title":"Jak przydziela\u0107 zadania?","text":"<p>Podczas sprint planningu zesp\u00f3\u0142 przypisuje ka\u017cdemu zadaniu Story Points, np. wed\u0142ug skali Fibonacciego (1, 2, 3, 5, 8, 13...).</p> <ul> <li> <p>1-2 SP \u2013 proste zadania, np. \u201eDodanie nowego przycisku w UI.\u201d</p> </li> <li> <p>3-5 SP \u2013 \u015brednie zadania, np. \u201eZaimplementowanie formularza logowania.\u201d</p> </li> <li> <p>8-13 SP \u2013 bardziej z\u0142o\u017cone zadania, np. \u201eStworzenie i integracja API do uwierzytelniania u\u017cytkownik\u00f3w.\u201d</p> </li> </ul> <p>Uwaga!</p> <p>Ka\u017cdy cz\u0142onek zespo\u0142u ma ograniczon\u0105 liczb\u0119 Story Points, kt\u00f3re mo\u017ce zrealizowa\u0107 w jednym sprincie. Np. je\u015bli sprint trwa 2 tygodnie, a programista mo\u017ce \u015brednio uko\u0144czy\u0107 10 SP, to nie powinien dostawa\u0107 wi\u0119cej ni\u017c 10 SP na sprint.</p>"},{"location":"02-environment/#sprint-jak-wyglada-praca-w-scrum","title":"Sprint \u2013 jak wygl\u0105da praca w Scrum?","text":"<p>Sprint to ustalony czas (np. 1-2 tygodnie), w kt\u00f3rym zesp\u00f3\u0142 realizuje zaplanowane zadania.</p> <p>Przebieg sprintu:</p> <ul> <li> <p>Sprint Planning \u2013 zaplanowanie zakresu pracy na sprint.</p> </li> <li> <p>Daily Stand-up \u2013 kr\u00f3tkie codzienne spotkania (15 min), ka\u017cdy odpowiada na 3 pytania:</p> <ul> <li> <p>Co zrobi\u0142em wczoraj?</p> </li> <li> <p>Co zrobi\u0119 dzisiaj?</p> </li> <li> <p>Czy mam jakie\u015b blokery?</p> </li> </ul> </li> <li> <p>Sprint Review \u2013 podsumowanie prac, prezentacja wynik\u00f3w.</p> </li> <li> <p>Sprint Retrospective \u2013 analiza, co posz\u0142o dobrze, a co mo\u017cna poprawi\u0107.</p> </li> </ul> <p></p>"},{"location":"02-environment/#role-w-scrum","title":"Role w Scrum","text":"<ul> <li> <p>Product Owner \u2013 osoba odpowiedzialna za backlog i priorytetyzacj\u0119 zada\u0144.</p> <p>Dba o to, by zesp\u00f3\u0142 dostarcza\u0142 warto\u015b\u0107 biznesow\u0105.</p> <p>Przyk\u0142ad: decyduje, czy doda\u0107 funkcj\u0119 \u201elogowania przez Facebook\u201d w tym sprincie czy w przysz\u0142ym.</p> </li> <li> <p>Scrum Master \u2013 wspiera zesp\u00f3\u0142, pomaga usuwa\u0107 blokery, pilnuje zasad Scrum.</p> <p>Przyk\u0142ad: je\u015bli zesp\u00f3\u0142 ma problem z dost\u0119pem do serwera, Scrum Master pomaga znale\u017a\u0107 rozwi\u0105zanie.</p> </li> <li> <p>Development Team \u2013 programi\u015bci, testerzy, DevOps \u2013 osoby realizuj\u0105ce zadania w backlogu.</p> </li> <li> <p>Stakeholder / Client \u2013 osoby, kt\u00f3re maj\u0105 wp\u0142yw na produkt (np. klient, szef, inwestorzy).</p> </li> </ul>"},{"location":"02-environment/#srodowiska-wirtualne-w-pythonie","title":"\u015arodowiska wirtualne w Pythonie","text":"<p>Wirtualne \u015brodowiska w Pythonie pozwalaj\u0105 na izolacj\u0119 pakiet\u00f3w dla r\u00f3\u017cnych projekt\u00f3w, dzi\u0119ki czemu unikamy konflikt\u00f3w zale\u017cno\u015bci i problem\u00f3w z r\u00f3\u017cnymi wersjami bibliotek.</p>"},{"location":"02-environment/#po-co-uzywac-wirtualnych-srodowisk","title":"Po co u\u017cywa\u0107 wirtualnych \u015brodowisk?","text":"<ul> <li> <p>Izolacja projekt\u00f3w \u2013 Ka\u017cdy projekt ma swoje w\u0142asne zale\u017cno\u015bci, dzi\u0119ki czemu nie ma konflikt\u00f3w mi\u0119dzy r\u00f3\u017cnymi wersjami pakiet\u00f3w.</p> </li> <li> <p>Bezpiecze\u0144stwo \u2013 Mo\u017cna testowa\u0107 nowe pakiety bez ryzyka uszkodzenia globalnego \u015brodowiska.</p> </li> <li>Reprodukowalno\u015b\u0107 \u2013 Mo\u017cna \u0142atwo odtworzy\u0107 \u015brodowisko na innym komputerze.</li> <li>Elastyczno\u015b\u0107 \u2013 Mo\u017cliwo\u015b\u0107 pracy z r\u00f3\u017cnymi wersjami Pythona i bibliotek w tym samym systemie.</li> </ul>"},{"location":"02-environment/#popularne-narzedzia-do-zarzadzania-srodowiskami","title":"Popularne narz\u0119dzia do zarz\u0105dzania \u015brodowiskami","text":"Narz\u0119dzie Opis Zalety Wady <code>venv</code> Wbudowane narz\u0119dzie Pythona do tworzenia \u015brodowisk wirtualnych. \u2705 Wbudowane w Python, lekkie, proste \u274c Brak zarz\u0105dzania zale\u017cno\u015bciami i wersjami Pythona <code>virtualenv</code> Starsza alternatywa dla <code>venv</code>, dzia\u0142a z wieloma wersjami Pythona. \u2705 Szybsze ni\u017c <code>venv</code>, kompatybilne z <code>pip</code> \u274c Nie zarz\u0105dza wersjami Pythona <code>pyenv</code> Narz\u0119dzie do zarz\u0105dzania wersjami Pythona. \u2705 Pozwala u\u017cywa\u0107 wielu wersji Pythona \u274c Nie zarz\u0105dza pakietami <code>conda</code> Zaawansowane narz\u0119dzie do zarz\u0105dzania \u015brodowiskami i pakietami (g\u0142\u00f3wnie dla Data Science). \u2705 Instalacja skompilowanych pakiet\u00f3w, zarz\u0105dza wersjami Pythona \u274c Wymaga pobrania Minicondy lub Anacondy <code>poetry</code> Nowoczesne narz\u0119dzie do zarz\u0105dzania zale\u017cno\u015bciami i \u015brodowiskami. \u2705 Automatyczne zarz\u0105dzanie pakietami, \u015bwietne do aplikacji \u274c Mo\u017ce by\u0107 nadmiarowe dla prostych projekt\u00f3w"},{"location":"02-environment/#jak-uzywac-venv","title":"Jak u\u017cywa\u0107 venv?","text":"<p>Tworzenie nowego \u015brodowiska:</p> <pre><code>python -m venv my_env\n</code></pre> <p>Aktywacja \u015brodowiska:</p> <pre><code>source my_env/bin/activate\n</code></pre> <p>Dezaktywacja \u015brodowiska:</p> <pre><code>deactivate\n</code></pre> <p>Instalowanie pakiet\u00f3w:</p> <pre><code>pip install numpy pandas\n</code></pre> <p>Zapisywanie zale\u017cno\u015bci do pliku (requirements.txt)</p> <pre><code>pip list --format=freeze &gt; requirements.txt\n</code></pre> <p>Odtworzenie \u015brodowiska na innym komputerze:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"02-environment/#jak-uzywac-conda","title":"Jak u\u017cywa\u0107 Conda?","text":""},{"location":"02-environment/#miniconda-vs-anaconda-jaka-roznica","title":"Miniconda vs Anaconda \u2013 jaka r\u00f3\u017cnica?","text":"Cecha Miniconda Anaconda Rozmiar \ud83d\udd39 Ma\u0142e (~50MB) \ud83d\udd39 Du\u017ce (~3GB) Domy\u015blne pakiety \ud83d\udeab Brak pakiet\u00f3w \u2705 Zawiera pakiety (numpy, pandas, Jupyter itp.) Elastyczno\u015b\u0107 \u2705 Instalujesz tylko to, czego potrzebujesz \u274c Instaluje si\u0119 du\u017co zb\u0119dnych rzeczy Licencja \u2705 Darmowa (w tym do u\u017cytku komercyjnego) \u274c Wymaga licencji komercyjnej dla firm <p>\ud83d\udccc Wniosek: Je\u015bli chcesz pe\u0142n\u0105 kontrol\u0119 i minimalny rozmiar \u2013 u\u017cywaj Minicondy. Je\u015bli chcesz gotowe \u015brodowisko do Data Science \u2013 wybierz Anacond\u0119.</p>"},{"location":"02-environment/#tworzenie-srodowiska-conda","title":"Tworzenie \u015brodowiska Conda","text":"<p>Tworzenie nowego \u015brodowiska:</p> <pre><code>conda create -n my_env python=3.9\n</code></pre> <p>Aktywacja \u015brodowiska:</p> <pre><code>conda activate my_env\n</code></pre> <p>Dezaktywacja \u015brodowiska:</p> <pre><code>conda deactivate\n</code></pre> <p>Usuni\u0119cie \u015brodowiska:</p> <pre><code>conda remove -n my_env --all\n</code></pre> <p>Instalowanie pakiet\u00f3w:</p> <pre><code>conda install numpy pandas\n</code></pre> <p>Dlaczego u\u017cywa\u0107 conda install zamiast pip install?</p> <ul> <li>Conda pobiera skompilowane pakiety \u2192 dzia\u0142a szybciej i eliminuje problemy z zale\u017cno\u015bciami.</li> <li>Mo\u017ce automatycznie dopasowa\u0107 wersje zale\u017cno\u015bci (pip nie robi tego tak dobrze).</li> <li>pip instaluje pakiety z PyPI, a conda z w\u0142asnych repozytori\u00f3w (np. conda-forge), co czasem prowadzi do konflikt\u00f3w.</li> </ul>"},{"location":"02-environment/#jak-zarzadzac-zaleznosciami","title":"Jak zarz\u0105dza\u0107 zale\u017cno\u015bciami?","text":""},{"location":"02-environment/#requirementstxt-pip","title":"requirements.txt (pip)","text":"<p>\ud83d\udccc Plik u\u017cywany przez pip, zawiera list\u0119 pakiet\u00f3w do zainstalowania.</p> <p>Przyk\u0142ad:</p> <pre><code>numpy==1.23.0\npandas&gt;=1.5,&lt;2.0\nscikit-learn\n</code></pre>"},{"location":"02-environment/#environmentyml-conda","title":"environment.yml (conda)","text":"<p>\ud83d\udccc Plik konfiguracyjny do odtwarzania \u015brodowisk Conda.</p> <pre><code>name: my_env\ndependencies:\n  - python=3.9\n  - numpy=1.23.0\n  - pandas&gt;=1.5,&lt;2.0\n  - pip\n  - pip:\n      - scikit-learn\n</code></pre> <p>Tworzenie \u015brodowiska z pliku:</p> <pre><code>conda env create -f environment.yml\n</code></pre> <p>Eksportowanie aktualnego \u015brodowiska do pliku:</p> <pre><code>conda env export &gt; environment.yml\n</code></pre>"},{"location":"02-environment/#dependency-hell-jak-conda-rozwiazuje-konflikty","title":"Dependency Hell \u2013 jak Conda rozwi\u0105zuje konflikty?","text":"<p>Conda automatycznie sprawdza zgodno\u015b\u0107 wersji pakiet\u00f3w i je\u015bli nowy pakiet wymaga innej wersji ju\u017c zainstalowanego pakietu, mo\u017ce go zaktualizowa\u0107, obni\u017cy\u0107 wersj\u0119 lub usun\u0105\u0107.</p> <p>Generowanie \"planu dzia\u0142ania\" poprzez conda:</p> <pre><code>conda install tensorflow --dry-run\n</code></pre>"},{"location":"02-environment/#podsumowanie","title":"Podsumowanie","text":"<ul> <li> <p>venv \u2013 dobre do prostych projekt\u00f3w.</p> </li> <li> <p>Conda \u2013 najlepsze dla Data Science i ML.</p> </li> <li> <p>pyenv \u2013 je\u015bli musisz pracowa\u0107 z wieloma wersjami Pythona.</p> </li> <li> <p>poetry \u2013 nowoczesne zarz\u0105dzanie pakietami dla aplikacji.</p> </li> </ul>"},{"location":"02-environment/#pierwsza-aplikacja-w-streamlit","title":"Pierwsza aplikacja w Streamlit","text":"<ul> <li> <p>Przyk\u0142adowa aplikacja znajduje si\u0119 w folderze <code>sentiment_analysis</code>.</p> </li> <li> <p>Dokumentacja pakietu, element\u00f3w oraz przyk\u0142adowe aplikacje dost\u0119pne na: https://docs.streamlit.io/</p> </li> </ul>"},{"location":"02-environment/example/","title":"Przyk\u0142ad","text":""},{"location":"02-environment/example/#instalacja-pythona-tworzenie-srodowiska-i-uruchomienie-aplikacji","title":"Instalacja Pythona, tworzenie \u015brodowiska i uruchomienie aplikacji","text":"<pre><code>#!/bin/bash\n\n# Install Python\nsudo apt update\nsudo apt install -y software-properties-common\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update\nsudo apt install -y python3.10\nsudo apt install -y python3.10-venv\n\n# Create virtual environment\npython3.10 -m venv .venv\n\n# Activate virtual environment and install dependencies\nsource .venv/bin/activate\npip install matplotlib nltk streamlit tensorflow tf-keras transformers \npip list --format=freeze &gt; requirements.txt\n\n# Start the app\nstreamlit run app.py --server.port 8000\n</code></pre>"},{"location":"02-environment/example/#kod-aplikacji-streamlit","title":"Kod aplikacji streamlit","text":"<pre><code>import streamlit as st\nfrom transformers import pipeline\nimport matplotlib.pyplot as plt\nimport nltk\nimport time\n\n# Za\u0142adowanie modelu analizy sentymentu\nclassifier = pipeline(\"sentiment-analysis\")\n\n# Za\u0142adowanie zasob\u00f3w do tokenizacji z NLTK\nnltk.download('punkt')\n\n# Ustawienia strony w Streamlit\nst.set_page_config(page_title=\"Analiza Sentymentu\", page_icon=\"\ud83d\udcac\")\n\n# Nag\u0142\u00f3wek aplikacji\nst.title(\"\ud83d\udd0d Analiza Sentymentu z Hugging Face\")\n\n# Pole tekstowe do wprowadzania danych\ntext = st.text_area(\"Wpisz d\u0142ugi tekst do analizy:\", \"\")\n\n# Przyciski do uruchomienia analizy\nif st.button(\"Analizuj sentyment\"):\n    if text:\n        with st.spinner('Przetwarzam tekst...'):\n            # Pauza, aby wida\u0107 by\u0142o \u0142adowanie\n            time.sleep(1)\n\n            # Podzia\u0142 tekstu na list\u0119 zda\u0144\n            sentences = nltk.sent_tokenize(text)\n\n            # Listy do przechowywania wynik\u00f3w\n            labels = []\n            scores = []\n\n            # Analiza sentymentu dla ka\u017cdego zdania\n            for sentence in sentences:\n                result = classifier(sentence)[0]\n                label = result[\"label\"]\n                score = result[\"score\"]\n                labels.append(label)\n                scores.append(score)\n\n            # Wy\u015bwietlenie wynik\u00f3w\n            st.write(\"**Wyniki analizy sentymentu dla ka\u017cdego zdania:**\")\n            for i, sentence in enumerate(sentences):\n                st.write(f\"**Zdanie {i+1}:** {sentence}\")\n                st.write(f\"\ud83d\udd39 **Label:** {labels[i]}\")\n                st.write(f\"\ud83d\udcca **Prawdopodobie\u0144stwo:** {scores[i]:.4f}\")\n                st.write(\"---\")\n\n            # Przygotowanie wykresu\n            fig, ax = plt.subplots(figsize=(10, 6))\n            # Rysowanie wykresu s\u0142upkowego za pomoc\u0105 pyplot.bar\n            bars = plt.bar(range(len(sentences)), scores, color=['green' if label == \"POSITIVE\" else 'red' for label in labels])\n            # Ustawienia osi X\n            plt.xticks(range(len(sentences)), [f\"Zdanie {i+1}\" for i in range(len(sentences))])\n            # Etykieta osi Y i tytu\u0142 wykresu\n            plt.ylabel('Prawdopodobie\u0144stwo')\n            plt.title('Analiza sentymentu dla ka\u017cdego zdania')\n            # Ustawienie zakresu osi Y\n            plt.ylim(0, 1.05)\n            # Dodanie warto\u015bci prawdopodobie\u0144stwa na s\u0142upkach\n            for i, bar in enumerate(bars):\n                yval = bar.get_height()\n                plt.text(bar.get_x() + bar.get_width() / 2, yval, f\"{yval:.2f}\", ha='center', va='bottom', fontsize=10)\n            # Optymalizacja rozmieszczenia element\u00f3w\n            plt.tight_layout()\n            # Wy\u015bwietlenie wykresu w Streamlit\n            st.pyplot(fig)\n    else:\n        st.warning(\"\u26a0\ufe0f Wprowad\u017a tekst do analizy!\")\n\n# Dodatkowa sekcja z informacjami o modelu\nst.sidebar.header(\"O modelu\")\nst.sidebar.write(\n    \"Aplikacja wykorzystuje model Hugging Face do analizy sentymentu wprowadzonych tekst\u00f3w. \"\n    \"Model ten rozr\u00f3\u017cnia, czy tekst jest pozytywny, czy negatywny na podstawie analizy s\u0142\u00f3w i kontekstu.\"\n)\n\n# Sekcja FAQ\nst.sidebar.header(\"FAQ\")\nst.sidebar.write(\n    \"1. **Jak dzia\u0142a analiza sentymentu?**\\n\"\n    \"   Model analizuje wprowadzone s\u0142owa i przypisuje im etykiety, np. 'Pozytywny' lub 'Negatywny', \"\n    \"w zale\u017cno\u015bci od kontekstu emocjonalnego.\\n\\n\"\n    \"2. **Czy mog\u0119 u\u017cywa\u0107 innych j\u0119zyk\u00f3w?**\\n\"\n    \"   Tak, mo\u017cesz za\u0142adowa\u0107 odpowiedni model dla r\u00f3\u017cnych j\u0119zyk\u00f3w.\"\n)\n</code></pre>"},{"location":"02-environment/extra/","title":"Zadanie dodatkowe","text":"<p>Stw\u00f3rz aplikacj\u0119 w Streamlit do t\u0142umaczenia zda\u0144 z angielskiego na francuski.</p> <ol> <li> <p>Stw\u00f3rz wirtualne \u015brodowisko korzystaj\u0105c z <code>venv</code> lub <code>conda</code>.</p> </li> <li> <p>Przygotuj skrypt <code>app.py</code>. Skorzystaj z modelu <code>Helsinki-NLP/opus-mt-en-fr</code>.</p> <pre><code>translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\nresult = translator(\"Hello, how are you?\")\n</code></pre> </li> </ol> <p>\ud83d\udea8 UWAGA! \ud83d\udea8</p> <p>Do poprawnego dzia\u0142ania wymagana jest instalacja biblioteki <code>sentencepiece</code></p>"},{"location":"02-environment/homework/","title":"Praca domowa","text":"<p>\ud83d\udccc Zadanie 2 \u2013 Dzia\u0142aj\u0105ca pierwsza wersja aplikacji</p> <p>\ud83c\udfaf Cel zadania:</p> <p>Celem zadania jest przygotowanie pierwszej dzia\u0142aj\u0105cej wersji aplikacji (w oparciu o Streamlit, Gradio lub inne podobne narz\u0119dzie) z prostym interfejsem, kt\u00f3ry umo\u017cliwia interakcj\u0119, jednak nie \u0142\u0105czy si\u0119 z \u017cadnym modelem AI (zamiast tego wykorzystaj placeholdery lub dummy content). Aplikacja powinna by\u0107 w pe\u0142ni uruchamialna i prezentowa\u0107 poprawne dzia\u0142anie.</p> <p>\ud83d\udcdd Elementy do przygotowania:</p> <ol> <li> <p>Aplikacja:</p> <p>Stw\u00f3rzcie prost\u0105 aplikacj\u0119 za pomoc\u0105 Streamlit, Gradio lub innego podobnego narz\u0119dzia, kt\u00f3ra:</p> <ul> <li> <p>Zawiera interfejs u\u017cytkownika (np. formularz, przyciski, inputy).</p> </li> <li> <p>Przyci\u015bni\u0119cie przycisku powoduje wy\u015bwietlenie placeholdera (np. \"Wkr\u00f3tce dost\u0119pne\" lub losowego tekstu zamiast odpowiedzi modelu AI).</p> </li> <li> <p>Aplikacja powinna by\u0107 uruchamialna lokalnie i dzia\u0142a\u0107 bez problem\u00f3w (przyk\u0142ad: streamlit run app.py).</p> </li> </ul> </li> <li> <p>Backlog projektu:</p> <ul> <li> <p>Stw\u00f3rzcie nowy sprint w Waszym narz\u0119dziu do zarz\u0105dzania projektem.</p> </li> <li> <p>Zamknijcie co najmniej 2 User Stories, gdzie ka\u017cda User Story powinna by\u0107 przypisana do innej osoby.</p> </li> </ul> </li> <li> <p>Dokumentacja:</p> <ul> <li> <p>Uzupe\u0142nijcie plik README.md w repozytorium kodu: opis aplikacji, jak uruchomi\u0107 aplikacj\u0119 lokalnie oraz jak zainstalowa\u0107 zale\u017cno\u015bci.</p> </li> <li> <p>Opcjonalnie, przygotujcie skrypt run.sh, kt\u00f3ry automatycznie uruchomi aplikacj\u0119.</p> </li> </ul> </li> </ol> <p>\ud83d\udce4 Elementy do przes\u0142ania:</p> <ul> <li>Zrzut ekranu aplikacji dzia\u0142aj\u0105cej lokalnie \u2013 na zrzucie ekranu powinno by\u0107 widoczne, jak klikni\u0119cie na przycisk wy\u015bwietla placeholder.</li> </ul> <p>\ud83c\udf99\ufe0f Elementy do pokazania na zaj\u0119ciach:</p> <ul> <li>Dzia\u0142aj\u0105ca aplikacja, kt\u00f3ra uruchamia si\u0119 lokalnie i pokazuje interakcj\u0119 u\u017cytkownika (np. klikni\u0119cie przycisku powoduje wy\u015bwietlenie tekstu placeholdera).</li> </ul>"},{"location":"02-environment/sentiment_analysis/app/","title":"App","text":"In\u00a0[\u00a0]: Copied! <pre>import streamlit as st\nfrom transformers import pipeline\nimport matplotlib.pyplot as plt\nimport nltk\nimport time\n</pre> import streamlit as st from transformers import pipeline import matplotlib.pyplot as plt import nltk import time In\u00a0[\u00a0]: Copied! <pre># Za\u0142adowanie modelu analizy sentymentu\nclassifier = pipeline(\"sentiment-analysis\")\n</pre> # Za\u0142adowanie modelu analizy sentymentu classifier = pipeline(\"sentiment-analysis\") In\u00a0[\u00a0]: Copied! <pre># Za\u0142adowanie zasob\u00f3w do tokenizacji z NLTK\nnltk.download('punkt')\n</pre> # Za\u0142adowanie zasob\u00f3w do tokenizacji z NLTK nltk.download('punkt') In\u00a0[\u00a0]: Copied! <pre># Ustawienia strony w Streamlit\nst.set_page_config(page_title=\"Analiza Sentymentu\", page_icon=\"\ud83d\udcac\")\n</pre> # Ustawienia strony w Streamlit st.set_page_config(page_title=\"Analiza Sentymentu\", page_icon=\"\ud83d\udcac\") In\u00a0[\u00a0]: Copied! <pre># Nag\u0142\u00f3wek aplikacji\nst.title(\"\ud83d\udd0d Analiza Sentymentu z Hugging Face\")\n</pre> # Nag\u0142\u00f3wek aplikacji st.title(\"\ud83d\udd0d Analiza Sentymentu z Hugging Face\") In\u00a0[\u00a0]: Copied! <pre># Pole tekstowe do wprowadzania danych\ntext = st.text_area(\"Wpisz d\u0142ugi tekst do analizy:\", \"\")\n</pre> # Pole tekstowe do wprowadzania danych text = st.text_area(\"Wpisz d\u0142ugi tekst do analizy:\", \"\") In\u00a0[\u00a0]: Copied! <pre># Przyciski do uruchomienia analizy\nif st.button(\"Analizuj sentyment\"):\n    if text:\n        with st.spinner('Przetwarzam tekst...'):\n            # Pauza, aby wida\u0107 by\u0142o \u0142adowanie\n            time.sleep(1)\n\n            # Podzia\u0142 tekstu na list\u0119 zda\u0144\n            sentences = nltk.sent_tokenize(text)\n\n            # Listy do przechowywania wynik\u00f3w\n            labels = []\n            scores = []\n\n            # Analiza sentymentu dla ka\u017cdego zdania\n            for sentence in sentences:\n                result = classifier(sentence)[0]\n                label = result[\"label\"]\n                score = result[\"score\"]\n                labels.append(label)\n                scores.append(score)\n\n            # Wy\u015bwietlenie wynik\u00f3w\n            st.write(\"**Wyniki analizy sentymentu dla ka\u017cdego zdania:**\")\n            for i, sentence in enumerate(sentences):\n                st.write(f\"**Zdanie {i+1}:** {sentence}\")\n                st.write(f\"\ud83d\udd39 **Label:** {labels[i]}\")\n                st.write(f\"\ud83d\udcca **Prawdopodobie\u0144stwo:** {scores[i]:.4f}\")\n                st.write(\"---\")\n\n            # Przygotowanie wykresu\n            fig, ax = plt.subplots(figsize=(10, 6))\n            # Rysowanie wykresu s\u0142upkowego za pomoc\u0105 pyplot.bar\n            bars = plt.bar(range(len(sentences)), scores, color=['green' if label == \"POSITIVE\" else 'red' for label in labels])\n            # Ustawienia osi X\n            plt.xticks(range(len(sentences)), [f\"Zdanie {i+1}\" for i in range(len(sentences))])\n            # Etykieta osi Y i tytu\u0142 wykresu\n            plt.ylabel('Prawdopodobie\u0144stwo')\n            plt.title('Analiza sentymentu dla ka\u017cdego zdania')\n            # Ustawienie zakresu osi Y\n            plt.ylim(0, 1.05)\n            # Dodanie warto\u015bci prawdopodobie\u0144stwa na s\u0142upkach\n            for i, bar in enumerate(bars):\n                yval = bar.get_height()\n                plt.text(bar.get_x() + bar.get_width() / 2, yval, f\"{yval:.2f}\", ha='center', va='bottom', fontsize=10)\n            # Optymalizacja rozmieszczenia element\u00f3w\n            plt.tight_layout()\n            # Wy\u015bwietlenie wykresu w Streamlit\n            st.pyplot(fig)\n    else:\n        st.warning(\"\u26a0\ufe0f Wprowad\u017a tekst do analizy!\")\n</pre> # Przyciski do uruchomienia analizy if st.button(\"Analizuj sentyment\"):     if text:         with st.spinner('Przetwarzam tekst...'):             # Pauza, aby wida\u0107 by\u0142o \u0142adowanie             time.sleep(1)              # Podzia\u0142 tekstu na list\u0119 zda\u0144             sentences = nltk.sent_tokenize(text)              # Listy do przechowywania wynik\u00f3w             labels = []             scores = []              # Analiza sentymentu dla ka\u017cdego zdania             for sentence in sentences:                 result = classifier(sentence)[0]                 label = result[\"label\"]                 score = result[\"score\"]                 labels.append(label)                 scores.append(score)              # Wy\u015bwietlenie wynik\u00f3w             st.write(\"**Wyniki analizy sentymentu dla ka\u017cdego zdania:**\")             for i, sentence in enumerate(sentences):                 st.write(f\"**Zdanie {i+1}:** {sentence}\")                 st.write(f\"\ud83d\udd39 **Label:** {labels[i]}\")                 st.write(f\"\ud83d\udcca **Prawdopodobie\u0144stwo:** {scores[i]:.4f}\")                 st.write(\"---\")              # Przygotowanie wykresu             fig, ax = plt.subplots(figsize=(10, 6))             # Rysowanie wykresu s\u0142upkowego za pomoc\u0105 pyplot.bar             bars = plt.bar(range(len(sentences)), scores, color=['green' if label == \"POSITIVE\" else 'red' for label in labels])             # Ustawienia osi X             plt.xticks(range(len(sentences)), [f\"Zdanie {i+1}\" for i in range(len(sentences))])             # Etykieta osi Y i tytu\u0142 wykresu             plt.ylabel('Prawdopodobie\u0144stwo')             plt.title('Analiza sentymentu dla ka\u017cdego zdania')             # Ustawienie zakresu osi Y             plt.ylim(0, 1.05)             # Dodanie warto\u015bci prawdopodobie\u0144stwa na s\u0142upkach             for i, bar in enumerate(bars):                 yval = bar.get_height()                 plt.text(bar.get_x() + bar.get_width() / 2, yval, f\"{yval:.2f}\", ha='center', va='bottom', fontsize=10)             # Optymalizacja rozmieszczenia element\u00f3w             plt.tight_layout()             # Wy\u015bwietlenie wykresu w Streamlit             st.pyplot(fig)     else:         st.warning(\"\u26a0\ufe0f Wprowad\u017a tekst do analizy!\") In\u00a0[\u00a0]: Copied! <pre># Dodatkowa sekcja z informacjami o modelu\nst.sidebar.header(\"O modelu\")\nst.sidebar.write(\n    \"Aplikacja wykorzystuje model Hugging Face do analizy sentymentu wprowadzonych tekst\u00f3w. \"\n    \"Model ten rozr\u00f3\u017cnia, czy tekst jest pozytywny, czy negatywny na podstawie analizy s\u0142\u00f3w i kontekstu.\"\n)\n</pre> # Dodatkowa sekcja z informacjami o modelu st.sidebar.header(\"O modelu\") st.sidebar.write(     \"Aplikacja wykorzystuje model Hugging Face do analizy sentymentu wprowadzonych tekst\u00f3w. \"     \"Model ten rozr\u00f3\u017cnia, czy tekst jest pozytywny, czy negatywny na podstawie analizy s\u0142\u00f3w i kontekstu.\" ) In\u00a0[\u00a0]: Copied! <pre># Sekcja FAQ\nst.sidebar.header(\"FAQ\")\nst.sidebar.write(\n    \"1. **Jak dzia\u0142a analiza sentymentu?**\\n\"\n    \"   Model analizuje wprowadzone s\u0142owa i przypisuje im etykiety, np. 'Pozytywny' lub 'Negatywny', \"\n    \"w zale\u017cno\u015bci od kontekstu emocjonalnego.\\n\\n\"\n    \"2. **Czy mog\u0119 u\u017cywa\u0107 innych j\u0119zyk\u00f3w?**\\n\"\n    \"   Tak, mo\u017cesz za\u0142adowa\u0107 odpowiedni model dla r\u00f3\u017cnych j\u0119zyk\u00f3w.\"\n)\n</pre> # Sekcja FAQ st.sidebar.header(\"FAQ\") st.sidebar.write(     \"1. **Jak dzia\u0142a analiza sentymentu?**\\n\"     \"   Model analizuje wprowadzone s\u0142owa i przypisuje im etykiety, np. 'Pozytywny' lub 'Negatywny', \"     \"w zale\u017cno\u015bci od kontekstu emocjonalnego.\\n\\n\"     \"2. **Czy mog\u0119 u\u017cywa\u0107 innych j\u0119zyk\u00f3w?**\\n\"     \"   Tak, mo\u017cesz za\u0142adowa\u0107 odpowiedni model dla r\u00f3\u017cnych j\u0119zyk\u00f3w.\" )"},{"location":"02-environment/translator/app/","title":"App","text":"In\u00a0[\u00a0]: Copied! <pre>from transformers import pipeline\n</pre> from transformers import pipeline In\u00a0[\u00a0]: Copied! <pre>translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\")\nresult = translator(\"Hello, how are you?\")\n</pre> translator = pipeline(\"translation_en_to_fr\", model=\"Helsinki-NLP/opus-mt-en-fr\") result = translator(\"Hello, how are you?\")"},{"location":"03-neural-networks/","title":"Wst\u0119p do sieci neuronowych","text":""},{"location":"03-neural-networks/#czym-jest-siec-neuronowa","title":"Czym jest sie\u0107 neuronowa?","text":"<p>Sie\u0107 neuronowa to model matematyczny inspirowany budow\u0105 ludzkiego m\u00f3zgu. Sk\u0142ada si\u0119 z neuron\u00f3w, kt\u00f3re przekazuj\u0105 sobie informacje, przekszta\u0142caj\u0105c wej\u015bciowe dane w u\u017cyteczne wyniki. Struktura sieci sk\u0142ada si\u0119 z trzech g\u0142\u00f3wnych warstw:</p> <ul> <li>Warstwa wej\u015bciowa \u2013 przyjmuje dane wej\u015bciowe (np. piksele obrazu, cechy tekstu).</li> <li>Warstwy ukryte \u2013 przekszta\u0142caj\u0105 dane poprzez po\u0142\u0105czenia mi\u0119dzy neuronami.</li> <li>Warstwa wyj\u015bciowa \u2013 zwraca wynik (np. klasyfikacja obrazu jako \"kot\" lub \"pies\").</li> </ul> <p></p> <p></p> <p>Dodatkowo, w celu wprowadzenia nieliniowo\u015bci, po ka\u017cdej warstwie stosuje si\u0119 tzw. funkcj\u0119 aktywacji.</p>"},{"location":"03-neural-networks/#funkcje-aktywacji","title":"Funkcje aktywacji","text":"<p>Funkcje aktywacji decyduj\u0105, jak przekszta\u0142cane s\u0105 sygna\u0142y przechodz\u0105ce przez neurony. Oto najwa\u017cniejsze z nich:</p> <ol> <li>Sigmoid \u2013 przekszta\u0142ca wej\u015bcie w zakres (0,1):</li> </ol> <p>f(x) = \\frac{1}{1 + e^{-x}}</p> <ol> <li>ReLU (Rectified Linear Unit) \u2013 najcz\u0119\u015bciej stosowana, zeruje warto\u015bci ujemne:</li> </ol> <p>f(x) = \\max(0, x)</p> <ol> <li>Tanh \u2013 podobna do sigmoid, ale zwraca warto\u015bci w zakresie (-1,1):</li> </ol> <p>f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}</p> <ol> <li>Softmax \u2013 stosowana w klasyfikacji wieloklasowej, przekszta\u0142ca wyniki w prawdopodobie\u0144stwa:</li> </ol> <p>f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{K} e^{x_j}}</p> <p>UWAGA: Funkcja softmax u\u017cywa warto\u015bci z ca\u0142ej warstwy sieci neuronowej, a nie przekszta\u0142ca tylko jedn\u0105 warto\u015b\u0107 na drug\u0105.</p> <p>UWAGA: Gdyby nie u\u017cywano funkcji aktywacji, ka\u017cd\u0105 g\u0142\u0119bok\u0105 sie\u0107 neuronow\u0105 mo\u017cnaby by\u0142o sprowadzi\u0107 do jednej warstwy (jednej macierzy).</p>"},{"location":"03-neural-networks/#frameworki-w-pythonie-do-budowy-i-trenowania-sieci-neuronowych","title":"Frameworki w Pythonie do budowy (i trenowania) sieci neuronowych","text":"<p>Do trenowania sieci neuronowych u\u017cywa si\u0119 narz\u0119dzi takich jak: - TensorFlow / Keras \u2013 prostszy do nauki, przypomina uk\u0142adanie klock\u00f3w. - PyTorch \u2013 bardziej elastyczny i cz\u0119sto u\u017cywany w badaniach.</p> <p>Na pocz\u0105tek skorzystamy z Keras, poniewa\u017c pozwala \u0142atwo definiowa\u0107 i trenowa\u0107 modele neuronowe.</p>"},{"location":"03-neural-networks/#podstawowe-elementy-sieci-neuronowej","title":"Podstawowe elementy sieci neuronowej","text":"<p>Aby zbudowa\u0107 sie\u0107 neuronow\u0105, potrzebujemy: 1. Danych \u2013 zestawu wej\u015bciowych przyk\u0142ad\u00f3w do klasyfikacji. 2. Funkcji straty \u2013 mierzy, jak dobrze model przewiduje wyniki. 3. Modelu statystycznego \u2013 struktury sieci neuronowej. 4. Metody optymalizacji \u2013 spos\u00f3b dostosowywania wag neuron\u00f3w do poprawy wynik\u00f3w.</p> <p></p>"},{"location":"03-neural-networks/#propagacja-wsteczna-i-optymalizacja","title":"Propagacja wsteczna i optymalizacja","text":"<ul> <li>Feedforward \u2013 dane przechodz\u0105 przez sie\u0107 od wej\u015bcia do wyj\u015bcia.</li> <li>Backpropagation (propagacja wsteczna) \u2013 metoda uczenia sieci, polega na aktualizacji wag poprzez minimalizacj\u0119 b\u0142\u0119du.</li> <li>Epoka - jedna iteracja danych treningowych Feedforward oraz propagacji wstecznej.</li> </ul> <p>Algorytm propagacji wstecznej polega na obliczaniu gradientu funkcji straty i aktualizacji wag wed\u0142ug wzoru:</p>  w_{i+1} = w_i - \\eta \\frac{\\partial L}{\\partial w_i}  <p>gdzie: - w_i \u2013 wagi modelu w iteracji i, - \\eta \u2013 wsp\u00f3\u0142czynnik uczenia, - L \u2013 funkcja straty.</p> <p>Najcz\u0119\u015bciej stosowane algorytmy optymalizacji: - SGD (Stochastic Gradient Descent) \u2013 klasyczna metoda spadku gradientu. - Adam (Adaptive Moment Estimation) \u2013 ulepszona metoda optymalizacji, szybciej konwerguje.</p>"},{"location":"03-neural-networks/#funkcje-straty-w-klasyfikacji","title":"Funkcje straty w klasyfikacji","text":"<p>Dla problem\u00f3w klasyfikacyjnych cz\u0119sto stosuje si\u0119: - Binary Crossentropy \u2013 dla klasyfikacji binarnej (np. \"kot\" vs \"nie kot\"):</p> <p>L = - \\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right)</p> <ul> <li>Categorical Crossentropy \u2013 dla klasyfikacji wieloklasowej (np. \"kot\", \"pies\", \"ptak\"):</li> </ul> <p>L = - \\sum_{i=1}^{N} \\sum_{j=1}^{K} y_{ij} \\log(\\hat{y}_{ij})</p> <p>Wizualizacja uczenia sieci neuronowej: https://playground.tensorflow.org/</p>"},{"location":"03-neural-networks/#konwolucyjne-sieci-neuronowe-cnn","title":"Konwolucyjne sieci neuronowe (CNN)","text":"<p>Sieci konwolucyjne to specjalny typ sieci neuronowych zaprojektowany do wykrywania wzorc\u00f3w w danych przestrzennych, takich jak obrazy. Kluczowe jest w nich to, \u017ce pewne warto\u015bci (np. piksele obrazu) s\u0105 istotne w kontek\u015bcie swoich s\u0105siad\u00f3w.</p> <p></p>"},{"location":"03-neural-networks/#jak-dziaa-konwolucja","title":"Jak dzia\u0142a konwolucja?","text":"<p>Konwolucja to operacja matematyczna, kt\u00f3ra przesuwa filtr (kernel) po obrazie, wydobywaj\u0105c cechy, takie jak kraw\u0119dzie czy tekstury. Filtry ucz\u0105 si\u0119 odpowiednich wzorc\u00f3w podczas treningu.</p> <p></p> <p>Matematycznie operacja konwolucji mo\u017ce by\u0107 zapisana jako:</p>  (I * K)(x, y) = \\sum_m \\sum_n I(x-m, y-n) K(m, n)  <p>gdzie: - I \u2013 oryginalny obraz, - K \u2013 filtr (kernel), - x,y \u2013 wsp\u00f3\u0142rz\u0119dne piksela w wyniku.</p>"},{"location":"03-neural-networks/#funkcja-aktywacji-relu","title":"Funkcja aktywacji (ReLU)","text":""},{"location":"03-neural-networks/#max-pooling","title":"Max Pooling","text":"<p>Max pooling to operacja redukuj\u0105ca wymiar obrazu, zachowuj\u0105c najwa\u017cniejsze informacje. Wybiera najwy\u017csz\u0105 warto\u015b\u0107 z ka\u017cdego fragmentu obrazu, co pomaga w: - Redukcji liczby parametr\u00f3w modelu. - Zwi\u0119kszeniu odporno\u015bci na przesuni\u0119cia obrazu.</p> <p></p> <p>Matematycznie, dla regionu :</p>  P(x, y) = \\max_{(i,j) \\in R} I(i,j)  <p>CNN sk\u0142adaj\u0105 si\u0119 zazwyczaj z naprzemiennych warstw konwolucyjnych, poolingowych i w pe\u0142ni po\u0142\u0105czonych.</p> <p>Wizualizacja dzia\u0142ania sieci konwolucyjnej: https://poloclub.github.io/cnn-explainer/</p>"},{"location":"03-neural-networks/#przykadowa-architektura-sieci-konwolucyjnej-lenet","title":"Przyk\u0142adowa architektura sieci konwolucyjnej (LeNet)","text":""},{"location":"03-neural-networks/extra/","title":"Zadanie dodatkowe","text":"<ol> <li> <p>Wytrenuj w\u0142asn\u0105 konwolucyjn\u0105 sie\u0107 neuronow\u0105 na zbiorze danych CIFAR-10. Szczeg\u00f3\u0142y s\u0105 przedstawione w notatniku notebook.ipynb. </p> </li> <li> <p>Wykonaj wszystkie punkty zapisane w komentarzu na ko\u0144cu notatnika.</p> </li> <li> <p>Warunkiem koniecznym do zaliczenia zadania jest accuracy na zbiorze treningowym, walidacyjnym oraz testowym co najmniej 60%.</p> </li> <li> <p>Znajd\u017a 10 obrazk\u00f3w w Internecie (lub u\u017cyj w\u0142asnych) i dokonaj klasyfikacji obiekt\u00f3w na niej.</p> </li> <li> <p>Przygotuj aplikacj\u0119 UI w gradio lub streamlit dla tego modelu.</p> </li> <li> <p>Prze\u015blij notatnik, kod aplikacji oraz zrzuty ekranu dzia\u0142aj\u0105cej aplikacji.</p> </li> </ol>"},{"location":"03-neural-networks/homework/","title":"Praca domowa","text":"<p>\ud83d\udccc Zadanie 3 \u2013 Aplikacja z dzia\u0142aj\u0105cym modelem AI</p> <p>\ud83c\udfaf Cel zadania:</p> <p>Celem zadania jest przygotowanie dzia\u0142aj\u0105cej aplikacji z interfejsem u\u017cytkownika (opartym na Streamlit, Gradio lub innym podobnym narz\u0119dziu), kt\u00f3ra integruje rzeczywisty model AI i umo\u017cliwia interakcj\u0119 z nim. Model powinien generowa\u0107 odpowiedzi na podstawie wprowadzonych przez u\u017cytkownika danych. Celem tego zadania jest dalsza praca nad aplikacj\u0105 przygotowan\u0105 w Zadaniu 2.</p> <p>\ud83d\udcdd Elementy do przygotowania:</p> <ol> <li> <p>Aplikacja:</p> <ul> <li>Posiada finalny interfejs u\u017cytkownika (np. formularz, przyciski, inputy).</li> <li>Przyci\u015bni\u0119cie przycisku powoduje przes\u0142anie danych u\u017cytkownika do modelu AI i wy\u015bwietlenie jego odpowiedzi.</li> <li>Wykorzystuje rzeczywisty model AI (np. OpenAI GPT, Hugging Face, lokalny model ML itp.).</li> <li>Je\u015bli korzystacie z w\u0142asnego modelu, prosz\u0119 o przes\u0142anie plik\u00f3w umo\u017cliwiaj\u0105cych reprodukcj\u0119 wynik\u00f3w (skrypty, notatniki) oraz dodanie ich do repozytorium.</li> <li>Aplikacja powinna by\u0107 uruchamialna lokalnie i dzia\u0142a\u0107 bez problem\u00f3w (przyk\u0142ad: streamlit run app.py).</li> </ul> </li> <li> <p>Backlog projektu:</p> <ul> <li>Stw\u00f3rzcie nowy sprint w Waszym narz\u0119dziu do zarz\u0105dzania projektem.</li> <li>Zamknijcie co najmniej 2 User Stories, gdzie ka\u017cda User Story powinna by\u0107 przypisana do innej osoby.</li> </ul> </li> <li> <p>Dokumentacja:</p> <ul> <li>Uzupe\u0142nijcie plik README.md w repozytorium kodu o najnowsz\u0105 instrukcj\u0119 uruchomienia aplikacji oraz opis modelu AI.</li> </ul> </li> </ol> <p>\ud83d\udce4 Elementy do przes\u0142ania:</p> <ul> <li>Zrzut ekranu aplikacji dzia\u0142aj\u0105cej lokalnie \u2013 na zrzucie ekranu powinno by\u0107 widoczne, jak klikni\u0119cie przycisku powoduje wys\u0142anie zapytania do modelu i wy\u015bwietlenie odpowiedzi.</li> </ul> <p>\ud83c\udf99\ufe0f Elementy do pokazania na zaj\u0119ciach:</p> <ul> <li>Dzia\u0142aj\u0105ca aplikacja, kt\u00f3ra uruchamia si\u0119 lokalnie i pokazuje interakcj\u0119 u\u017cytkownika z modelem AI.</li> </ul>"},{"location":"03-neural-networks/cifar-10/notebook/","title":"Konwolucyjne sieci neuronowe","text":"In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n</pre> import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt import random In\u00a0[\u00a0]: Copied! <pre># for reproducibility\n\ndef set_seed(seed=42):\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \nset_seed(42)   \n</pre> # for reproducibility  def set_seed(seed=42):     tf.random.set_seed(seed)     np.random.seed(seed)     random.seed(seed)      set_seed(42)    In\u00a0[\u00a0]: Copied! <pre># check if GPU or TPU is available\n\ndef check_device():\n    print(\"Available devices:\")\n    print(tf.config.list_physical_devices())\n    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n    print(\"Num TPUs Available: \", len(tf.config.list_physical_devices('TPU')))\n    print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n\ncheck_device()\n</pre> # check if GPU or TPU is available  def check_device():     print(\"Available devices:\")     print(tf.config.list_physical_devices())     print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))     print(\"Num TPUs Available: \", len(tf.config.list_physical_devices('TPU')))     print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))  check_device() In\u00a0[\u00a0]: Copied! <pre># load cifar10 dataset\n\ndef load_cifar10():\n    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n    x_train = x_train.astype('float32') / 255.0\n    x_test = x_test.astype('float32') / 255.0\n    return (x_train, y_train), (x_test, y_test)\n\n(x_train, y_train), (x_test, y_test) = load_cifar10()\n</pre> # load cifar10 dataset  def load_cifar10():     (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()     x_train = x_train.astype('float32') / 255.0     x_test = x_test.astype('float32') / 255.0     return (x_train, y_train), (x_test, y_test)  (x_train, y_train), (x_test, y_test) = load_cifar10() In\u00a0[\u00a0]: Copied! <pre># create validation set from training data\n\ndef create_validation_set(x_train, y_train, val_size=0.1):\n    num_classes = len(np.unique(y_train))\n    val_indices = []\n    for i in range(num_classes):\n        class_indices = np.where(y_train == i)[0]\n        np.random.shuffle(class_indices)\n        val_count = int(len(class_indices) * val_size)\n        val_indices.extend(class_indices[:val_count])\n    x_val = x_train[val_indices]\n    y_val = y_train[val_indices]\n    x_train = np.delete(x_train, val_indices, axis=0)\n    y_train = np.delete(y_train, val_indices, axis=0)\n    return (x_train, y_train), (x_val, y_val)\n\n(x_train, y_train), (x_val, y_val) = create_validation_set(x_train, y_train, val_size=0.1)\nprint(\"New training data shape:\", x_train.shape, y_train.shape)\nprint(\"Validation data shape:\", x_val.shape, y_val.shape)\n</pre> # create validation set from training data  def create_validation_set(x_train, y_train, val_size=0.1):     num_classes = len(np.unique(y_train))     val_indices = []     for i in range(num_classes):         class_indices = np.where(y_train == i)[0]         np.random.shuffle(class_indices)         val_count = int(len(class_indices) * val_size)         val_indices.extend(class_indices[:val_count])     x_val = x_train[val_indices]     y_val = y_train[val_indices]     x_train = np.delete(x_train, val_indices, axis=0)     y_train = np.delete(y_train, val_indices, axis=0)     return (x_train, y_train), (x_val, y_val)  (x_train, y_train), (x_val, y_val) = create_validation_set(x_train, y_train, val_size=0.1) print(\"New training data shape:\", x_train.shape, y_train.shape) print(\"Validation data shape:\", x_val.shape, y_val.shape) In\u00a0[\u00a0]: Copied! <pre># visualize some samples from the dataset for each class\n\ndef visualize_samples(x, y):\n    plt.figure(figsize=(8, 4))\n    for i in range(10):\n        plt.subplot(2, 5, i + 1)\n        plt.imshow(x[y.flatten() == i][0])\n        plt.axis('off')\n    plt.show()\n    \nvisualize_samples(x_train, y_train)\n</pre> # visualize some samples from the dataset for each class  def visualize_samples(x, y):     plt.figure(figsize=(8, 4))     for i in range(10):         plt.subplot(2, 5, i + 1)         plt.imshow(x[y.flatten() == i][0])         plt.axis('off')     plt.show()      visualize_samples(x_train, y_train) In\u00a0[\u00a0]: Copied! <pre># create model\n\ndef create_model():\n    # Define a CNN model\n    model = keras.Sequential([\n        keras.layers.InputLayer(shape=(32, 32, 3)),\n        # 2-3 blocks of a convolutional layer and a pooling layer\n        # then flatten the output and add 2-3 dense layers\n    ])\n    \n    # Compile the model\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n</pre> # create model  def create_model():     # Define a CNN model     model = keras.Sequential([         keras.layers.InputLayer(shape=(32, 32, 3)),         # 2-3 blocks of a convolutional layer and a pooling layer         # then flatten the output and add 2-3 dense layers     ])          # Compile the model     model.compile(         optimizer='adam',         loss='categorical_crossentropy',         metrics=['accuracy']     )      return model In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># HOMEWORK\n\n# 1. Create a CNN model with 2-3 blocks of a convolutional layer and a pooling layer, then flatten the output and add 2-3 dense layers.\n# 2. Train the model on the training data and validate it on the validation data (expected accuracy on train, val and test &gt; 60%).\n# 3. Evaluate the model on the test data.\n# 4. Visualize the training and validation loss and accuracy.\n# 5. Save the model.\n# 6. Create UI to load the model and make predictions on new images (use gradio or streamlit).\n# 7. Use 10 images from the Internet (provide a link) or your own, but do do not use CIFAR10 dataset, and make predictions. \n#    Attention: the images should be of the same size as CIFAR10 (32x32) and in RGB format. You can use PIL or OpenCV to resize the images.\n# (additional task) 8.* Use a more sophisticated network architecture (e.g. ResNet, VGG, etc.) and compare the results with the simple CNN model.\n# (additional task) 9.* Use data augmentation to improve the model performance.\n</pre> # HOMEWORK  # 1. Create a CNN model with 2-3 blocks of a convolutional layer and a pooling layer, then flatten the output and add 2-3 dense layers. # 2. Train the model on the training data and validate it on the validation data (expected accuracy on train, val and test &gt; 60%). # 3. Evaluate the model on the test data. # 4. Visualize the training and validation loss and accuracy. # 5. Save the model. # 6. Create UI to load the model and make predictions on new images (use gradio or streamlit). # 7. Use 10 images from the Internet (provide a link) or your own, but do do not use CIFAR10 dataset, and make predictions.  #    Attention: the images should be of the same size as CIFAR10 (32x32) and in RGB format. You can use PIL or OpenCV to resize the images. # (additional task) 8.* Use a more sophisticated network architecture (e.g. ResNet, VGG, etc.) and compare the results with the simple CNN model. # (additional task) 9.* Use data augmentation to improve the model performance. In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"03-neural-networks/cifar-10/notebook/#konwolucyjne-sieci-neuronowe","title":"Konwolucyjne sieci neuronowe\u00b6","text":"<p>Klasyfikacja obrazk\u00f3w na 10 klas (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck) - zbi\u00f3r danych CIFAR10.</p> <p>https://keras.io/api/datasets/cifar10/</p>"},{"location":"03-neural-networks/mnist-10/app/","title":"App","text":"In\u00a0[\u00a0]: Copied! <pre>import gradio as gr\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\n</pre> import gradio as gr import numpy as np import tensorflow as tf from tensorflow.keras.models import load_model In\u00a0[\u00a0]: Copied! <pre>MODEL_PATH = \"mnist_model.keras\"\nmodel = load_model(MODEL_PATH)\n</pre> MODEL_PATH = \"mnist_model.keras\" model = load_model(MODEL_PATH) In\u00a0[\u00a0]: Copied! <pre># Predict function\ndef predict(image):\n    # Expand the dimensions of the image to match the input shape of the model\n    image = np.expand_dims(image, axis=0)\n    \n    # Make prediction\n    predictions = model.predict(image)\n    predicted_class = np.argmax(predictions, axis=1)[0]\n    \n    # Get the probabilities for each class\n    probabilities = predictions[0]\n    \n    return gr.update(value=str(predicted_class)), gr.update(value={str(i): float(prob) for i, prob in enumerate(probabilities)})\n</pre> # Predict function def predict(image):     # Expand the dimensions of the image to match the input shape of the model     image = np.expand_dims(image, axis=0)          # Make prediction     predictions = model.predict(image)     predicted_class = np.argmax(predictions, axis=1)[0]          # Get the probabilities for each class     probabilities = predictions[0]          return gr.update(value=str(predicted_class)), gr.update(value={str(i): float(prob) for i, prob in enumerate(probabilities)}) In\u00a0[\u00a0]: Copied! <pre># Gradio interface\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# MNIST Digit Classification\")\n    gr.Markdown(\"Draw a digit in the box below and click 'Predict' to see the result.\")\n    \n    with gr.Row():\n        with gr.Column():\n            image_input = gr.Image(type=\"numpy\", label=\"Input Image\", image_mode=\"L\")\n            predict_button = gr.Button(\"Predict from file\")\n            \n        with gr.Column():\n            predicted_class_output = gr.Textbox(label=\"Predicted Class\", interactive=False)\n            probabilities_output = gr.Label(label=\"Probabilities\")\n\n    predict_button.click(predict, inputs=image_input, outputs=[predicted_class_output, probabilities_output])\n</pre> # Gradio interface with gr.Blocks() as demo:     gr.Markdown(\"# MNIST Digit Classification\")     gr.Markdown(\"Draw a digit in the box below and click 'Predict' to see the result.\")          with gr.Row():         with gr.Column():             image_input = gr.Image(type=\"numpy\", label=\"Input Image\", image_mode=\"L\")             predict_button = gr.Button(\"Predict from file\")                      with gr.Column():             predicted_class_output = gr.Textbox(label=\"Predicted Class\", interactive=False)             probabilities_output = gr.Label(label=\"Probabilities\")      predict_button.click(predict, inputs=image_input, outputs=[predicted_class_output, probabilities_output]) In\u00a0[\u00a0]: Copied! <pre># Launch the app\nif __name__ == \"__main__\":\n    demo.launch()\n</pre> # Launch the app if __name__ == \"__main__\":     demo.launch()"},{"location":"03-neural-networks/mnist-10/app_md/","title":"Kod aplikacji gradio","text":"<pre><code>import gradio as gr\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\n\n\nMODEL_PATH = \"mnist_model.keras\"\nmodel = load_model(MODEL_PATH)\n\n\n# Predict function\ndef predict(image):\n    # Expand the dimensions of the image to match the input shape of the model\n    image = np.expand_dims(image, axis=0)\n\n    # Make prediction\n    predictions = model.predict(image)\n    predicted_class = np.argmax(predictions, axis=1)[0]\n\n    # Get the probabilities for each class\n    probabilities = predictions[0]\n\n    return gr.update(value=str(predicted_class)), gr.update(value={str(i): float(prob) for i, prob in enumerate(probabilities)})\n\n\n# Gradio interface\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# MNIST Digit Classification\")\n    gr.Markdown(\"Draw a digit in the box below and click 'Predict' to see the result.\")\n\n    with gr.Row():\n        with gr.Column():\n            image_input = gr.Image(type=\"numpy\", label=\"Input Image\", image_mode=\"L\")\n            predict_button = gr.Button(\"Predict from file\")\n\n        with gr.Column():\n            predicted_class_output = gr.Textbox(label=\"Predicted Class\", interactive=False)\n            probabilities_output = gr.Label(label=\"Probabilities\")\n\n    predict_button.click(predict, inputs=image_input, outputs=[predicted_class_output, probabilities_output])\n\n\n# Launch the app\nif __name__ == \"__main__\":\n    demo.launch()\n</code></pre>"},{"location":"03-neural-networks/mnist-10/notebook/","title":"Wst\u0119p do sieci neuronowych","text":"In\u00a0[1]: Copied! <pre>import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport os\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n</pre> import tensorflow as tf from tensorflow import keras import numpy as np import matplotlib.pyplot as plt import random import os from sklearn.metrics import confusion_matrix import seaborn as sns <pre>2025-04-03 18:23:05.311182: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-04-03 18:23:05.332774: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1743697385.369485  255532 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1743697385.380640  255532 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1743697385.407202  255532 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743697385.407242  255532 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743697385.407244  255532 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1743697385.407245  255532 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-04-03 18:23:05.415269: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n</pre> In\u00a0[\u00a0]: Copied! <pre># for reproducibility\n\ndef set_seed(seed=42):\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \nset_seed(42)\n</pre> # for reproducibility  def set_seed(seed=42):     tf.random.set_seed(seed)     np.random.seed(seed)     random.seed(seed)      set_seed(42) In\u00a0[3]: Copied! <pre># check if GPU or TPU is available\n\ndef check_device():\n    print(\"Available devices:\")\n    print(tf.config.list_physical_devices())\n    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n    print(\"Num TPUs Available: \", len(tf.config.list_physical_devices('TPU')))\n    print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n\ncheck_device()\n</pre> # check if GPU or TPU is available  def check_device():     print(\"Available devices:\")     print(tf.config.list_physical_devices())     print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))     print(\"Num TPUs Available: \", len(tf.config.list_physical_devices('TPU')))     print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))  check_device() <pre>Available devices:\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nNum GPUs Available:  1\nNum TPUs Available:  0\nNum CPUs Available:  1\n</pre> In\u00a0[4]: Copied! <pre># load the MNIST dataset\n\ndef load_mnist():\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n    x_train = x_train.astype('float32') / 255.0\n    x_test = x_test.astype('float32') / 255.0\n    return (x_train, y_train), (x_test, y_test)\n\n(x_train, y_train), (x_test, y_test) = load_mnist()\nprint(\"Training data shape:\", x_train.shape, y_train.shape)\nprint(\"Testing data shape:\", x_test.shape, y_test.shape)\n</pre> # load the MNIST dataset  def load_mnist():     (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()     x_train = x_train.astype('float32') / 255.0     x_test = x_test.astype('float32') / 255.0     return (x_train, y_train), (x_test, y_test)  (x_train, y_train), (x_test, y_test) = load_mnist() print(\"Training data shape:\", x_train.shape, y_train.shape) print(\"Testing data shape:\", x_test.shape, y_test.shape)  <pre>Training data shape: (60000, 28, 28) (60000,)\nTesting data shape: (10000, 28, 28) (10000,)\n</pre> In\u00a0[5]: Copied! <pre># create validation set from training data\n\ndef create_validation_set(x_train, y_train, val_size=0.1):\n    num_classes = len(np.unique(y_train))\n    val_indices = []\n    for i in range(num_classes):\n        class_indices = np.where(y_train == i)[0]\n        np.random.shuffle(class_indices)\n        val_count = int(len(class_indices) * val_size)\n        val_indices.extend(class_indices[:val_count])\n    x_val = x_train[val_indices]\n    y_val = y_train[val_indices]\n    x_train = np.delete(x_train, val_indices, axis=0)\n    y_train = np.delete(y_train, val_indices, axis=0)\n    return (x_train, y_train), (x_val, y_val)\n\n(x_train, y_train), (x_val, y_val) = create_validation_set(x_train, y_train, val_size=0.1)\nprint(\"New training data shape:\", x_train.shape, y_train.shape)\nprint(\"Validation data shape:\", x_val.shape, y_val.shape)\n</pre> # create validation set from training data  def create_validation_set(x_train, y_train, val_size=0.1):     num_classes = len(np.unique(y_train))     val_indices = []     for i in range(num_classes):         class_indices = np.where(y_train == i)[0]         np.random.shuffle(class_indices)         val_count = int(len(class_indices) * val_size)         val_indices.extend(class_indices[:val_count])     x_val = x_train[val_indices]     y_val = y_train[val_indices]     x_train = np.delete(x_train, val_indices, axis=0)     y_train = np.delete(y_train, val_indices, axis=0)     return (x_train, y_train), (x_val, y_val)  (x_train, y_train), (x_val, y_val) = create_validation_set(x_train, y_train, val_size=0.1) print(\"New training data shape:\", x_train.shape, y_train.shape) print(\"Validation data shape:\", x_val.shape, y_val.shape) <pre>New training data shape: (54004, 28, 28) (54004,)\nValidation data shape: (5996, 28, 28) (5996,)\n</pre> In\u00a0[6]: Copied! <pre># use a subset of the data for faster training\n\n# def use_subset(x, y, num_samples=1000):\n#     indices = np.random.choice(len(x), num_samples, replace=False)\n#     return x[indices], y[indices]\n\n# x_train, y_train = use_subset(x_train, y_train, num_samples=1000)\n# x_val, y_val = use_subset(x_val, y_val, num_samples=200)\n# x_test, y_test = use_subset(x_test, y_test, num_samples=200)\n\n# print(\"Subset training data shape:\", x_train.shape, y_train.shape)\n# print(\"Subset validation data shape:\", x_val.shape, y_val.shape)\n# print(\"Subset testing data shape:\", x_test.shape, y_test.shape)\n</pre> # use a subset of the data for faster training  # def use_subset(x, y, num_samples=1000): #     indices = np.random.choice(len(x), num_samples, replace=False) #     return x[indices], y[indices]  # x_train, y_train = use_subset(x_train, y_train, num_samples=1000) # x_val, y_val = use_subset(x_val, y_val, num_samples=200) # x_test, y_test = use_subset(x_test, y_test, num_samples=200)  # print(\"Subset training data shape:\", x_train.shape, y_train.shape) # print(\"Subset validation data shape:\", x_val.shape, y_val.shape) # print(\"Subset testing data shape:\", x_test.shape, y_test.shape) In\u00a0[7]: Copied! <pre># visualize some samples from the dataset for each class\n\ndef visualize_samples(x, y):\n    plt.figure(figsize=(10, 10))\n    for i in range(10):\n        plt.subplot(5, 5, i + 1)\n        plt.imshow(x[y == i][0], cmap='gray')\n        plt.title(f\"Label: {i}\")\n        plt.axis('off')\n    plt.show()\n    \nvisualize_samples(x_train, y_train)\n</pre> # visualize some samples from the dataset for each class  def visualize_samples(x, y):     plt.figure(figsize=(10, 10))     for i in range(10):         plt.subplot(5, 5, i + 1)         plt.imshow(x[y == i][0], cmap='gray')         plt.title(f\"Label: {i}\")         plt.axis('off')     plt.show()      visualize_samples(x_train, y_train) In\u00a0[8]: Copied! <pre># save one image per class to a directory\n\ndef save_samples(x, y, save_dir='samples'):\n    os.makedirs(save_dir, exist_ok=True)\n    for i in range(10):\n        class_indices = np.where(y == i)[0]\n        sample_image = x[class_indices[0]]\n        plt.imsave(os.path.join(save_dir, f'class_{i}.png'), sample_image, cmap='gray')\n        \nsave_samples(x_train, y_train)\n</pre> # save one image per class to a directory  def save_samples(x, y, save_dir='samples'):     os.makedirs(save_dir, exist_ok=True)     for i in range(10):         class_indices = np.where(y == i)[0]         sample_image = x[class_indices[0]]         plt.imsave(os.path.join(save_dir, f'class_{i}.png'), sample_image, cmap='gray')          save_samples(x_train, y_train) In\u00a0[9]: Copied! <pre># create model\n\ndef create_model():\n    # Define a simple feedforward neural network\n    model = keras.Sequential([\n        keras.layers.InputLayer(shape=(28, 28)),\n        keras.layers.Flatten(),\n        keras.layers.Dense(128),\n        keras.layers.ReLU(),\n        keras.layers.Dense(32),\n        keras.layers.ReLU(),\n        keras.layers.Dense(10),\n        keras.layers.Softmax()\n    ])\n    \n    # Compile the model\n    model.compile(\n        optimizer='adam',\n        loss='sparse_categorical_crossentropy',\n        metrics=['accuracy']\n    )\n\n    return model\n</pre> # create model  def create_model():     # Define a simple feedforward neural network     model = keras.Sequential([         keras.layers.InputLayer(shape=(28, 28)),         keras.layers.Flatten(),         keras.layers.Dense(128),         keras.layers.ReLU(),         keras.layers.Dense(32),         keras.layers.ReLU(),         keras.layers.Dense(10),         keras.layers.Softmax()     ])          # Compile the model     model.compile(         optimizer='adam',         loss='sparse_categorical_crossentropy',         metrics=['accuracy']     )      return model In\u00a0[10]: Copied! <pre>model = create_model()\nmodel.summary()\n</pre> model = create_model() model.summary() <pre>I0000 00:00:1743697390.467528  255532 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3620 MB memory:  -&gt; device: 0, name: NVIDIA RTX A1000 6GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n</pre> <pre>Model: \"sequential\"\n</pre> <pre>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Layer (type)                    \u2503 Output Shape           \u2503       Param # \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 flatten (Flatten)               \u2502 (None, 784)            \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense (Dense)                   \u2502 (None, 128)            \u2502       100,480 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 re_lu (ReLU)                    \u2502 (None, 128)            \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_1 (Dense)                 \u2502 (None, 32)             \u2502         4,128 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 re_lu_1 (ReLU)                  \u2502 (None, 32)             \u2502             0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 dense_2 (Dense)                 \u2502 (None, 10)             \u2502           330 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 softmax (Softmax)               \u2502 (None, 10)             \u2502             0 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</pre> <pre> Total params: 104,938 (409.91 KB)\n</pre> <pre> Trainable params: 104,938 (409.91 KB)\n</pre> <pre> Non-trainable params: 0 (0.00 B)\n</pre> In\u00a0[11]: Copied! <pre>def train_model(model, x_train, y_train, x_val, y_val, epochs=10):\n    history = model.fit(\n        x_train,\n        y_train,\n        validation_data=(x_val, y_val),\n        epochs=epochs,\n        batch_size=32,\n        verbose=1\n    )\n    return history\n</pre> def train_model(model, x_train, y_train, x_val, y_val, epochs=10):     history = model.fit(         x_train,         y_train,         validation_data=(x_val, y_val),         epochs=epochs,         batch_size=32,         verbose=1     )     return history In\u00a0[12]: Copied! <pre># train the model\n\nhistory = train_model(model, x_train, y_train, x_val, y_val, epochs=5)\n</pre> # train the model  history = train_model(model, x_train, y_train, x_val, y_val, epochs=5) <pre>Epoch 1/5\n</pre> <pre>WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1743697392.472925  255690 service.cc:152] XLA service 0x7f71780043c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1743697392.472978  255690 service.cc:160]   StreamExecutor device (0): NVIDIA RTX A1000 6GB Laptop GPU, Compute Capability 8.6\n2025-04-03 18:23:12.493894: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\nI0000 00:00:1743697392.611316  255690 cuda_dnn.cc:529] Loaded cuDNN version 90300\n</pre> <pre>  41/1688 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 6s 4ms/step - accuracy: 0.3902 - loss: 1.9273</pre> <pre>I0000 00:00:1743697394.838480  255690 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n</pre> <pre>1688/1688 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 14s 6ms/step - accuracy: 0.8648 - loss: 0.4695 - val_accuracy: 0.9576 - val_loss: 0.1487\nEpoch 2/5\n1688/1688 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 9s 5ms/step - accuracy: 0.9617 - loss: 0.1275 - val_accuracy: 0.9670 - val_loss: 0.1087\nEpoch 3/5\n1688/1688 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 4ms/step - accuracy: 0.9748 - loss: 0.0833 - val_accuracy: 0.9723 - val_loss: 0.0969\nEpoch 4/5\n1688/1688 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 8s 5ms/step - accuracy: 0.9805 - loss: 0.0600 - val_accuracy: 0.9736 - val_loss: 0.0938\nEpoch 5/5\n1688/1688 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 7s 4ms/step - accuracy: 0.9869 - loss: 0.0435 - val_accuracy: 0.9740 - val_loss: 0.0967\n</pre> In\u00a0[13]: Copied! <pre># plot with history loss and accuracy\n\ndef plot_history(history):\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='train_loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.title('Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.xticks(np.arange(0, len(history.history['loss']), 1))\n    plt.legend()\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['accuracy'], label='train_accuracy')\n    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.xticks(np.arange(0, len(history.history['accuracy']), 1))\n    plt.legend()\n    \nplt.figure(figsize=(12, 4))\nplot_history(history)\nplt.show()\n</pre> # plot with history loss and accuracy  def plot_history(history):     plt.subplot(1, 2, 1)     plt.plot(history.history['loss'], label='train_loss')     plt.plot(history.history['val_loss'], label='val_loss')     plt.title('Loss')     plt.xlabel('Epoch')     plt.ylabel('Loss')     plt.xticks(np.arange(0, len(history.history['loss']), 1))     plt.legend()          plt.subplot(1, 2, 2)     plt.plot(history.history['accuracy'], label='train_accuracy')     plt.plot(history.history['val_accuracy'], label='val_accuracy')     plt.title('Accuracy')     plt.xlabel('Epoch')     plt.ylabel('Accuracy')     plt.xticks(np.arange(0, len(history.history['accuracy']), 1))     plt.legend()      plt.figure(figsize=(12, 4)) plot_history(history) plt.show() In\u00a0[14]: Copied! <pre>def evaluate_model(model, x, y):\n    loss, accuracy = model.evaluate(x, y, verbose=0)\n    return loss, accuracy\n\ndef show_accuracy_table(model, x_train, y_train, x_val, y_val, x_test, y_test):\n    train_loss, train_accuracy = evaluate_model(model, x_train, y_train)\n    val_loss, val_accuracy = evaluate_model(model, x_val, y_val)\n    test_loss, test_accuracy = evaluate_model(model, x_test, y_test)\n\n    print(f\"{'Dataset':&lt;15} {'Loss':&lt;10} {'Accuracy':&lt;10}\")\n    print(\"-\" * 35)\n    print(f\"{'Train':&lt;15} {train_loss:&lt;10.4f} {train_accuracy:&lt;10.4f}\")\n    print(f\"{'Validation':&lt;15} {val_loss:&lt;10.4f} {val_accuracy:&lt;10.4f}\")\n    print(f\"{'Test':&lt;15} {test_loss:&lt;10.4f} {test_accuracy:&lt;10.4f}\")\n    \nshow_accuracy_table(model, x_train, y_train, x_val, y_val, x_test, y_test)\n</pre> def evaluate_model(model, x, y):     loss, accuracy = model.evaluate(x, y, verbose=0)     return loss, accuracy  def show_accuracy_table(model, x_train, y_train, x_val, y_val, x_test, y_test):     train_loss, train_accuracy = evaluate_model(model, x_train, y_train)     val_loss, val_accuracy = evaluate_model(model, x_val, y_val)     test_loss, test_accuracy = evaluate_model(model, x_test, y_test)      print(f\"{'Dataset':&lt;15} {'Loss':&lt;10} {'Accuracy':&lt;10}\")     print(\"-\" * 35)     print(f\"{'Train':&lt;15} {train_loss:&lt;10.4f} {train_accuracy:&lt;10.4f}\")     print(f\"{'Validation':&lt;15} {val_loss:&lt;10.4f} {val_accuracy:&lt;10.4f}\")     print(f\"{'Test':&lt;15} {test_loss:&lt;10.4f} {test_accuracy:&lt;10.4f}\")      show_accuracy_table(model, x_train, y_train, x_val, y_val, x_test, y_test) <pre>Dataset         Loss       Accuracy  \n-----------------------------------\nTrain           0.0332     0.9886    \nValidation      0.0967     0.9740    \nTest            0.0966     0.9721    \n</pre> In\u00a0[15]: Copied! <pre># show conflusion matrix on test set\n\ndef plot_confusion_matrix(y_true, y_pred, classes):\n    cm = confusion_matrix(y_true, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes, linecolor='black', linewidths=0.5)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title('Confusion Matrix')\n\nplt.figure(figsize=(8, 6))\nplot_confusion_matrix(y_test, model.predict(x_test).argmax(axis=1), classes=np.arange(10))\nplt.show()\n</pre> # show conflusion matrix on test set  def plot_confusion_matrix(y_true, y_pred, classes):     cm = confusion_matrix(y_true, y_pred)     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes, linecolor='black', linewidths=0.5)     plt.xlabel('Predicted')     plt.ylabel('True')     plt.title('Confusion Matrix')  plt.figure(figsize=(8, 6)) plot_confusion_matrix(y_test, model.predict(x_test).argmax(axis=1), classes=np.arange(10)) plt.show() <pre>313/313 \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1s 2ms/step\n</pre> In\u00a0[16]: Copied! <pre># save the model\n\ndef save_model(model, model_name):\n    model.save(model_name)\n    print(f\"Model saved as {model_name}\")\n\nsave_model(model, model_name='mnist_model.keras')\n</pre> # save the model  def save_model(model, model_name):     model.save(model_name)     print(f\"Model saved as {model_name}\")  save_model(model, model_name='mnist_model.keras') <pre>Model saved as mnist_model.keras\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"03-neural-networks/mnist-10/notebook/#wstep-do-sieci-neuronowych","title":"Wst\u0119p do sieci neuronowych\u00b6","text":"<p>Klasyfikacja obrazk\u00f3w MNIST-10 (czarno-bia\u0142e zdj\u0119cia r\u0119cznie pisanych cyfr).</p>"},{"location":"04-docker/","title":"Docker","text":""},{"location":"04-docker/#czym-jest-docker","title":"Czym jest docker?","text":"<p>Docker to narz\u0119dzie do tworzenia, dystrybucji i uruchamiania aplikacji w tzw. kontenerach. Kontener to lekka, przeno\u015bna jednostka, kt\u00f3ra zawiera aplikacj\u0119 wraz z ca\u0142ym jej \u015brodowiskiem \u2013 bibliotekami, zale\u017cno\u015bciami, systemem plik\u00f3w. Kontenery wraz z zawarto\u015bci\u0105 dzia\u0142aj\u0105 niezale\u017cnie od siebie i nie wiedz\u0105 o swoim istnieniu. Mog\u0105 si\u0119 jednak ze sob\u0105 komunikowa\u0107 w ramach \u015bci\u015ble zdefiniowanych kana\u0142\u00f3w wymiany informacji. Konteneryzacja jest znacznie l\u017cejszym sposobem uruchamiania aplikacji ni\u017c pe\u0142na wirtualizacja.</p> <p>Strona projektu: https://www.docker.com/</p> <p>Dokumentacja: https://docs.docker.com/</p>"},{"location":"04-docker/#rozne-sposoby-wirtualizacji","title":"R\u00f3\u017cne sposoby wirtualizacji","text":"Cecha Bare-metal Virtual Machine Docker Container Python Virtualenv Izolacja Brak Pe\u0142ny system operacyjny Lekka (na poziomie procesu) Tylko dependencies Pythona Narzut zasob\u00f3w Brak Du\u017cy (ca\u0142y OS) Niski (wsp\u00f3\u0142dzielony kernel) Minimalny Czas uruchomienia natychmiast sekundy-minuty milisekundy natychmiast Zakres izolacji Pe\u0142ny system Ca\u0142y system operacyjny App + \u015brodowisko Tylko Python Przeno\u015bno\u015b\u0107 \u015brodowiska Niska \u015arednia Wysoka (obraz) Niska U\u017cycie w produkcji Nie Czasem Tak Nie Cecha Bare Metal Virtual Machines (VMs) Containers Opis Fizyczny serwer z OS, uruchamiaj\u0105cy aplikacje bez warstwy wirtualizacji Wirtualna maszyna z w\u0142asnym systemem operacyjnym Samodzielna jednostka z aplikacj\u0105 + zale\u017cno\u015bci Warstwa wirtualizacji Brak (bezpo\u015brednio na sprz\u0119cie) Hypervisor Silnik kontenerowy (np. Docker) Wydajno\u015b\u0107 \u2705 Najwy\u017csza \u2013 bezpo\u015bredni dost\u0119p do sprz\u0119tu \u26a0\ufe0f Dobra, ale z narzutem przez hypervisor \u2705 Bardzo dobra \u2013 minimalny narzut Izolacja \u2705 Przy jednej aplikacji \u274c Dla wielu aplikacji na jednym sprz\u0119cie \u2705 Wysoka \u2013 ka\u017cdy VM ma osobny OS i kernel \u26a0\ufe0f Umiarkowana \u2013 wsp\u00f3\u0142dzielony kernel OS Zarz\u0105dzanie \u274c Trudniejsze \u2013 fizyczna konserwacja, konfiguracja \u2705 Narz\u0119dzia do zarz\u0105dzania VMs (np. Proxmox) \u2705 \u0141atwe \u2013 CI/CD, orchestracja (Docker, K8s) Skalowalno\u015b\u0107 \u274c Ograniczona \u2013 wymaga nowych serwer\u00f3w fizycznych \u2705 Dobra \u2013 szybkie tworzenie nowych VMs \u2705 Doskona\u0142a \u2013 natychmiastowe skalowanie Elastyczno\u015b\u0107 \u274c Ma\u0142a \u2013 zmiany wymagaj\u0105 rekonstrukcji OS/sprz\u0119tu \u2705 Wysoka \u2013 r\u00f3\u017cne OSy i konfiguracje \u2705 Bardzo wysoka \u2013 \u0142atwa migracja i przeno\u015bno\u015b\u0107 Bezpiecze\u0144stwo \u2705 Wysokie (przy jednej aplikacji na serwer)  \u26a0\ufe0f Ograniczone (wiele aplikacji wsp\u00f3\u0142dzieli system) \u2705 Wysokie \u2013 silna izolacja mi\u0119dzy VMs \u26a0\ufe0f Ni\u017csze \u2013 wsp\u00f3\u0142dzielony kernel, mo\u017cliwe luki w izolacji Przeno\u015bno\u015b\u0107 (portability) \u274c Niska \u2013 zale\u017cno\u015b\u0107 od sprz\u0119tu i OS \u26a0\ufe0f Umiarkowana \u2013 zale\u017cna od platformy hypervisora \u2705 Wysoka \u2013 kontener = uniwersalne \u015brodowisko uruch. Typowe zastosowania HPC, aplikacje z wysokimi wymaganiami sprz\u0119towymi Chmura prywatna/publiczna, legacy, systemy OS-specyficzne Mikroserwisy, dev/test, aplikacje cloud-native (tworzone specjalnie z my\u015bl\u0105 o chmurze) Najwi\u0119ksza zaleta Wydajno\u015b\u0107 + pe\u0142na kontrola nad sprz\u0119tem Izolacja + elastyczno\u015b\u0107 systemowa Lekko\u015b\u0107 + przeno\u015bno\u015b\u0107 + szybki czas wdro\u017cenia Najwi\u0119ksza wada Brak elastyczno\u015bci, problemy ze skalowaniem Narzut wydajno\u015bci + z\u0142o\u017cono\u015b\u0107 zarz\u0105dzania Bezpiecze\u0144stwo + z\u0142o\u017cone zarz\u0105dzanie \u015brodowiskiem"},{"location":"04-docker/#elementy-dockera","title":"Elementy dockera","text":"<ul> <li> <p>image (obraz) - niezmienny, samodzielny pakiet zawieraj\u0105cy wszystko, co potrzebne do uruchomienia aplikacji: kod, zale\u017cno\u015bci, konfiguracj\u0119 \u015brodowiska, itd. Obrazy s\u0105 baz\u0105 do tworzenia kontener\u00f3w.</p> </li> <li> <p>container (kontener) - uruchomiona instancja obrazu. To izolowane \u015brodowisko dzia\u0142aj\u0105ce niezale\u017cnie od systemu hosta. Kontener ma w\u0142asny system plik\u00f3w, sie\u0107, procesy, ale korzysta z j\u0105dra hosta. Mo\u017cemy uruchomi\u0107 kilka kontener\u00f3w na podstawie tego samego obrazu.</p> </li> <li> <p>network (sie\u0107) - wirtualna sie\u0107 tworzona przez Dockera, umo\u017cliwiaj\u0105ca komunikacj\u0119 mi\u0119dzy kontenerami.</p> </li> <li> <p>volume -  trwa\u0142y magazyn danych. Pozwala przechowywa\u0107 dane poza kontenerem (np. pliki, bazy danych), tak aby nie znikn\u0119\u0142y po jego usuni\u0119ciu. Przydatne do backup\u00f3w i pracy z danymi mi\u0119dzy restartami.</p> </li> </ul>"},{"location":"04-docker/#podstawowe-komendy-docker","title":"Podstawowe komendy docker","text":"<pre><code>docker pull &lt;nazwa_obrazu&gt;              # Pobiera obraz z rejestru\ndocker build -t &lt;tag&gt; .                 # Buduje obraz z Dockerfile z nadaniem nazwy\ndocker run -p 8080:80 &lt;obraz&gt;           # Uruchamia kontener z mapowaniem port\u00f3w\ndocker ps                               # Pokazuje dzia\u0142aj\u0105ce kontenery\ndocker stop &lt;container_id&gt;              # Zatrzymuje kontener\ndocker exec -it &lt;nazwa_kontenera&gt; bash  # interaktywna sesja wewn\u0105trz kontenera\ndocker system prune -a                  # usuwanie wszystkich zasob\u00f3w dockera\n</code></pre> <p>Dodatkowo, wiele operacji ma nast\u0119puj\u0105c\u0105 konstrukcj\u0119:</p> <pre><code>docker &lt;element&gt; &lt;komenda&gt; &lt;argumenty&gt;\n</code></pre> <p>Na przyk\u0142ad:</p> <pre><code>docker container ls\ndocker container logs nginx\ndocker network rm example-network\n</code></pre> <p>Po wi\u0119cej informacji wystarczy wpisa\u0107 w bashu komend\u0119 <code>docker</code>.</p>"},{"location":"04-docker/#dockerfile","title":"Dockerfile","text":"<p>Plik konfiguracyjny, kt\u00f3ry opisuje, jak zbudowa\u0107 obraz Dockera. Sk\u0142adnia jest prosta, oparta na komendach:</p> <ul> <li>FROM - definuje bazowy obraz</li> <li>RUN - wykonuje konkretne polecenie w trakcie nudowania obrazu (np. <code>apt-get update</code>)</li> <li>COPY - kopiuje pliki do obrazu</li> <li>WORKDIR - ustawianie katalogu roboczego</li> <li>ENTRYPOINT - definiuje co ma by\u0107 wywo\u0142ane przy starcie kontenera</li> <li>CMD - definiuje co ma by\u0107 wywo\u0142ane przy starcie kontenera; mo\u017cna natomiast t\u0119 komend\u0119 zastapi\u0107 w trakcie uruchamiania kontenera przez <code>docker run &lt;args&gt;</code>.</li> <li>EXPOSE - definuje, kt\u00f3re porty udost\u0119pnia obraz; nie jest wymagany nawet je\u015bli jakie\u015b porty s\u0105 otwarte; jest to raczej metadana obrazu</li> </ul> <p>CMD vs. ENTRYPOINT</p> <ol> <li>U\u017cywamy CMD</li> </ol> <pre><code>CMD [\"python\", \"app.py\"]\n</code></pre> <p>Wtedy:</p> <pre><code>docker run myapp                 # uruchomi python app.py\ndocker run myapp echo hello      # nadpisze CMD, uruchomi `echo hello`\n</code></pre> <ol> <li>U\u017cywamy ENTRYPOINT</li> </ol> <pre><code>ENTRYPOINT [\"python\", \"app.py\"]\n</code></pre> <p>Wtedy:</p> <pre><code>docker run myapp                # uruchomi python app.py\ndocker run myapp dev            # uruchomi python app.py z argumentem `dev`\n</code></pre>"},{"location":"04-docker/#budowanie-i-uruchamianie-kontenera-w-praktyce","title":"Budowanie i uruchamianie kontenera w praktyce","text":"<ol> <li>Przejd\u017a do folderu <code>example</code></li> <li>Uruchom <code>docker build -t simple-website:v1 .</code></li> <li>Po wywo\u0142aniu <code>docker images</code> powinni\u015bmy widzie\u0107 w\u0142a\u015bnie zbudowany obraz.</li> <li>Uruchamiamy obraz mapuj\u0105c port 80 kontenera na port 8080 naszego komputera <code>docker run -p 8080:80 simple-website:v1</code>.</li> </ol> <p>Dodatkowe parametry do uruchomienia kontenera</p> <pre><code>docker run \\\n  --name my-website \\                               # nadaje nazw\u0119 kontenerowi\n  -p 8080:80 \\                                      # mapowanie port\u00f3w\n  --network my-custom-network \\                     # pod\u0142\u0105czenie kontenera do istniejacej sieci\n  -v my-website-volume:/var/log/nginx  \\            # montuje wolumen do \u015bcie\u017cki w kontenerze\n  -v /home/user/css:/usr/share/nginx/css \\          # montuje wolumen w konkretnym miejscu na dysku do \u015bcie\u017cki w kontenerze\n  -e ENVIRONMENT=production \\                       # ustawia zmienn\u0105 \u015brodowiskow\u0105\n  simple-website:v1\n</code></pre>"},{"location":"04-docker/#docker-registry","title":"Docker registry","text":"<p>Docker Registry to serwer, kt\u00f3ry przechowuje obrazy Dockera. Mo\u017cna go por\u00f3wna\u0107 do \"repozytorium pakiet\u00f3w\", ale dla ca\u0142ych \u015brodowisk/aplikacji zamkni\u0119tych w obrazach kontenerowych.</p> <p>Jak to dzia\u0142a?</p> <ol> <li>Budujesz obraz lokalnie (docker build)</li> <li>Tagujesz go (docker tag)</li> <li>Pushujesz do registry (docker push)</li> <li>Inni (lub Ty sam) mog\u0105 go \u015bci\u0105gn\u0105\u0107 na innej maszynie (docker pull)</li> </ol> <p>Przyk\u0142adowe rejestry:</p> Rodzaj Opis Docker Hub domy\u015blne publiczne registry Dockera (https://hub.docker.com) Azure Container Registry (ACR) prywatne registry w chmurze Microsoft Azure GitHub Container Registry (GHCR) obrazy hostowane na GitHubie (ghcr.io) Self-hosted Registry mo\u017cesz postawi\u0107 w\u0142asne registry (registry:2 jako kontener!) <p>Python na Docker Hub: https://hub.docker.com/_/python</p> <p>Tutorial o kontenerach na Azure: https://learn.microsoft.com/en-us/training/paths/az-204-implement-iaas-solutions/</p>"},{"location":"04-docker/#dodatkowe-narzedzia","title":"Dodatkowe narz\u0119dzia","text":"<ol> <li>Docker Compose</li> <li>Docker Compose to narz\u0119dzie do definiowania i uruchamiania aplikacji z\u0142o\u017conych z wielu kontener\u00f3w.</li> <li>Konfiguracja w pliku docker-compose.yml (YAML)</li> <li>Wszystkie us\u0142ugi (np. frontend, backend, baza danych) uruchamiane jednym poleceniem</li> <li>Przydatne na etapie lokalnego developmentu</li> </ol> <p>Przydatne komendy:   <pre><code>docker compose up       # uruchomienie aplikacji\ndocker compose down     # zatrzymanie i usuni\u0119cie kontener\u00f3w\n</code></pre></p> <ol> <li>Docker Swarm</li> <li>Docker Swarm to system do tworzenia klastr\u00f3w kontener\u00f3w \u2013 grupuje wiele maszyn w jedn\u0105 logiczn\u0105 ca\u0142o\u015b\u0107.</li> <li>Narz\u0119dzie jest wbudowane w dockera.</li> </ol> <p>Przydatne komendy:   <pre><code>docker swarm init                                     # uruchomienie trybu Swarm\ndocker node ls                                        # lista w\u0119z\u0142\u00f3w w klastrze\ndocker service create nginx                           # uruchomienie us\u0142ugi\ndocker swarm join-token worker                        # generowanie tokena, kt\u00f3ry umo\u017cliwia pod\u0142\u0105czenie si\u0119 worker\u00f3w\ndocker swarm join --token &lt;token&gt; 192.168.1.100:2377  # pod\u0142\u0105czenie workera do maszyny master\n</code></pre></p> <ol> <li>Kubernetes</li> <li>Kubernetes (w skr\u00f3cie: K8s) to zaawansowana platforma do zarz\u0105dzania kontenerami w klastrze.</li> <li>Automatyzuje deployment, skalowanie, load balancing</li> <li>Obs\u0142uguje roll-outy, roll-backi, samonaprawianie us\u0142ug</li> <li>U\u017cywa w\u0142asnych obiekt\u00f3w: Pod, Service, Deployment, itp.</li> <li>Najmniejsz\u0105 jednostk\u0105 jest Pod. Zawiera on jeden lub wi\u0119cej kontener\u00f3w, kt\u00f3re wsp\u00f3\u0142dziel\u0105 sie\u0107 i volumes. W praktyce najcz\u0119sciej 1 Pod = 1 kontener. Mo\u017cna dodatkowo okre\u015bla\u0107 jakie zasoby s\u0105 przydzielane do ka\u017cdego kontenera, a Kubernetes w spos\u00f3b optymalny uruchomi kontenery na odpowiednich node'ach.</li> </ol> <p>Przydatne komendy:   <pre><code>kubectl apply -f app.yaml                                                      # uruchomienie aplikacji\nkubectl get pods                                                               # sprawdzenie uruchomionych pod\u00f3w\nkubectl delete pod my-pod                                                      # usuni\u0119cie poda\nkubeadm init                                                                   # stworzenie node'a master\nkubeadm token create --print-join-command                                      # generowanie tokeny na masterze\nkubeadm join 192.168.1.100:6443 --token ... --discovery-token-ca-cert-hash ... # pod\u0142\u0105czenie workera do mastera\nkubectl get nodes                                                              # wylistowanie node'\u00f3w\n</code></pre></p>"},{"location":"04-docker/#rejestracja-do-darmowego-dostepu-do-azure-dla-studentow","title":"Rejestracja do darmowego dost\u0119pu do Azure dla student\u00f3w","text":"<p>https://azure.microsoft.com/en-us/free/students/?WT.mc_id=academic-0000-cxa</p>"},{"location":"04-docker/#program-do-zarzadzania-dockerem","title":"Program do zarz\u0105dzania dockerem","text":"<p>Portainer https://hub.docker.com/r/portainer/portainer-ce jest platform\u0105, kt\u00f3ra pozwala na zarz\u0105dzanie lokaln\u0105 instancj\u0105 dockera i wszystkimi jej elementami.</p>"},{"location":"04-docker/example/","title":"Przyk\u0142ad","text":""},{"location":"04-docker/example/#plik-dockerfile","title":"Plik Dockerfile","text":"<pre><code>FROM nginx:1.28.0-bookworm\n\nWORKDIR /usr/share/nginx/html\n\nRUN apt-get update\n\nCOPY index.html .\n\nEXPOSE 8080\n\nCMD [\"nginx\",\"-g\",\"daemon off;\"]\n</code></pre>"},{"location":"04-docker/example/#plik-indexhtml","title":"Plik index.html","text":"<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n    &lt;body&gt;\n        &lt;h1&gt;Hello world!&lt;/h1&gt;\n    &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"04-docker/extra/","title":"Zadanie dodatkowe","text":"<p>Przygotuj system trzech kontener\u00f3w w architekturze MVC (model-view-controler) po\u0142\u0105czonych w sieci Docker i uruchamianych przy pomocy <code>docker compose</code>. Pocz\u0105tkowa struktura systemu znajduje si\u0119 w folderze mvc-task w repozytorium.</p> <p>System sk\u0142ada si\u0119 z nast\u0119puj\u0105cych element\u00f3w:</p> <ul> <li>model (baza danych) - obraz do \u015bci\u0105gni\u0119cia z Docker Hub; mo\u017cliwo\u015b\u0107 skorzystania z dowolnie wybranej bazy danych; w pocz\u0105tkowej propozycji jest PostgreSQL (ale mo\u017cna j\u0105 zamieni\u0107 na dowoln\u0105 inn\u0105) oraz skrypt tworz\u0105cy baz\u0119 danych <code>user_db</code> i jedn\u0105 tabel\u0119 <code>people</code> z dost\u0119pem dla u\u017cytkownika <code>admin</code> i has\u0142em <code>password</code>.</li> <li>controler (kontroler) - serwer REST API z jednym endpointem do tworzenia nowych rekord\u00f3w w bazie danych; pocz\u0105tkowy schemat aplikacji jest napisany w Pythonie przy pomocy bilbioteki FastAPI; mo\u017cecie u\u017cy\u0107 dowolnego j\u0119zyka: jedynym warunkiem jest to, \u017ce ma by\u0107 co najmniej jeden endpoint do tworzenia nowych rekord\u00f3w w bazie danych, kt\u00f3ra le\u017cy w oddzielnym kontenerze.</li> <li>view (widok) - prosta strona WWW wy\u015bwietlaj\u0105ca dane z bazy danych; poczatkowy schemat aplikacji proponuje u\u017cycie Pythona i biblioteki streamlit, ale nie jest to obowi\u0105zkowe; mo\u017ce to by\u0107 dowolny j\u0119zyk i dowolny pakiet.</li> </ul> <p>Model dzia\u0142ania:</p> <ol> <li>Wstawiamy dane poprzez API (kontroler) do bazy danych, np. poprzez <code>curl -X 'POST' 'http://localhost:8000/people' -H 'Content-Type: application/json' -d '{\"name\": \"Jan\",\"surname\": \"Kowalski\",\"position\": \"Intern\",\"salary\": 3000.00}'</code></li> <li>Dane l\u0105duj\u0105 w bazie danych.</li> <li>Tabel\u0119 z bazy danych (wszystkie dane) mo\u017cna zobaczy\u0107 w postaci tabeli w aplikacji webowej frontend.</li> </ol> <p>UWAGA: Login i has\u0142o do bazy danych musz\u0105 by\u0107 podane jako zmienne \u015brodowiskowe (nie mog\u0105 by\u0107 hard-codowane w kodzie aplikacji!).</p> <p>UWAGA: W tym systemie dwa kontenery musz\u0105 by\u0107 zbudowane poprzez Dockerfile: controler (api) oraz view (widok/frontend). Mo\u017cna je budowa\u0107 korzystaj\u0105c z sekcji <code>build</code> w pliku <code>docker-compose.yaml</code> (wtedy kontenery b\u0119d\u0105 zbudowane w momencie uruchamiania <code>docker compose up</code>).</p> <p>Ca\u0142o\u015b\u0107 ma by\u0107 uruchamiana skryptem <code>docker-compose.yaml</code>.</p> <p>Do przes\u0142ania:</p> <ul> <li>spakowany folder ze wszystkimi plikami i folderami (<code>docker-compose.yaml</code>, foldery <code>api</code>, <code>db</code>, <code>frontend</code>)</li> <li>screenshot frontendu, gdzie widoczne s\u0105 dane w formie tabelki (Uwaga! Na zrzucie ekranu powinny by\u0107 wi\u0119cej ni\u017c dwa domy\u015blne rekordy!)</li> </ul>"},{"location":"04-docker/homework/","title":"Praca domowa","text":"<p>\ud83d\udccc Zadanie 4 \u2013 Konteneryzacja aplikacji z modelem AI</p> <p>\ud83c\udfaf Cel zadania:</p> <p>Celem zadania jest konteneryzacja aplikacji, aby by\u0142a uruchamialna w kontenerze Docker. Aplikacja powinna dzia\u0142a\u0107 na wybranym porcie i umo\u017cliwia\u0107 interakcj\u0119 z modelem AI poprzez aplikacj\u0119 streamlit/gradio. Aplikacja powinna dzia\u0142a\u0107 lokalnie po uruchomieniu komendy <code>docker run</code>.</p> <p>\ud83d\udcdd Elementy do przygotowania:</p> <ol> <li> <p>Konteneryzacja aplikacji:</p> <ul> <li>Utw\u00f3rz plik Dockerfile, kt\u00f3ry b\u0119dzie zawiera\u0142 wszystkie zale\u017cno\u015bci wymagane do uruchomienia aplikacji (np. Python, biblioteki, model AI).</li> <li>Aplikacja powinna dzia\u0142a\u0107 na okre\u015blonym porcie.</li> <li>Po uruchomieniu kontenera aplikacja powinna by\u0107 dost\u0119pna pod adresem <code>http://localhost:&lt;wybrany_port&gt;</code>.</li> </ul> </li> <li> <p>Testowanie aplikacji w Dockerze:</p> <ul> <li>Upewnij si\u0119, \u017ce aplikacja dzia\u0142a p\u0142ynnie, a komunikacja z modelem AI dzia\u0142a poprawnie.</li> </ul> </li> <li> <p>Dokumentacja:</p> <ul> <li>Zaktualizuj plik README.md w repozytorium o instrukcje dotycz\u0105ce uruchamiania aplikacji w Dockerze.</li> <li>W README.md powinna pojawi\u0107 si\u0119 komenda <code>docker run</code> wraz ze wskazaniem mapowanego portu.</li> </ul> </li> </ol> <p>\ud83d\udce4 Elementy do przes\u0142ania:</p> <ul> <li>Plik Dockerfile.</li> <li>Zrzut ekranu log\u00f3w Dockera pokazuj\u0105cy, \u017ce kontener zbudowa\u0142 si\u0119 poprawnie i zosta\u0142 uruchomiony.</li> </ul> <p>\ud83c\udf99\ufe0f Elementy do pokazania na zaj\u0119ciach:</p> <ul> <li>Dzia\u0142aj\u0105ca aplikacja uruchomiona w Dockerze, dzia\u0142aj\u0105ca na wybranym porcie i umo\u017cliwiaj\u0105ca interakcj\u0119 z modelem AI.</li> <li>UWAGA! Prosz\u0119 o przygotowanie si\u0119, aby pokaza\u0107 uruchomiony kontener na w\u0142asnym komputerze / komputerze w pracowni.</li> </ul>"},{"location":"04-docker/mvc-task/api/main/","title":"Main","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport psycopg2 # biblioteka do \u0142\u0105czenia si\u0119 z baz\u0105 danych\nimport uvicorn\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel # biblioteka do walidacji danych w Pythonie\n</pre> import os import psycopg2 # biblioteka do \u0142\u0105czenia si\u0119 z baz\u0105 danych import uvicorn from fastapi import FastAPI from pydantic import BaseModel # biblioteka do walidacji danych w Pythonie In\u00a0[\u00a0]: Copied! <pre>app = FastAPI()\nDATABASE_URL = f\"postgresql://{os.environ['DB_USER']}:{os.environ.get['DB_PASSWORD']}@{os.environ.get['DB_ADDRESS']}:5432/{os.environ.get['DB_TABLE']}\"\n</pre> app = FastAPI() DATABASE_URL = f\"postgresql://{os.environ['DB_USER']}:{os.environ.get['DB_PASSWORD']}@{os.environ.get['DB_ADDRESS']}:5432/{os.environ.get['DB_TABLE']}\" In\u00a0[\u00a0]: Copied! <pre># Model danych wej\u015bciowych\nclass Person(BaseModel):\n    name: str\n    surname: str\n    position: str\n    salary: float\n</pre> # Model danych wej\u015bciowych class Person(BaseModel):     name: str     surname: str     position: str     salary: float In\u00a0[\u00a0]: Copied! <pre># Endpoint do wstawiania danych do bazy danych\n@app.post(\"/people\")\ndef create_entry(person: Person):\n    name = person.name\n    surname = person.surname\n    position = person.position\n    salary = person.salary\n    \n    # logika do wstawiania danych do bazy danych\n    \n    return {\"message\": \"Person created successfully!\"}\n</pre> # Endpoint do wstawiania danych do bazy danych @app.post(\"/people\") def create_entry(person: Person):     name = person.name     surname = person.surname     position = person.position     salary = person.salary          # logika do wstawiania danych do bazy danych          return {\"message\": \"Person created successfully!\"} <p>aby doda\u0107 dane, wywo\u0142aj w konsoli: curl -X 'POST' 'http://localhost:8000/people' -H 'Content-Type: application/json' -d '{\"name\": \"Jan\",\"surname\": \"Kowalski\",\"position\": \"Intern\",\"salary\": 3000.00}'</p> In\u00a0[\u00a0]: Copied! <pre># Sprawdzanie, czy skrypt jest uruchamiany bezpo\u015brednio\nif __name__ == \"__main__\":\n    # Uruchamianie aplikacji FastAPI za pomoc\u0105 Uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n</pre> # Sprawdzanie, czy skrypt jest uruchamiany bezpo\u015brednio if __name__ == \"__main__\":     # Uruchamianie aplikacji FastAPI za pomoc\u0105 Uvicorn     uvicorn.run(app, host=\"0.0.0.0\", port=8000)"},{"location":"04-docker/mvc-task/frontend/app/","title":"App","text":"In\u00a0[\u00a0]: Copied! <pre>import streamlit as st\n</pre> import streamlit as st <p>kod aplikacji</p>"},{"location":"05-ci-cd/","title":"CI/CD","text":""},{"location":"05-ci-cd/#czym-jest-cicd","title":"Czym jest CI/CD?","text":"<p>CI/CD = Continuous Integration + Continuous Delivery/Deployment</p> \ud83e\uddea CI \u2013 Continuous Integration \ud83d\ude80 CD \u2013 Continuous Delivery / Deployment Automatyczne testowanie i budowanie kodu po ka\u017cdej zmianie Automatyczne wdra\u017canie aplikacji na \u015brodowiska Sprawdza, czy zmiana nie psuje niczego Umo\u017cliwia szybkie wypuszczanie nowych wersji Testy, build, analiza kodu Packaging, deploy i rollback <p></p> <p></p> <p>\ud83d\udd27 Continuous Integration (CI)</p> <ul> <li>Programi\u015bci cz\u0119sto integruj\u0105 kod do wsp\u00f3lnego repozytorium (np. GitHub).</li> <li>Ka\u017cdy commit uruchamia pipeline: budowanie, testy jednostkowe, analiza jako\u015bci (np. SonarQube).</li> <li>Celem jest szybkie wykrycie i naprawienie b\u0142\u0119d\u00f3w.</li> </ul> <p>\ud83d\ude80 Continuous Delivery (CD)</p> <ul> <li>Kod po testach trafia automatycznie na \u015brodowisko staging (zwykle podobne do produkcji).</li> <li>Dalsze testy (np. testy obci\u0105\u017ceniowe).</li> <li>Gotowo\u015b\u0107 do wypuszczenia na produkcj\u0119 w ka\u017cdej chwili, z r\u0119cznym \"zatwierdzeniem\".</li> </ul> <p>\ud83e\udd16 Continuous Deployment</p> <ul> <li>Ka\u017cda zmiana, kt\u00f3ra przejdzie przez pipeline test\u00f3w, trafia automatycznie na produkcyjne \u015brodowisko.</li> <li>Brak \"manualnego zatwierdzenia\" \u2192 potrzebne 100% zaufanie do test\u00f3w automatycznych.</li> </ul>"},{"location":"05-ci-cd/#czym-w-praktyce-jest-cicd","title":"Czym w praktyce jest CI/CD?","text":"<p>CI/CD to po prostu lista krok\u00f3w (instrukcji), kt\u00f3re komputer automatycznie wykonuje, np.:</p> <ol> <li>\ud83d\udce5 Pobierz kod z repozytorium</li> <li>\ud83d\udee0 Zainstaluj zale\u017cno\u015bci</li> <li>\ud83d\udc33 Zbuduj obraz Dockera</li> <li>\ud83d\ude80 Wdr\u00f3\u017c aplikacj\u0119 na \u015brodowisko testowe / produkcyjne</li> </ol> <p>Wszystkie te kroki s\u0105 zapisane np. w pliku .yml w GitHub Actions \u2014 i za ka\u017cdym razem, gdy kto\u015b co\u015b zmieni w kodzie, komputer \"czyta\" ten plan i go realizuje.</p>"},{"location":"05-ci-cd/#architektura-dziaania-gitea-actions","title":"Architektura dzia\u0142ania Gitea Actions","text":"<p>Gitea to lekka, open-source'owa platforma DevOps \u2013 co\u015b jak \"w\u0142asny GitHub\", kt\u00f3ry mo\u017cesz uruchomi\u0107 na swoim serwerze.</p> <p>Gitea Actions to system CI/CD wbudowany w Gitea, inspirowany i kompatybilny z GitHub Actions. Ka\u017cde uruchomienie workflow (pipeline\u2019u) odbywa si\u0119 wewn\u0105trz kontenera Docker, nazywanego runnerem. Runnery mog\u0105 znajdowa\u0107 si\u0119 na tym samym serwerze co serwer lub na zewn\u0119trznych maszynach.</p> <p></p> <p>Po\u0142\u0105czenie 1</p> <p>Runner \u0142\u0105czy si\u0119 do serwera Gitea, aby otrzyma\u0107 zadania.</p> <p>Po\u0142\u0105czenie 2</p> <p>Job container \u0142\u0105czy si\u0119 do serwera Gitea, aby zaci\u0105gn\u0105\u0107 kod (np. zadanie <code>actions/checkout@v4</code>).</p> <p>Po\u0142\u0105czenie 3</p> <p>Runner \u015bciaga z Internetu definicje zada\u0144 (actions).</p> <p>Po\u0142\u0105czenie 4</p> <p>Job container \u015bciaga dane z Internetu, np. instaluje paczki Pythonowe.</p>"},{"location":"05-ci-cd/#typowe-elementy-pipelineu-cicd","title":"Typowe elementy pipeline'u CI/CD","text":"<ol> <li>\ud83d\udd14 Trigger \u2013 wyzwalacz, np. push lub pull request do konkretnego brancha</li> <li>\ud83d\udce5 Checkout \u2013 pobranie repozytorium do przestrzeni roboczej</li> <li>\ud83e\uddf0 Instalacja zale\u017cno\u015bci \u2013 setup j\u0119zyka, bibliotek, narz\u0119dzi</li> <li>\ud83e\uddea Testy jednostkowe \u2013 sprawdzenie, czy zmiany nie psuj\u0105 istniej\u0105cego kodu</li> <li>\ud83d\udd12 Skaner bezpiecze\u0144stwa \u2013 np. Snyk, Trivy, CodeQL</li> <li>\ud83d\udee0 Budowanie artefaktu \u2013 np. obraz Docker, paczka .zip, .jar, itp.</li> <li>\u2601\ufe0f Tworzenie infrastruktury \u2013 tworzenie zasob\u00f3w chmurowych (np. serwery, bazy danych) z narz\u0119dziem typu IaC (np. Terraform)</li> <li>\ud83d\ude80 Deployment \u2013 wdro\u017cenie na \u015brodowisko (testowe, staging, produkcyjne)</li> <li>\ud83d\udce3 Powiadomienia \u2013 webhooki, e-maile, Slack/Discord/Teams \u2014 co posz\u0142o OK/nie-OK</li> </ol>"},{"location":"05-ci-cd/#dobre-praktyki-zwiazane-z-cicd","title":"Dobre praktyki zwi\u0105zane z CI/CD","text":""},{"location":"05-ci-cd/#branche-w-narzedziu-git","title":"Branche w narz\u0119dziu git","text":"Ga\u0142\u0105\u017a Przeznaczenie CI/CD <code>feature/*</code> Rozw\u00f3j nowych funkcji CI (analiza kodu, testy) <code>dev</code> Integracja i testowanie CI + CD na \u015brodowisko staging/dev <code>main</code> Produkcja Pe\u0142ny pipeline: testy + deploy na prod"},{"location":"05-ci-cd/#ustawienie-tzw-policies-dla-repozytorium","title":"Ustawienie tzw. policies dla repozytorium","text":"<p>Zasady te mog\u0105 wprowadzi\u0107 zasady i ograniczy\u0107 pewnie dzia\u0142ania w celu zachowania wysokiej jako\u015bci kodu, np. wymagane review przed mergowaniem PR, poprawno\u015b\u0107 test\u00f3w jednostkowych przed zatwierdzeniem PR, wdra\u017canie na prod tylko z main, zakazanie force push, itd.</p>"},{"location":"05-ci-cd/#zmienne-srodowiskowe-i-sekrety","title":"Zmienne \u015brodowiskowe i sekrety","text":"<p>W kodzie nie wolno trzyma\u0107 \u017cadnych kluczy i hase\u0142 hard-coded. Je\u015bli s\u0105 one potrzebne w trakcie wykonywania pipeline'a, powinny zosta\u0107 one zdefiniowane w Sekretach / Zmiennych repozytorium/organizacji. </p> <p>Na GitHubie ustawia si\u0119 je w <code>Settings \u2192 Secrets and variables \u2192 Action</code>, a w pipeline odwo\u0142a\u0107 si\u0119 do nich jako <code>${{ secrets.AZURE_CREDENTIALS }}</code>. Mo\u017cna te\u017c grupowa\u0107 sekrety i zmienne w tzw. \u015brodowiska - w r\u00f3\u017cnych \u015brodowiskach te same zmienne mog\u0105 mie\u0107 r\u00f3\u017cne warto\u015bci.</p> <p>Je\u015bli pewne sekrety s\u0105 wsp\u00f3\u0142dzielone w dedykowanych narz\u0119dziach (np. Azure Key Vault, Hashicorp Vault), w sekretach repo mo\u017cna trzyma\u0107 dane potrzebne do zalogowania si\u0119 do tych zasob\u00f3w i bezpo\u015brednio z nich wyci\u0105ga\u0107 sekrety.</p>"},{"location":"05-ci-cd/#inne","title":"Inne:","text":"<ul> <li>Optymalizacja czasowa: r\u00f3wnoleg\u0142e uruchamianie pipeline'\u00f3w</li> <li>Definiowanie konkretnych wersji bibliotek i obraz\u00f3w Dockera, aby mie\u0107 kontrol\u0119 w przypadku nowych wersji bazowych program\u00f3w (np. nie ustawia\u0107 tagu <code>latest</code> w Dockerze)</li> <li>Monitoring przebiegu pipeline'\u00f3w (maile, alery, wiadomo\u015bci, itp.)</li> <li>Wersjonowanie artefakt\u00f3w, aby unikn\u0105\u0107 problem\u00f3w z identyfikowaniem, kt\u00f3ry artefakt zosta\u0142 wdro\u017cony w danym \u015brodowisku</li> <li>Rollback w przypadku nueudanego deploymentu</li> </ul>"},{"location":"05-ci-cd/#korzysci-z-cicd","title":"Korzy\u015bci z CI/CD","text":"Korzy\u015b\u0107 Opis \ud83e\uddea Wy\u017csza jako\u015b\u0107 B\u0142\u0119dy wychwytywane automatycznie przy ka\u017cdym commicie \u26a1 Szybsze releasy Aktualizacje dost\u0119pne cz\u0119\u015bciej dla klient\u00f3w \ud83d\udd04 Mniej pracy r\u0119cznej Automatyzacja eliminuje powtarzalne czynno\u015bci \ud83d\udcc9 Mniejsze ryzyko b\u0142\u0119d\u00f3w Ma\u0142e zmiany = \u0142atwiej je przetestowa\u0107 i cofn\u0105\u0107 \ud83d\udee1 Lepsze bezpiecze\u0144stwo Testy i skanery wykrywaj\u0105 podatno\u015bci wcze\u015bniej"},{"location":"05-ci-cd/example/","title":"Przyk\u0142ad","text":""},{"location":"05-ci-cd/example/#github","title":"GitHub","text":"<p>Aby skonfigurowa\u0107 pipeline, wystarczy wstawi\u0107 pliki do folderu <code>.github/workflows</code>.</p>"},{"location":"05-ci-cd/example/#github-actions-demo","title":"GitHub Actions demo","text":"<pre><code>name: Python CI/CD Demo\n\non:\n  push:\n    branches:\n        - main\n\njobs:\n  build-and-deploy-app-demo:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: \ud83e\uddfe Checkout repo\n      uses: actions/checkout@v3\n\n    - name: \ud83d\udc0d Setup Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: '3.10'\n\n    - name: \ud83d\udce6 Install dependencies\n      working-directory: ./02-environment/sentiment_analysis\n      run: |\n        echo \"Installing dependencies...\"\n        # python -m pip install --upgrade pip\n        # pip install -r requirements.txt\n\n    - name: \ud83d\udee0\ufe0f Build Docker image\n      working-directory: ./04-docker/example\n      run: |\n        echo \"Building Docker image...\"\n        # docker build -t myapp:latest .\n\n    - name: \ud83d\udc33 Push Docker image\n      working-directory: ./04-docker/example\n      run: |\n        echo \"Pushing Docker image...\"\n        # docker login\n        # docker push myregistry/myapp:latest\n\n    - name: \ud83d\ude80 Deploy to production\n      run: echo \"Deploying myapp:latest to PROD...\"\n</code></pre>"},{"location":"05-ci-cd/example/#github-actions-mkdocs","title":"GitHub Actions (MkDocs)","text":"<pre><code>name: Build &amp; Deploy MkDocs\n\non:\n  push:\n    branches:\n        - main\n\npermissions:\n  contents: write\n\njobs:\n  build-and-deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Checkout\n      uses: actions/checkout@v3\n\n    - name: Setup Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: '3.10'\n\n    - name: Install dependencies\n      working-directory: ./docs_config\n      run: |\n        pip install -r requirements.txt\n\n    - name: Deploy \ud83d\ude80\n      working-directory: ./docs_config\n      run: |\n        mkdocs gh-deploy --force\n</code></pre>"},{"location":"05-ci-cd/example/#azure-devops","title":"Azure DevOps","text":"<p>Dwa sposoby:</p> <ul> <li>W Azure DevOps wybieramy po lewej stronie Pipelines -&gt; New pipeline -&gt; Wybieramy \u017ar\u00f3d\u0142o kodu i \u015bcie\u017ck\u0119 do pliku konfiguracyjnego.</li> <li>W repozytorium klikamy Set up build -&gt; Wybieramy \u015bcie\u017ck\u0119 do pliku konfiguracyjnego.</li> </ul> <p>UWAGA! Przy pierwszym uruchomieniu pipeline'u pojawia si\u0119 nast\u0119puj\u0105cy komunikat: </p> <p></p> <p>Aby go rozwi\u0105za\u0107, trzeba wype\u0142ni\u0107 formularz pod podanym linkiem i poczeka\u0107 kilka dni!</p>"},{"location":"05-ci-cd/example/#azure-pipelines-demo","title":"Azure Pipelines demo","text":"<pre><code>trigger:\n  branches:\n    include:\n      - main\n\npool:\n  vmImage: 'ubuntu-latest'\n\njobs:\n- job: build_and_deploy_app_demo\n  displayName: 'Build and Deploy App Demo'\n  steps:\n\n  - checkout: self\n    displayName: '\ud83e\uddfe Checkout repo'\n\n  - task: UsePythonVersion@0\n    displayName: '\ud83d\udc0d Setup Python'\n    inputs:\n      versionSpec: '3.10'\n      addToPath: true\n\n  - script: |\n      echo \"Installing dependencies...\"\n      # python -m pip install --upgrade pip\n      # pip install -r requirements.txt\n    workingDirectory: ./02-environment/sentiment_analysis\n    displayName: '\ud83d\udce6 Install dependencies'\n\n  - script: |\n      echo \"Building Docker image...\"\n      # docker build -t myapp:latest .\n    workingDirectory: ./04-docker/example\n    displayName: '\ud83d\udee0\ufe0f Build Docker image'\n\n  - script: |\n      echo \"Pushing Docker image...\"\n      # docker login\n      # docker push myregistry/myapp:latest\n    workingDirectory: ./04-docker/example\n    displayName: '\ud83d\udc33 Push Docker image'\n\n  - script: echo \"Deploying myapp:latest to PROD...\"\n    displayName: '\ud83d\ude80 Deploy to production'\n</code></pre>"},{"location":"05-ci-cd/homework/","title":"Praca domowa","text":"<p>\ud83d\udccc Zadanie 5 \u2013 Stworzenie pipeline'\u00f3w CI/CD</p> <p>\ud83c\udfaf Cel zadania:</p> <p>Celem zadania jest skonfigurowanie procesu CI/CD dla aplikacji. Zadanie sk\u0142ada si\u0119 z dw\u00f3ch osobnych pipeline'\u00f3w \u2013 pierwszy odpowiada za budowanie i publikacj\u0119 obrazu Docker, a drugi za analiz\u0119 jako\u015bci lub bezpiecze\u0144stwa kodu. Opcjonalnie, osoby bardziej zaawansowane mog\u0105 wdro\u017cy\u0107 pipeline do tworzenia infrastruktury w chmurze za pomoc\u0105 narz\u0119dzi typu Infrastructure as Code (IaC).</p> <p>\ud83d\udcdd Elementy do przygotowania:</p> <ol> <li> <p>Pipeline #1 \u2013 Budowanie i publikacja obrazu Docker:</p> <ul> <li>Pipeline uruchamia si\u0119 po ka\u017cdej aktualizacji (merge/commit) do ga\u0142\u0119zi main.</li> <li>Buduje obraz Dockera z aplikacj\u0105.</li> <li>Wysy\u0142a obraz do wybranego Docker Registry (Docker Hub, GitHub Container Registry, Azure Container Registry itp.).</li> <li>UWAGA: Wszystkie sekrety (tokeny, loginy, has\u0142a) musz\u0105 by\u0107 trzymane wy\u0142\u0105cznie w sekretach repozytorium. \u017baden klucz API, login, has\u0142o nie mo\u017ce znajdowa\u0107 si\u0119 w kodzie ani by\u0107 w obrazie Dockera.</li> </ul> </li> <li> <p>Pipeline #2 \u2013 Kilka opcji do wyboru (wystarczy jedna z nich!):</p> <ol> <li>Jako\u015b\u0107 kodu:<ul> <li>Pipeline uruchamia si\u0119 automatycznie przy otwarciu Pull Request do ga\u0142\u0119zi main.</li> <li>Sprawdzana jest jako\u015b\u0107 kodu przy pomocu automatycznych narz\u0119dzi.</li> <li>Celem jest, aby kod by\u0142 czysty, sp\u00f3jny i utrzymywalny.</li> <li>Pipeline powinien korzysta\u0107 z framework'u <code>pre-commit</code> (https://pre-commit.com/) i wykorzystywa\u0107 co najmniej 5 narz\u0119dzi, np.:<ul> <li>black \u2013 formatowanie kodu zgodnie z PEP8.</li> <li>flake8 \u2013 analiza stylu i b\u0142\u0119d\u00f3w.</li> <li>isort \u2013 uporz\u0105dkowanie import\u00f3w.</li> <li>autoflake \u2013 usuwanie nieu\u017cywanych import\u00f3w i zmiennych.</li> <li>end-of-file-fixer, trailing-whitespace \u2013 usuwanie niepotrzebnych bia\u0142ych znak\u00f3w.</li> </ul> </li> <li>Pipeline musi zako\u0144czy\u0107 si\u0119 sukcesem, aby m\u00f3c zmergowa\u0107 zmiany do ga\u0142\u0119zi main.</li> </ul> </li> <li>Poprawno\u015b\u0107 dzia\u0142ania aplikacji (testy jednostkowe):<ul> <li>Pipeline uruchamia si\u0119 automatycznie przy otwarciu Pull Request do ga\u0142\u0119zi main.</li> <li>Sprawdzana jest poprawno\u015b\u0107 dzia\u0142ania aplikacji.</li> <li>Testy jednostkowe mog\u0105 by\u0107 napisane przy pomocy narz\u0119dzia <code>pytest</code>.</li> <li>Pipeline musi zako\u0144czy\u0107 si\u0119 sukcesem, aby m\u00f3c zmergowa\u0107 zmiany do ga\u0142\u0119zi main.</li> </ul> </li> <li>Skan bezpiecze\u0144stwa:<ul> <li>Pipeline uruchamia si\u0119 automatycznie przy otwarciu Pull Request do ga\u0142\u0119zi main.</li> <li>Narz\u0119dzie do skanowania podatno\u015bci w kodzie, zale\u017cno\u015bciach, kontenerze.</li> <li>Cel: zapewnienie bezpiecze\u0144stwa na poziomie kodu i zale\u017cno\u015bci.</li> <li>Przyk\u0142ad narz\u0119dzia: <code>trivy</code> (https://github.com/aquasecurity/trivy).</li> <li>Pipeline ma si\u0119 nie ko\u0144czy\u0107 sukcesem, je\u015bli wyst\u0119puj\u0105 podatno\u015bci typu HIGH lub CRITICAL \u2013 nale\u017cy je rozwi\u0105za\u0107, a nie wycisza\u0107.</li> </ul> </li> <li>Infrastructure as a Code (IaC):<ul> <li>Tworzenie infrastruktury w chmurze (Azure/AWS/GCP) przy pomocy pipeline'u i narz\u0119dzia typu Terraform.</li> <li>Stworzenie instancji aplikacji webowej / funkcji oraz zasobu typu storage (Azure Blob Storage/S3).</li> <li>Uwaga: Sam deployment (wdro\u017cenie aplikacji w chmurze) b\u0119dzie wymagane na kolejnym checkpointcie - teraz wymagane jest tylko stworzenie zasob\u00f3w.</li> <li>Pipeline IaC powinien wywo\u0142ywa\u0107 si\u0119 po ka\u017cdej aktualizacji (merge/commit) do ga\u0142\u0119zi main.</li> </ul> </li> </ol> </li> </ol> <p>\ud83d\udce4 Elementy do przes\u0142ania:</p> <ul> <li>Link do obrazu Dockera w wybranym rejestrze obraz\u00f3w.</li> <li>Zrzuty ekranu z dzia\u0142aj\u0105cymi pipeline'ami (zielone statusy!).</li> </ul> <p>\ud83c\udf99\ufe0f Elementy do pokazania na zaj\u0119ciach:</p> <ul> <li>Widok opublikowanego obrazu Dockera.</li> <li>Zielone statusy pipeline'\u00f3w.</li> </ul>"},{"location":"06-cloud/","title":"Cloud computing","text":""},{"location":"06-cloud/#co-to-jest-chmura-obliczeniowa","title":"Co to jest chmura obliczeniowa?","text":"<p>Chmura obliczeniowa (cloud computing) to dostarczanie us\u0142ug IT (np. serwer\u00f3w, baz danych, przechowywania, analizy, sztucznej inteligencji) przez Internet \u2013 na \u017c\u0105danie, elastycznie, bez inwestycji w fizyczn\u0105 infrastruktur\u0119.</p> <p>Przyk\u0142ad: Zamiast kupowa\u0107 serwer do hostowania strony internetowej, w Azure uruchamiasz maszyn\u0119 wirtualn\u0105 w kilka minut i p\u0142acisz tylko za czas jej dzia\u0142ania.</p>"},{"location":"06-cloud/#hierarchia-zarzadzania-w-azure","title":"Hierarchia zarz\u0105dzania w Azure","text":"<ol> <li> <p>Microsoft Entra ID Tenant (wcze\u015bniej Azure Active Directory Tenant)</p> <ul> <li>To najwy\u017cszy poziom w strukturze \u2013 reprezentuje ca\u0142\u0105 organizacj\u0119.</li> <li>Zawiera u\u017cytkownik\u00f3w, grupy, aplikacje, role i zasady to\u017csamo\u015bci.</li> <li>Ka\u017cdy Tenant ma unikalny identyfikator (np. twojafirma.onmicrosoft.com).</li> <li>W tenancie mo\u017cesz mie\u0107 wiele subskrypcji.</li> <li>Przyk\u0142ad: Firma \"Contoso\" ma jedn\u0105 dzier\u017caw\u0119 AAD, a w niej dzia\u0142 IT, HR, Finanse \u2013 ka\u017cdy z dost\u0119pem tylko do swoich subskrypcji.</li> </ul> </li> <li> <p>Management Group</p> <ul> <li>Organizuje wiele subskrypcji w struktur\u0119 hierarchiczn\u0105.</li> <li>S\u0142u\u017cy do wsp\u00f3lnego zarz\u0105dzania politykami, kontrol\u0105 dost\u0119pu, itd.</li> </ul> </li> <li> <p>Subscription (subskrypcja)</p> <ul> <li>To poziom rozlicze\u0144 i kontroli dost\u0119pu do zasob\u00f3w.</li> <li>Ka\u017cda subskrypcja ma unikalny ID i limit zasob\u00f3w.</li> <li>Mo\u017cna mie\u0107 wiele subskrypcji w jednej dzier\u017cawie \u2013 np. osobna dla dev, osobna dla produkcji.</li> </ul> </li> <li> <p>Resource Group</p> <ul> <li>Logiczna \"paczka\" zasob\u00f3w \u2013 trzymasz tam powi\u0105zane elementy aplikacji.</li> <li>Mo\u017cna stosowa\u0107 do organizacji, zarz\u0105dzania uprawnieniami, tagowania itd.</li> </ul> </li> <li> <p>Resource (zas\u00f3b)</p> <ul> <li>Pojedynczy element w chmurze: VM, baza danych, App Service, Storage Account, itd.</li> <li>Zawsze nale\u017cy do jednej resource group i jednej subskrypcji.</li> </ul> </li> </ol> <pre><code>Azure Active Directory Tenant\n\u2502\n\u251c\u2500\u2500 Management Groups (opcjonalne)\n\u2502   \u251c\u2500\u2500 Subscription A\n\u2502   \u2502   \u251c\u2500\u2500 Resource Group A\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 Resource 1 (VM)\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 Resource 2 (Storage)\n\u2502   \u2502   \u251c\u2500\u2500 Resource Group B\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 Resource 3 (Azure Function)\n\u2502   \u2514\u2500\u2500 Subscription B\n\u2502       \u2514\u2500\u2500 Resource Group C\n\u2502           \u2514\u2500\u2500 Resource 4 (SQL Database)\n</code></pre>"},{"location":"06-cloud/#regiony-i-dostepnosc","title":"Regiony i dost\u0119pno\u015b\u0107","text":""},{"location":"06-cloud/#region-np-poland-central-sweden-central-north-europe-east-us","title":"Region (np. Poland Central, Sweden Central, North Europe, East US)","text":"<p>To fizyczne lokalizacje centr\u00f3w danych Azure, rozproszone globalnie. Wyb\u00f3r regionu wp\u0142ywa na:</p> <ul> <li>Szybko\u015b\u0107 dzia\u0142ania aplikacji (im bli\u017cej u\u017cytkownika, tym lepiej),</li> <li>Spe\u0142nienie przepis\u00f3w prawnych (np. przetwarzanie danych tylko w UE),</li> <li>Koszty (niekt\u00f3re regiony s\u0105 ta\u0144sze).</li> <li>Dost\u0119pno\u015b\u0107 us\u0142ug (niekt\u00f3re us\u0142ugi, w szczeg\u00f3lno\u015bci nowe albo specjalistyczne, jak Azure OpenAI, s\u0105 dost\u0119pne tylko w wybranych regionach).</li> </ul>"},{"location":"06-cloud/#availability-zones-az","title":"Availability Zones (AZ)","text":"<p>To fizycznie niezale\u017cne strefy w jednym regionie \u2013 ka\u017cda ma w\u0142asne zasilanie, ch\u0142odzenie, sie\u0107. Dzi\u0119ki temu mo\u017cna osi\u0105gn\u0105\u0107:</p> <ul> <li>Wysok\u0105 dost\u0119pno\u015b\u0107 (High Availability),</li> <li>Odporno\u015b\u0107 na awarie jednej strefy (failover),</li> <li>Automatyczne prze\u0142\u0105czanie ruchu (load balancing).</li> </ul> <p>Przyk\u0142ad: Twoja aplikacja dzia\u0142a w 3 strefach w regionie West Europe. Je\u015bli jedna strefa przestanie dzia\u0142a\u0107, ruch automatycznie przekieruje si\u0119 do pozosta\u0142ych.</p> <p>Uwaga! Nie ka\u017cdy region Azure ma AZ!</p>"},{"location":"06-cloud/#modele-usug-w-azure","title":"Modele us\u0142ug w Azure","text":"<ol> <li> <p>IaaS \u2013 Infrastructure as a Service</p> <p>Wynajmujesz \"go\u0142\u0105\" infrastruktur\u0119: maszyny wirtualne (Azure Virtual Machines), sieci, storage.</p> </li> <li> <p>PaaS \u2013 Platform as a Service</p> <p>Dostajesz gotowe \u015brodowisko do uruchamiania aplikacji (np. Azure App Service). Nie interesuje ci\u0119 np. system operacyjny i wersja np. Ubuntu.</p> </li> <li> <p>SaaS \u2013 Software as a Service</p> <p>Gotowe aplikacje w chmurze (np. Microsoft 365, Azure DevOps).</p> </li> <li> <p>FaaS \u2013 Function as a Service</p> <p>Kod dzia\u0142a tylko, gdy trzeba (np. Azure Functions). Jedyne, co trzeba zrobi\u0107, to przygotowa\u0107 kod w wybranym j\u0119zyku. Idealne do event-driven logiki np. reagowanie na przes\u0142anie pliku.</p> </li> </ol>"},{"location":"06-cloud/#skalowalnosc-redundancja-balansowanie-ruchu","title":"Skalowalno\u015b\u0107, redundancja, balansowanie ruchu","text":"<ol> <li> <p>Skalowalno\u015b\u0107</p> <p>Azure umo\u017cliwia automatyczne skalowanie (np. App Service Plan, VM Scale Sets) \u2013 czyli uruchamia wi\u0119cej instancji, gdy ro\u015bnie ruch.</p> </li> <li> <p>Redundancja i Load Balancing</p> <ul> <li>Load Balancer (Azure Load Balancer / Application Gateway) \u2013 rozdziela ruch mi\u0119dzy serwery.</li> <li>Geo-replikacja \u2013 dane/aplikacje s\u0105 dost\u0119pne z wielu region\u00f3w (np. Azure Traffic Manager, Azure Front Door), co:<ul> <li>Skraca czas \u0142adowania globalnie (np. u\u017cytkownik z Japonii trafia do najbli\u017cszego serwera),</li> <li>Zwi\u0119ksza dost\u0119pno\u015b\u0107 (je\u015bli region A padnie, prze\u0142\u0105cza si\u0119 na B).</li> </ul> </li> </ul> </li> </ol>"},{"location":"06-cloud/#modele-chmury-obliczeniowej","title":"Modele chmury obliczeniowej","text":""},{"location":"06-cloud/#public-cloud-chmura-publiczna","title":"Public Cloud (Chmura publiczna)","text":"<p>To najcz\u0119stszy model \u2013 us\u0142ugi i infrastruktura s\u0105 w\u0142asno\u015bci\u0105 Microsoftu i dost\u0119pne przez internet dla wielu klient\u00f3w.</p> <p>Zalety:</p> <ul> <li>Brak koszt\u00f3w infrastruktury \u2013 wszystko zapewnia Azure (np. serwery, sie\u0107, ch\u0142odzenie).</li> <li>Skalowalno\u015b\u0107 \u2013 zasoby mo\u017cna elastycznie zwi\u0119ksza\u0107 i zmniejsza\u0107.</li> <li>P\u0142acisz tylko za to, co u\u017cywasz \u2013 model \"pay-as-you-go\".</li> <li>Szybkie wdro\u017cenie \u2013 masz dost\u0119p do gotowych us\u0142ug (np. Azure App Services, Azure VMs).</li> </ul> <p>Wady:</p> <ul> <li>Mniejsza kontrola nad infrastruktur\u0105 \u2013 wszystko zarz\u0105dzane przez dostawc\u0119.</li> <li>Zale\u017cno\u015b\u0107 od po\u0142\u0105czenia internetowego.</li> <li>Potencjalne ryzyko zwi\u0105zane z przechowywaniem danych poza organizacj\u0105 (cho\u0107 Azure ma wysokie standardy bezpiecze\u0144stwa).</li> </ul>"},{"location":"06-cloud/#private-cloud-chmura-prywatna","title":"Private Cloud (Chmura prywatna)","text":"<p>Wszystkie zasoby dzia\u0142aj\u0105 na infrastrukturze firmy, ale z wykorzystaniem technologii chmurowej.</p> <p>Zalety:</p> <ul> <li>Pe\u0142na kontrola nad danymi i infrastruktur\u0105 \u2013 istotne np. przy danych medycznych, finansowych.</li> <li>Wi\u0119ksze bezpiecze\u0144stwo i zgodno\u015b\u0107 z regulacjami (np. RODO).</li> <li>Mo\u017cliwo\u015b\u0107 dostosowania pod konkretne potrzeby organizacji.</li> </ul> <p>Wady:</p> <ul> <li>Wysokie koszty pocz\u0105tkowe \u2013 trzeba kupi\u0107 i utrzyma\u0107 w\u0142asny sprz\u0119t.</li> <li>Konieczno\u015b\u0107 posiadania w\u0142asnego zespo\u0142u IT do zarz\u0105dzania infrastruktur\u0105.</li> <li>Mniejsza elastyczno\u015b\u0107 skalowania ni\u017c w public cloud.</li> </ul>"},{"location":"06-cloud/#hybrid-cloud-chmura-hybrydowa","title":"Hybrid Cloud (Chmura hybrydowa)","text":"<p>\u0141\u0105czy chmur\u0119 publiczn\u0105 (np. Azure) i prywatn\u0105 (np. Azure Stack lub lokalne serwery). Mo\u017cna np. przechowywa\u0107 dane w chmurze prywatnej, a aplikacje uruchamia\u0107 w publicznej. Mog\u0105 by\u0107 po\u0142\u0105czone poprzez VPN lub ExpressRoute (dedykowane, prywatne \u0142\u0105cze do Azure, nie przez Internet)</p> <p>Zalety:</p> <ul> <li>Elastyczno\u015b\u0107 \u2013 mo\u017cesz korzysta\u0107 z chmury publicznej, ale wra\u017cliwe dane trzymasz lokalnie.</li> <li>Optymalizacja koszt\u00f3w \u2013 mniej krytyczne operacje mo\u017cesz przenie\u015b\u0107 do ta\u0144szej chmury publicznej.</li> <li>Zgodno\u015b\u0107 z przepisami \u2013 np. przetwarzanie danych w kraju.</li> </ul> <p>Wady:</p> <ul> <li>Z\u0142o\u017cono\u015b\u0107 konfiguracji i zarz\u0105dzania \u2013 wymaga dobrego planowania.</li> <li>Wi\u0119ksze wymagania dla zespo\u0142u IT \u2013 musz\u0105 ogarnia\u0107 oba \u015bwiaty (lokalne + cloud).</li> <li>Potencjalne problemy z integracj\u0105 danych i aplikacji mi\u0119dzy \u015brodowiskami.</li> </ul>"},{"location":"06-cloud/#ciekawostki","title":"Ciekawostki","text":"<ol> <li> <p>Microsoft Azure d\u0105\u017cy by do 2025 r. wszystkie centra danych by\u0142y zasilane energi\u0105 odnawialn\u0105 i inwestuj\u0105 na rzecz innowacji energetycznych. link</p> </li> <li> <p>Azure, AWS, GCP to chmury komercyjne. Mo\u017cliwe jest zbudowanie chmury prywatnej, na w\u0142asnych zasobach, np. u\u017cywaj\u0105c:</p> <ul> <li>Azure Stack \u2013 lokalna wersja Azure dzia\u0142aj\u0105ca we w\u0142asnym data center.</li> <li>Open source'owe rozwi\u0105zania jak OpenStack, VMware vSphere.</li> </ul> </li> </ol>"},{"location":"06-cloud/#dlaczego-firmy-przechodza-do-chmury","title":"Dlaczego firmy przechodz\u0105 do chmury?","text":"<ul> <li>Mniejsze koszty (brak fizycznych serwerowni)</li> <li>\u0141atwe skalowanie zasob\u00f3w</li> <li>Dost\u0119pno\u015b\u0107 z ka\u017cdego miejsca</li> <li>Automatyczne aktualizacje</li> <li>Wysoka niezawodno\u015b\u0107 i odporno\u015b\u0107 na awarie</li> </ul>"},{"location":"06-cloud/#linki-i-wskazowki-przydatne-przy-pracy-z-projektem","title":"Linki i wskaz\u00f3wki przydatne przy pracy z projektem","text":"<p>Linki:</p> <ul> <li>Materia\u0142y treningowe Microsoft Azure</li> <li>Materia\u0142y treningowe Azure Container Registry &amp; Azure App Service</li> <li>Materia\u0142y treningowe Azure Container Registry &amp; Azure App Service</li> <li>CI/CD dla Azure Web App (container)</li> <li>Tutorial Azure Container App</li> <li>Python i Azure Blob Storage</li> </ul> <p>Wskaz\u00f3wki:</p> <ul> <li>Tutorial z linku pokazuje jak wdro\u017cy\u0107 na chmur\u0119 aplikacj\u0119 ze zbudowanego ju\u017c wcze\u015bniej kontenera znajduj\u0105cego si\u0119 w rejestrze. W projekcie, natomiast, wdra\u017canie nowych wersji ma nast\u0119powa\u0107 z u\u017cyciem pipeline'u CI/CD.</li> <li>W celu zapisywania danych w Azure Blob Storage, mo\u017cna skorzysta\u0107 z \u0142atwiejszej metody autoryzacji, czyli tzw. <code>Connection String</code>. W \u015brodowiskach produkcyjncyh rekomendowanym podej\u015bciem jest to oparte na Azure RBAC (Azure Role-Based Access Control), natomiast wymaga ono dodatkowej konfiguracji (u\u017cycie tego podejscia w projekcie b\u0119dzie na plus).</li> <li>Klucze i sekrety niezb\u0119dne do dzia\u0142ania aplikacji mo\u017cna ustawi\u0107 manualnie w Azure Portal jako zmienne \u015brodowiskowe aplikacji. W \u015brodowiskach produkcyjncyh rekomendowanym podej\u015bciem jest korzystanie z Azure Key Vault, natomiast wymaga to dodatkowej konfiguracji (u\u017cycie tego podejscia w projekcie b\u0119dzie na plus).</li> </ul>"},{"location":"06-cloud/homework/","title":"Praca domowa","text":"<p>\ud83d\udccc Zadanie 6 \u2013 Aplikacja dzia\u0142aj\u0105ca w \u015brodowisku chmurowym</p> <p>\ud83c\udfaf Cel zadania:</p> <p>Celem tego zadania jest uruchomienie aplikacji w \u015brodowisku chmurowym (Azure, AWS lub GCP), z pe\u0142nym procesem CI/CD. Aplikacja powinna dzia\u0142a\u0107 jako kontener, by\u0107 wdra\u017cana z poziomu pipeline'u i zapisywa\u0107 dane wej\u015bciowe oraz odpowiedzi modelu do zasobu typu storage (np. Blob/S3/GCP Storage).</p> <p>\ud83d\udcdd Elementy do przygotowania:</p> <ol> <li> <p>Wdro\u017cenie aplikacji z pipeline'u:</p> <ul> <li>Deployment aplikacji na chmur\u0119 ma by\u0107 wykonywany wy\u0142\u0105cznie z poziomu pipeline CI/CD.</li> <li>Pipeline powinien pobiera\u0107 obraz z registry (Docker Hub / GHCR / ACR) i wdra\u017ca\u0107 go na konkretn\u0105 us\u0142ug\u0119.</li> </ul> </li> <li> <p>Aplikacja w kontenerze:</p> <ul> <li>Aplikacja powinna by\u0107 wdro\u017cona jako kontener na chmurze.</li> <li>Publicznie dost\u0119pna pod adresem URL (autoryzacja \u2013 opcjonalna).</li> </ul> </li> <li> <p>Zapis danych do storage:</p> <ul> <li>Aplikacja ma mie\u0107 zaimplementowan\u0105 logik\u0119 do przetrzymywania danych wej\u015bciowych i wyj\u015bciowych.</li> <li>Ka\u017cde wywo\u0142anie modelu zapisuje dane do oddzielnego folderu w zasobie storage (np. 2024-01-1_12:00:03/):<ul> <li>input.txt / input.png \u2013 dane wej\u015bciowe u\u017cytkownika.</li> <li>output.txt \u2013 wynik predykcji modelu.</li> <li>Inne struktura plik\u00f3w jest r\u00f3wnie\u017c mo\u017cliwa - najwa\u017cniejsze, \u017ceby pliki by\u0142y tam zapisywane w spos\u00f3b ustrukturyzowany.</li> </ul> </li> <li>Mo\u017ce by\u0107 u\u017cyte: Azure Blob, AWS S3, Google Cloud Storage \u2013 dowolny chmurowy storage.</li> <li>Zamiast zasobu typu storage, mo\u017cna wykorzysta\u0107 baz\u0119 danych (jako odzielny zas\u00f3b na chmurze, poza aplikacj\u0105).</li> </ul> </li> <li> <p>Infrastruktura chmurowa:</p> <ul> <li>Aplikacja webowa (us\u0142uga hostuj\u0105ca kontener).</li> <li>Storage bucket / baza danych (do zapis\u00f3w danych).</li> </ul> </li> <li> <p>CI/CD \u2013 wymagania:</p> <ul> <li>Mo\u017cna rozszerzy\u0107 pipeline (z poprzedniego zadania) s\u0142u\u017c\u0105cy do budowania i wypychania kontenera. Mo\u017cna r\u00f3wnie\u017c stworzy\u0107 odzielny pipeline, ale wywo\u0142ywany tylko po sukcesie wy\u017cej wspomnianego pipeline'u.</li> <li>Sekrety (dost\u0119py do chmury) musz\u0105 by\u0107 przechowywane w sekretach repozytorium \u2013 zero hase\u0142 w kodzie!</li> </ul> </li> </ol> <p>\ud83d\udce4 Elementy do przes\u0142ania:</p> <ul> <li>Link do uruchomionej aplikacji (adres URL).</li> <li>Screenshot folder\u00f3w input/output zapisanych w storage.</li> <li>Screenshot dzia\u0142ajacej aplikacji w chmurze.</li> </ul> <p>\ud83c\udf99\ufe0f Elementy do pokazania na zaj\u0119ciach:</p> <ul> <li>Interakcja z uruchomion\u0105 aplikacj\u0105 w chmurze.</li> <li>Pokazanie wygenerowanych danych (input/output) w storage.</li> </ul>"},{"location":"07-ml-tools/","title":"Narz\u0119dzia ML","text":""},{"location":"07-ml-tools/#automl","title":"AutoML","text":"<p>AutoML (Automated Machine Learning) to zestaw metod i narz\u0119dzi, kt\u00f3rych celem jest zautomatyzowanie procesu tworzenia modeli ML \u2013 od przygotowania danych po dob\u00f3r algorytmu i jego strojenie. Dzi\u0119ki temu osoby bez g\u0142\u0119bokiej wiedzy o ML mog\u0105 tworzy\u0107 modele o wysokiej jako\u015bci.</p>"},{"location":"07-ml-tools/#typowy-cykl-automl-obejmuje","title":"Typowy cykl AutoML obejmuje:","text":"Etap Co si\u0119 dzieje? Przygotowanie danych Czyszczenie, brakuj\u0105ce dane, skalowanie, kodowanie kategorii In\u017cynieria cech Tworzenie nowych zmiennych (np. daty, interakcje) Dob\u00f3r modelu Wyb\u00f3r algorytmu ML (np. Random Forest, SVM, itp.) Strojenie parametr\u00f3w Optymalizacja hiperparametr\u00f3w, walidacja krzy\u017cowa (cross-validation) Ensemble \u0141\u0105czenie wielu modeli Ewaluacja i selekcja Testowanie i wyb\u00f3r najlepszego modelu"},{"location":"07-ml-tools/#autosklearn","title":"AutoSklearn","text":"<p>AutoSklearn to otwarto\u017ar\u00f3d\u0142owy framework AutoML w Pythonie oparty na scikit-learn.</p> <p>Cechy:</p> <ul> <li>dzia\u0142a na danych tabelarycznych (np. klasyfikacja, regresja),</li> <li>automatyzuje przygotowanie danych, wyb\u00f3r modelu, tuning,</li> <li>u\u017cywa technik:<ul> <li>Bayesian Optimization</li> <li>Meta-learning</li> <li>Ensemble construction</li> </ul> </li> </ul> <p>Meta-learning</p> <p>Meta-learning pochodzi od metadata learning. AutoSklearn korzysta z bazy wiedzy o poprzednich zadaniach ML - posiada sw\u00f3j w\u0142asny \"meta-model\", kt\u00f3ry wspomaga i skraca czas znalezienia optymalnego modelu.</p> <p>\ud83e\udde0 \"Je\u015bli wcze\u015bniej dane podobne do Twoich najlepiej dzia\u0142a\u0142y z XGBoostem, to zaczn\u0119 od XGBoosta.\"</p> <p>Bayesian Optimization</p> <p>Zamiast \u015blepo testowa\u0107 wszystkie kombinacje hiperparametr\u00f3w, AutoSklearn inteligentnie wybiera kolejne eksperymenty, buduj\u0105c model przewiduj\u0105cy, kt\u00f3re ustawienia mog\u0105 dzia\u0142a\u0107 najlepiej.</p> <p>\ud83e\udde0 \"W poprzednich krokach najlepiej dzia\u0142a\u0142y modele z n_estimators ~100 i max_depth ~5. Sprawd\u017amy w tej okolicy!\"</p> <p>Ensembling</p> <p>AutoSklearn automatycznie \u0142\u0105czy najlepsze modele w ensemble, co poprawia dok\u0142adno\u015b\u0107 i stabilno\u015b\u0107.</p>"},{"location":"07-ml-tools/#zalety-i-wady","title":"Zalety i wady","text":"<p>Zalety AutoML:</p> <ul> <li>Oszcz\u0119dno\u015b\u0107 czasu \u2013 bez r\u0119cznego strojenia modeli</li> <li>Ni\u017cszy pr\u00f3g wej\u015bcia \u2013 dzia\u0142a nawet dla pocz\u0105tkuj\u0105cych</li> <li>Automatyczna optymalizacja \u2013 model cz\u0119sto dor\u00f3wnuje r\u0119cznie budowanym</li> </ul> <p>Wady AutoML:</p> <ul> <li>Czarna skrzynka \u2013 nie wiadomo, co dok\u0142adnie robi model (problem interpretowalno\u015bci)</li> <li>Ograniczona kontrola \u2013 trudniej dopasowa\u0107 model do specyficznego problemu</li> <li>Wydajno\u015b\u0107 i koszt \u2013 potrafi by\u0107 wolne i zasobo\u017cerne</li> <li>Nie zast\u0119puje wiedzy domenowej \u2013 trzeba wiedzie\u0107, co znacz\u0105 dane</li> </ul>"},{"location":"07-ml-tools/#mlflow","title":"MLflow","text":"<p>MLflow to open-source'owa platforma do zarz\u0105dzania ca\u0142ym cyklem \u017cycia modeli ML. Jest ona j\u0119zyk agnostyczna \u2014 wspiera Python, R, Java, REST API. U\u0142atwia \u015bledzenie eksperyment\u00f3w, pakowanie modeli, wdra\u017canie i monitorowanie.</p>"},{"location":"07-ml-tools/#uruchomienie-mlflow","title":"Uruchomienie MLflow:","text":"<pre><code>pip install mlflow\nmlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///07-ml-tools/mlflow/mlflow.db --default-artifact-root 07-ml-tools/mlflow/mlruns\n</code></pre>"},{"location":"07-ml-tools/#gowne-etapty-cyklu-zycia-modelu-ml","title":"G\u0142\u00f3wne etapty cyklu \u017cycia modelu ML","text":"<ol> <li>Zbieranie i przygotowanie danych</li> <li>Trenowanie modelu</li> <li>\u015aledzenie eksperyment\u00f3w (parametry, metryki, wyniki)</li> <li>Pakowanie modeli do \u0142atwego wdro\u017cenia (np. konetener)</li> <li>Wdra\u017canie modeli do produkcji</li> <li>Monitorowanie modeli w dzia\u0142aniu (np. spadek jako\u015bci)</li> </ol>"},{"location":"07-ml-tools/#kluczowe-komponenty-mlflow","title":"Kluczowe komponenty MLflow","text":"<ol> <li>MLflow Tracking: logowanie i por\u00f3wnywanie eksperyment\u00f3w (parametry, metryki, artefakty, wersje kodu).</li> <li>MLflow Projects: standaryzowany spos\u00f3b pakowania kodu do powtarzalnych eksperyment\u00f3w.</li> <li>MLflow Models: format pakowania modeli kompatybilny z r\u00f3\u017cnymi platformami (TensorFlow, Spark, Scikit-learn).</li> <li>MLflow Model Registry: centralne repozytorium modeli, wersjonowanie, kontrola etap\u00f3w (np. staging, production).</li> </ol>"},{"location":"07-ml-tools/#podsumowanie","title":"Podsumowanie","text":"<p>MLflow usprawnia ca\u0142y proces od eksperymentu po produkcj\u0119.</p> <p>Zapewnia lepsz\u0105 reprodukowalno\u015b\u0107, skalowalno\u015b\u0107 i kontrol\u0119 nad modelami.</p> <p>Idealne narz\u0119dzie dla data scientist\u00f3w i in\u017cynier\u00f3w ML do organizacji pracy i produkcyjnego wdra\u017cania modeli.</p>"},{"location":"07-ml-tools/#kwantyzacja-sieci-neuronowych","title":"Kwantyzacja sieci neuronowych","text":"<p>Kwantyzacja to proces redukcji precyzji liczb zmiennoprzecinkowych (np. float32) do mniejszych format\u00f3w, takich jak int8. Stosuje si\u0119 j\u0105 w celu:</p> <ul> <li>zmniejszenia rozmiaru modelu,</li> <li>przyspieszenia dzia\u0142ania (inference),</li> <li>umo\u017cliwienia uruchamiania modeli na s\u0142abszym sprz\u0119cie (CPU, edge, embedded).</li> </ul> <p>Kiedy si\u0119 j\u0105 stosuje?</p> <ul> <li>Po treningu (post-training quantization) \u2013 najprostszy wariant.</li> <li>W trakcie treningu (quantization-aware training) \u2013 dok\u0142adniejsze wyniki.</li> <li>Po treningu z fine-tuningiem \u2013 opcja po\u015brednia, z niewielk\u0105 ilo\u015bci\u0105 dodatkowych danych.</li> </ul> <p>Metody:</p> <ul> <li>K-Means Quantization - wagi z danej warstwy s\u0105 klastrowane algorytmem k-means, a nast\u0119pnie zast\u0119powane przez centroid.</li> <li>Linear Quantization - wagi z danej warstwy s\u0105 skalowane liniowo i przybli\u017cane do najbli\u017cszych element\u00f3w w skwantyzowanej przestrzeni.</li> </ul> <p></p> <p></p> <p></p> <p>Zalety kwantyzacji:</p> <ul> <li>znacznie mniejszy rozmiar modeli,</li> <li>szybsze przetwarzanie na CPU / edge devices (smartfony, PC, Raspberry Pi, specjalne uk\u0142ady jak Google Coral),</li> <li>ni\u017csze zu\u017cycie energii,</li> <li>mo\u017cliwo\u015b\u0107 dzia\u0142ania na urz\u0105dzeniach mobilnych,</li> <li>prywatno\u015b\u0107 i bezpiecze\u0144stwo - dzi\u0119ki mniejszym rozmiarom, dane mog\u0105 by\u0107 przetwarzane lokalnie, bez potrzeby zewn\u0105trznych dostawc\u00f3w.</li> </ul> <p>Wady:</p> <ul> <li>mo\u017cliwa utrata dok\u0142adno\u015bci (jednak z zaawansowanymi technikami optymalizacyjnymi mog\u0105 to by\u0107 zmiany rz\u0119du kilka punkt\u00f3w procentowych w accuracy),</li> <li>bardziej skomplikowane debugowanie,</li> <li>czasem konieczny fine-tuning.</li> </ul>"},{"location":"07-ml-tools/#ollama-lokalne-llm","title":"Ollama: lokalne LLM","text":"<p>Ollama to narz\u0119dzie CLI i runtime do uruchamiania du\u017cych modeli j\u0119zykowych (LLM) lokalnie \u2013 na lokalnym komputerze (CPU/GPU). Obs\u0142uguje zoptymalizowane modele w formacie GGUF, kt\u00f3re pozwalaj\u0105 na szybkie i efektywne wnioskowanie bez potrzeby u\u017cycia chmury.</p> <ul> <li>U\u0142atwia pobieranie, uruchamianie i interakcj\u0119 z modelami (np. Mistral, LLaMA, Gemma, Phi-3).</li> <li>Nie wymaga dodatkowego kodu czy framework\u00f3w.</li> <li>Dzia\u0142a na macOS, Windows, Linux.</li> <li>Pe\u0142ni te\u017c funkcj\u0119 serwera API i udost\u0119pnia udost\u0119pnia REST API do wysy\u0142ania zapyta\u0144 - umo\u017cliwia integracj\u0119 z frontendem, aplikacjami, bibliotekami takimi jak LangChain, LangGraph.</li> </ul> <p>Instrukcja instalacji: https://ollama.com/download/linux</p> <p>Dost\u0119pne modele: https://ollama.com/search</p> <p>LLM mo\u017cna odpali\u0107 lokalnie komend\u0105:</p> <pre><code>ollama run llama3.2:latest\n</code></pre>"},{"location":"07-ml-tools/homework/","title":"Praca domowa","text":"<p>\ud83d\udccc Zadanie 7 \u2013 Prezentacja i podsumowanie projektu</p> <p>\ud83c\udfaf Cel zadania:</p> <p>Celem tego zadania jest przygotowanie prezentacji podsumowuj\u0105cej projekt. Zesp\u00f3\u0142 ma zaprezentowa\u0107 ca\u0142\u0105 aplikacj\u0119 oraz procesy, kt\u00f3re zosta\u0142y zaimplementowane w trakcie pracy nad projektem. Prezentacja ma obejmowa\u0107 om\u00f3wienie architektury rozwi\u0105zania, u\u017cytych narz\u0119dzi oraz problem\u00f3w napotkanych w trakcie realizacji, a tak\u017ce podanie dalszych krok\u00f3w, kt\u00f3re by\u0142yby realizowane w projekcie, gdyby czas pozwala\u0142 na jego kontynuacj\u0119.</p> <p>\ud83d\udcdd Elementy do przygotowania:</p> <ul> <li> <p>Prezentacja:</p> <ul> <li>Om\u00f3wienie architektury rozwi\u0105zania \u2013 jak dzia\u0142a aplikacja, jakie komponenty s\u0105 u\u017cywane, jak komunikuj\u0105 si\u0119 ze sob\u0105.</li> <li>Przedstawienie u\u017cytych narz\u0119dzi i technologii \u2013 Docker, CI/CD, chmurowe us\u0142ugi, narz\u0119dzia do analizy kodu (pre-commit, pytest), storage, itp.</li> <li>Opis funkcji aplikacji i sposobu jej dzia\u0142ania.</li> <li>Opis tego, co si\u0119 nie uda\u0142o zrobi\u0107 \u2013 wyja\u015bnienie, dlaczego niekt\u00f3re funkcjonalno\u015bci zosta\u0142y pomini\u0119te (np. \"Won't have\") i co mo\u017cna by by\u0142o poprawi\u0107 lub kontynuowa\u0107, gdyby projekt trwa\u0142 d\u0142u\u017cej.</li> <li>Live demo aplikacji \u2013 zaprezentowanie dzia\u0142ania aplikacji na \u017cywo. Je\u017celi generowanie lub przetwarzanie obrazk\u00f3w trwa d\u0142ugo, przygotowanie nagrania wideo z przyspieszeniem czasu oczekiwania.</li> <li>Okre\u015blenie czasu przetwarzania danych przez aplikacj\u0119 (np. \"Czas generowania odpowiedzi wynosi \u015brednio 30 sekund\").</li> <li>Om\u00f3wienie zastosowa\u0144 rozwi\u0105zania \u2013 gdzie mo\u017cna je zastosowa\u0107, jaki problem rozwi\u0105zuje (biznesowy, \u015brodowiskowy, dla rozrywki).</li> <li>Prezentacja powinna mie\u0107 charakter profesjonalny, jakby by\u0142a prezentowana klientowi na zako\u0144czenie projektu. W zwi\u0105zku z tym, ma by\u0107 merytoryczna, ale mo\u017cecie r\u00f3wnie\u017c porusza\u0107 bardziej techniczne aspekty, poniewa\u017c jeste\u015bmy technicznym zespo\u0142em.</li> <li>Oczekiwany czas prezentacji: oko\u0142o 5 minut na zesp\u00f3\u0142.</li> </ul> </li> <li> <p>Pytania i odpowiedzi:</p> <ul> <li>Oczekiwane s\u0105 sensowne pytania od innych zespo\u0142\u00f3w. Je\u015bli kto\u015b zada pytanie zespo\u0142owi podczas prezentacji, zesp\u00f3\u0142 powinien odpowiedzie\u0107 na nie merytorycznie.</li> <li>Za zadanie pytania przyznawany b\u0119dzie 1 punkt dodatkowy (max 1 punkt z 1 prezentacji dla 1 osoby).</li> </ul> </li> </ul> <p>\ud83d\udce4 Elementy do przes\u0142ania:</p> <ul> <li>Plik prezentacji (PDF lub PowerPoint).</li> <li>Nagranie demo aplikacji.</li> <li>Link do dzia\u0142aj\u0105cej aplikacji (w chmurze).</li> </ul> <p>\ud83c\udf99\ufe0f Elementy do pokazania na zaj\u0119ciach:</p> <ul> <li>Prezentacja w formie slajd\u00f3w na \u017cywo.</li> <li>Live demo aplikacji lub nagranie wideo z demonstracj\u0105.</li> <li>Odpowiedzi na pytania zespo\u0142u.</li> </ul>"},{"location":"07-ml-tools/automl/notebook/","title":"AutoML - toy example","text":"In\u00a0[1]: Copied! <pre># docker run -it -p 8888:8888 mfeurer/auto-sklearn:master /bin/bash -c \"mkdir -p /opt/nb &amp;&amp; jupyter notebook --notebook-dir=/opt/nb --ip='0.0.0.0' --port=8888 --no-browser --allow-root\"\n</pre> # docker run -it -p 8888:8888 mfeurer/auto-sklearn:master /bin/bash -c \"mkdir -p /opt/nb &amp;&amp; jupyter notebook --notebook-dir=/opt/nb --ip='0.0.0.0' --port=8888 --no-browser --allow-root\" In\u00a0[2]: Copied! <pre>import sklearn.datasets\nimport sklearn.metrics\n\nimport autosklearn.classification\n\nfrom pprint import pprint\n</pre> import sklearn.datasets import sklearn.metrics  import autosklearn.classification  from pprint import pprint In\u00a0[3]: Copied! <pre># laod data\nX, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n    X, y, random_state=1\n)\n</pre> # laod data X, y = sklearn.datasets.load_breast_cancer(return_X_y=True) X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(     X, y, random_state=1 ) In\u00a0[4]: Copied! <pre># set up automl model\nautoml = autosklearn.classification.AutoSklearnClassifier(\n    time_left_for_this_task=120,\n    per_run_time_limit=30,\n    tmp_folder=\"/tmp/autosklearn_classification_example_tmp\",\n    n_jobs=2\n)\n</pre> # set up automl model automl = autosklearn.classification.AutoSklearnClassifier(     time_left_for_this_task=120,     per_run_time_limit=30,     tmp_folder=\"/tmp/autosklearn_classification_example_tmp\",     n_jobs=2 ) In\u00a0[5]: Copied! <pre># train models\nautoml.fit(X_train, y_train, dataset_name=\"breast_cancer\")\n</pre> # train models automl.fit(X_train, y_train, dataset_name=\"breast_cancer\") Out[5]: <pre>AutoSklearnClassifier(ensemble_class=&lt;class 'autosklearn.ensembles.ensemble_selection.EnsembleSelection'&gt;,\n                      n_jobs=2, per_run_time_limit=30,\n                      time_left_for_this_task=120,\n                      tmp_folder='/tmp/autosklearn_classification_example_tmp')</pre> In\u00a0[6]: Copied! <pre># leaderboard\nprint(automl.leaderboard())\n</pre> # leaderboard print(automl.leaderboard()) <pre>          rank  ensemble_weight                type      cost  duration\nmodel_id                                                               \n7            3             0.02         extra_trees  0.014184  1.192859\n56           2             0.10         extra_trees  0.014184  1.254303\n57           1             0.02   gradient_boosting  0.014184  1.179003\n16           8             0.02   gradient_boosting  0.021277  0.822546\n39           6             0.02         extra_trees  0.021277  1.181524\n43           5             0.06                 mlp  0.021277  0.720480\n44           4             0.08         extra_trees  0.021277  1.192886\n63           7             0.02         extra_trees  0.021277  1.444731\n2            9             0.04       random_forest  0.028369  1.340270\n3           17             0.04                 mlp  0.028369  0.892900\n6           16             0.04                 mlp  0.028369  0.994666\n10          15             0.02       random_forest  0.028369  1.443509\n11          14             0.02       random_forest  0.028369  1.575592\n13          13             0.02   gradient_boosting  0.028369  1.140763\n19          12             0.04         extra_trees  0.028369  2.323289\n22          11             0.02   gradient_boosting  0.028369  0.822245\n33          10             0.06                 lda  0.028369  0.727018\n5           21             0.02       random_forest  0.035461  1.460076\n8           20             0.02       random_forest  0.035461  1.426606\n17          22             0.02   gradient_boosting  0.035461  1.156664\n18          18             0.04       random_forest  0.035461  1.375844\n60          19             0.02         extra_trees  0.035461  1.234626\n9           24             0.02         extra_trees  0.042553  1.365160\n28          23             0.02         extra_trees  0.042553  1.232903\n61          25             0.02                 mlp  0.042553  0.922660\n30          26             0.08         extra_trees  0.049645  1.046803\n42          27             0.02         extra_trees  0.049645  1.123572\n58          28             0.02   gradient_boosting  0.049645  0.951374\n64          29             0.02         extra_trees  0.056738  1.309829\n20          30             0.04  passive_aggressive  0.078014  0.633003\n</pre> In\u00a0[7]: Copied! <pre># the final ensemble\npprint(automl.show_models(), indent=4)\n</pre> # the final ensemble pprint(automl.show_models(), indent=4) <pre>{   2: {   'balancing': Balancing(random_state=1),\n           'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7bead53a3c70&gt;,\n           'cost': 0.028368794326241176,\n           'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7bead51a90a0&gt;,\n           'ensemble_weight': 0.04,\n           'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7bead53a3070&gt;,\n           'model_id': 2,\n           'rank': 1,\n           'sklearn_classifier': RandomForestClassifier(max_features=5, n_estimators=512, n_jobs=1,\n                       random_state=1, warm_start=True)},\n    3: {   'balancing': Balancing(random_state=1),\n           'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7bead53a31c0&gt;,\n           'cost': 0.028368794326241176,\n           'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7bead562dd00&gt;,\n           'ensemble_weight': 0.04,\n           'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7bead53a3190&gt;,\n           'model_id': 3,\n           'rank': 2,\n           'sklearn_classifier': MLPClassifier(activation='tanh', alpha=0.0001363185819149026, beta_1=0.999,\n              beta_2=0.9, early_stopping=True,\n              hidden_layer_sizes=(115, 115, 115),\n              learning_rate_init=0.00018009776276177523, max_iter=32,\n              n_iter_no_change=32, random_state=1, verbose=0, warm_start=True)},\n    5: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n           'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beae06060a0&gt;,\n           'cost': 0.03546099290780147,\n           'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7bead51e56d0&gt;,\n           'ensemble_weight': 0.02,\n           'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7bead5730dc0&gt;,\n           'model_id': 5,\n           'rank': 3,\n           'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=3, min_samples_leaf=2,\n                       n_estimators=512, n_jobs=1, random_state=1,\n                       warm_start=True)},\n    6: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n           'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beae0c4ca60&gt;,\n           'cost': 0.028368794326241176,\n           'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7bead500b460&gt;,\n           'ensemble_weight': 0.04,\n           'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beae06d5a00&gt;,\n           'model_id': 6,\n           'rank': 4,\n           'sklearn_classifier': MLPClassifier(alpha=0.0017940473175767063, beta_1=0.999, beta_2=0.9,\n              early_stopping=True, hidden_layer_sizes=(101, 101),\n              learning_rate_init=0.0004684917334431039, max_iter=32,\n              n_iter_no_change=32, random_state=1, verbose=0, warm_start=True)},\n    7: {   'balancing': Balancing(random_state=1),\n           'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7bead7829b20&gt;,\n           'cost': 0.014184397163120588,\n           'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7bead7935c10&gt;,\n           'ensemble_weight': 0.02,\n           'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7bead7829940&gt;,\n           'model_id': 7,\n           'rank': 5,\n           'sklearn_classifier': ExtraTreesClassifier(max_features=34, min_samples_leaf=3, min_samples_split=11,\n                     n_estimators=512, n_jobs=1, random_state=1,\n                     warm_start=True)},\n    8: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n           'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7bead514d4c0&gt;,\n           'cost': 0.03546099290780147,\n           'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beae06ee1c0&gt;,\n           'ensemble_weight': 0.02,\n           'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7bead51560a0&gt;,\n           'model_id': 8,\n           'rank': 6,\n           'sklearn_classifier': RandomForestClassifier(max_features=2, min_samples_leaf=2, n_estimators=512,\n                       n_jobs=1, random_state=1, warm_start=True)},\n    9: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n           'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beacf0bf5b0&gt;,\n           'cost': 0.04255319148936165,\n           'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beae0c430d0&gt;,\n           'ensemble_weight': 0.02,\n           'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7bead78da460&gt;,\n           'model_id': 9,\n           'rank': 7,\n           'sklearn_classifier': ExtraTreesClassifier(max_features=9, min_samples_split=10, n_estimators=512,\n                     n_jobs=1, random_state=1, warm_start=True)},\n    10: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beacf012940&gt;,\n            'cost': 0.028368794326241176,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7bead797fc10&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beacf012370&gt;,\n            'model_id': 10,\n            'rank': 8,\n            'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=4, min_samples_split=6,\n                       n_estimators=512, n_jobs=1, random_state=1,\n                       warm_start=True)},\n    11: {   'balancing': Balancing(random_state=1),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beacc554d60&gt;,\n            'cost': 0.028368794326241176,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7bead7e55490&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beacc5549a0&gt;,\n            'model_id': 11,\n            'rank': 9,\n            'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=23, min_samples_leaf=7,\n                       n_estimators=512, n_jobs=1, random_state=1,\n                       warm_start=True)},\n    13: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7bead57b2160&gt;,\n            'cost': 0.028368794326241176,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7bead7874c10&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7bead57b2790&gt;,\n            'model_id': 13,\n            'rank': 10,\n            'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=False,\n                               l2_regularization=1.0647401999412075e-10,\n                               learning_rate=0.08291320147381159, max_iter=512,\n                               max_leaf_nodes=39, n_iter_no_change=0,\n                               random_state=1, validation_fraction=None,\n                               warm_start=True)},\n    16: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beac3020250&gt;,\n            'cost': 0.021276595744680882,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beacc672280&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beac30200a0&gt;,\n            'model_id': 16,\n            'rank': 11,\n            'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=True,\n                               l2_regularization=3.387912939529945e-10,\n                               learning_rate=0.30755227194768237, max_iter=128,\n                               max_leaf_nodes=60, min_samples_leaf=39,\n                               n_iter_no_change=18, random_state=1,\n                               validation_fraction=None, warm_start=True)},\n    17: {   'balancing': Balancing(random_state=1),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beabe9f9be0&gt;,\n            'cost': 0.03546099290780147,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beacc5083a0&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beabeadec70&gt;,\n            'model_id': 17,\n            'rank': 12,\n            'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=True,\n                               l2_regularization=0.4635442279519353,\n                               learning_rate=0.09809681787962342, max_iter=512,\n                               max_leaf_nodes=328, min_samples_leaf=2,\n                               n_iter_no_change=2, random_state=1,\n                               validation_fraction=None, warm_start=True)},\n    18: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beabc7291c0&gt;,\n            'cost': 0.03546099290780147,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beac3fda1c0&gt;,\n            'ensemble_weight': 0.04,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beabc76ba60&gt;,\n            'model_id': 18,\n            'rank': 13,\n            'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=3, n_estimators=512,\n                       n_jobs=1, random_state=1, warm_start=True)},\n    19: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beabc6cf5e0&gt;,\n            'cost': 0.028368794326241176,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beabeb34250&gt;,\n            'ensemble_weight': 0.04,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beabc6cf280&gt;,\n            'model_id': 19,\n            'rank': 14,\n            'sklearn_classifier': ExtraTreesClassifier(criterion='entropy', max_features=448, min_samples_leaf=2,\n                     min_samples_split=20, n_estimators=512, n_jobs=1,\n                     random_state=1, warm_start=True)},\n    20: {   'balancing': Balancing(random_state=1),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beabc4e9d00&gt;,\n            'cost': 0.07801418439716312,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beabea51460&gt;,\n            'ensemble_weight': 0.04,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beabc4e9580&gt;,\n            'model_id': 20,\n            'rank': 15,\n            'sklearn_classifier': PassiveAggressiveClassifier(C=0.14268277711454813, max_iter=32, random_state=1,\n                            tol=0.0002600768160857831, warm_start=True)},\n    22: {   'balancing': Balancing(random_state=1),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beabc22afd0&gt;,\n            'cost': 0.028368794326241176,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beabc6f6760&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beabc3b4d90&gt;,\n            'model_id': 22,\n            'rank': 16,\n            'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=True,\n                               l2_regularization=8.057778875694463e-05,\n                               learning_rate=0.09179220974965213, max_iter=256,\n                               max_leaf_nodes=200, n_iter_no_change=18,\n                               random_state=1,\n                               validation_fraction=0.14295295806077554,\n                               warm_start=True)},\n    28: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beabc465c70&gt;,\n            'cost': 0.04255319148936165,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beabc6396a0&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beabc465ee0&gt;,\n            'model_id': 28,\n            'rank': 17,\n            'sklearn_classifier': ExtraTreesClassifier(bootstrap=True, max_features=9, min_samples_leaf=10,\n                     min_samples_split=7, n_estimators=512, n_jobs=1,\n                     random_state=1, warm_start=True)},\n    30: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab9f4a340&gt;,\n            'cost': 0.049645390070921946,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beabc4a5be0&gt;,\n            'ensemble_weight': 0.08,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab9f4a0d0&gt;,\n            'model_id': 30,\n            'rank': 18,\n            'sklearn_classifier': ExtraTreesClassifier(criterion='entropy', max_features=1, min_samples_leaf=10,\n                     min_samples_split=17, n_estimators=512, n_jobs=1,\n                     random_state=1, warm_start=True)},\n    33: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab9d60550&gt;,\n            'cost': 0.028368794326241176,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beabc2bf4c0&gt;,\n            'ensemble_weight': 0.06,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab9d600d0&gt;,\n            'model_id': 33,\n            'rank': 19,\n            'sklearn_classifier': LinearDiscriminantAnalysis(tol=0.028148128759024622)},\n    39: {   'balancing': Balancing(random_state=1),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab9b77670&gt;,\n            'cost': 0.021276595744680882,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beabc452ca0&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab9bc5f70&gt;,\n            'model_id': 39,\n            'rank': 20,\n            'sklearn_classifier': ExtraTreesClassifier(max_features=8, min_samples_leaf=5, min_samples_split=15,\n                     n_estimators=512, n_jobs=1, random_state=1,\n                     warm_start=True)},\n    42: {   'balancing': Balancing(random_state=1),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab9a0f340&gt;,\n            'cost': 0.049645390070921946,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beab9f2c670&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab9a43eb0&gt;,\n            'model_id': 42,\n            'rank': 21,\n            'sklearn_classifier': ExtraTreesClassifier(criterion='entropy', max_features=10, min_samples_leaf=7,\n                     min_samples_split=11, n_estimators=512, n_jobs=1,\n                     random_state=1, warm_start=True)},\n    43: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab778a0d0&gt;,\n            'cost': 0.021276595744680882,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beab9c91400&gt;,\n            'ensemble_weight': 0.06,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab77e6cd0&gt;,\n            'model_id': 43,\n            'rank': 22,\n            'sklearn_classifier': MLPClassifier(alpha=2.881894106251383e-06, beta_1=0.999, beta_2=0.9,\n              early_stopping=True, hidden_layer_sizes=(82,),\n              learning_rate_init=0.04692911071083661, max_iter=32,\n              n_iter_no_change=32, random_state=1, verbose=0, warm_start=True)},\n    44: {   'balancing': Balancing(random_state=1),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab770ffd0&gt;,\n            'cost': 0.021276595744680882,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beab9b24fd0&gt;,\n            'ensemble_weight': 0.08,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab770faf0&gt;,\n            'model_id': 44,\n            'rank': 23,\n            'sklearn_classifier': ExtraTreesClassifier(bootstrap=True, criterion='entropy', max_features=9,\n                     min_samples_leaf=2, min_samples_split=5, n_estimators=512,\n                     n_jobs=1, random_state=1, warm_start=True)},\n    56: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab7526b80&gt;,\n            'cost': 0.014184397163120588,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beab9a28dc0&gt;,\n            'ensemble_weight': 0.1,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab75268b0&gt;,\n            'model_id': 56,\n            'rank': 24,\n            'sklearn_classifier': ExtraTreesClassifier(bootstrap=True, max_features=32, min_samples_leaf=4,\n                     min_samples_split=18, n_estimators=512, n_jobs=1,\n                     random_state=1, warm_start=True)},\n    57: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab7216100&gt;,\n            'cost': 0.014184397163120588,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beab77acd30&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab71f5c40&gt;,\n            'model_id': 57,\n            'rank': 25,\n            'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=True,\n                               l2_regularization=0.005020321718482131,\n                               learning_rate=0.05900711838534716, max_iter=512,\n                               max_leaf_nodes=1337, min_samples_leaf=30,\n                               n_iter_no_change=8, random_state=1,\n                               validation_fraction=None, warm_start=True)},\n    58: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab6ddb0d0&gt;,\n            'cost': 0.049645390070921946,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beab7633280&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab6dffc40&gt;,\n            'model_id': 58,\n            'rank': 26,\n            'sklearn_classifier': HistGradientBoostingClassifier(early_stopping=False,\n                               l2_regularization=6.67252806331209e-10,\n                               learning_rate=0.31966188390105227, max_iter=512,\n                               max_leaf_nodes=3, min_samples_leaf=86,\n                               n_iter_no_change=0, random_state=1,\n                               validation_fraction=None, warm_start=True)},\n    60: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab6b57130&gt;,\n            'cost': 0.03546099290780147,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beab74440a0&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab6b46f70&gt;,\n            'model_id': 60,\n            'rank': 27,\n            'sklearn_classifier': ExtraTreesClassifier(criterion='entropy', max_features=8, min_samples_leaf=2,\n                     min_samples_split=19, n_estimators=512, n_jobs=1,\n                     random_state=1, warm_start=True)},\n    61: {   'balancing': Balancing(random_state=1),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab6b24d90&gt;,\n            'cost': 0.04255319148936165,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beab6e9cd60&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab6b24a00&gt;,\n            'model_id': 61,\n            'rank': 28,\n            'sklearn_classifier': MLPClassifier(alpha=0.019796448518578556, beta_1=0.999, beta_2=0.9,\n              early_stopping=True, hidden_layer_sizes=(53,),\n              learning_rate_init=0.0005207451237850025, max_iter=32,\n              n_iter_no_change=32, random_state=1, verbose=0, warm_start=True)},\n    63: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab6a007f0&gt;,\n            'cost': 0.021276595744680882,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beab6e15910&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab6a003a0&gt;,\n            'model_id': 63,\n            'rank': 29,\n            'sklearn_classifier': ExtraTreesClassifier(bootstrap=True, max_features=82, min_samples_split=18,\n                     n_estimators=512, n_jobs=1, random_state=1,\n                     warm_start=True)},\n    64: {   'balancing': Balancing(random_state=1, strategy='weighting'),\n            'classifier': &lt;autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7beab45e7cd0&gt;,\n            'cost': 0.05673758865248224,\n            'data_preprocessor': &lt;autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7beab6b57d60&gt;,\n            'ensemble_weight': 0.02,\n            'feature_preprocessor': &lt;autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7beab685d940&gt;,\n            'model_id': 64,\n            'rank': 30,\n            'sklearn_classifier': ExtraTreesClassifier(bootstrap=True, max_features=3, min_samples_leaf=11,\n                     min_samples_split=13, n_estimators=512, n_jobs=1,\n                     random_state=1, warm_start=True)}}\n</pre> In\u00a0[8]: Copied! <pre># final score\npredictions = automl.predict(X_test)\nprint(\"Accuracy score:\", sklearn.metrics.accuracy_score(y_test, predictions))\n</pre> # final score predictions = automl.predict(X_test) print(\"Accuracy score:\", sklearn.metrics.accuracy_score(y_test, predictions)) <pre>Accuracy score: 0.951048951048951\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"07-ml-tools/automl/notebook/#automl-toy-example","title":"AutoML - toy example\u00b6","text":""},{"location":"07-ml-tools/mlflow/notebook/","title":"MLflow - toy example","text":"In\u00a0[1]: Copied! <pre>import mlflow\nimport mlflow.sklearn\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport requests\nfrom datetime import datetime\n</pre> import mlflow import mlflow.sklearn from sklearn.linear_model import ElasticNet from sklearn.metrics import mean_squared_error, r2_score import requests from datetime import datetime In\u00a0[2]: Copied! <pre>alpha = 1.0\nX = [[0, 0], [1, 1], [2, 2]]\ny = [0, 1, 2]\n</pre> alpha = 1.0 X = [[0, 0], [1, 1], [2, 2]] y = [0, 1, 2] In\u00a0[3]: Copied! <pre>mlflow.set_tracking_uri(\"http://localhost:5000\")\nmlflow.set_experiment(\"linear_regression_experiment\")\n</pre> mlflow.set_tracking_uri(\"http://localhost:5000\") mlflow.set_experiment(\"linear_regression_experiment\") <pre>2025/06/12 13:24:30 INFO mlflow.tracking.fluent: Experiment with name 'linear_regression_experiment' does not exist. Creating a new experiment.\n</pre> Out[3]: <pre>&lt;Experiment: artifact_location='/home/adrian/projects/2025L-PJATK-SUML/07-ml-tools/mlflow/mlruns/1', creation_time=1749727470732, experiment_id='1', last_update_time=1749727470732, lifecycle_stage='active', name='linear_regression_experiment', tags={}&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[4]: Copied! <pre>with mlflow.start_run() as run:\n    # Log parameters\n    mlflow.log_param(\"alpha\", alpha)\n    mlflow.log_param(\"model_type\", \"ElasticNet\")\n    mlflow.log_param(\"data_source\", \"synthetic_data\")\n    \n    # Train model\n    reg = ElasticNet(alpha=alpha)\n    reg.fit(X, y)\n    \n    # calculate mse and r2 score\n    y_pred = reg.predict(X)\n    mse = mean_squared_error(y, y_pred)\n    r2 = r2_score(y, y_pred)\n    \n    # Log metrics\n    mlflow.log_metric(\"mse\", mse)\n    mlflow.log_metric(\"r2_score\", r2)\n    \n    # Log model\n    mlflow.sklearn.log_model(reg, \"model\")\n    \n    run_id = run.info.run_id\n\nprint(f\"Run ID: {run_id}\")\n</pre> with mlflow.start_run() as run:     # Log parameters     mlflow.log_param(\"alpha\", alpha)     mlflow.log_param(\"model_type\", \"ElasticNet\")     mlflow.log_param(\"data_source\", \"synthetic_data\")          # Train model     reg = ElasticNet(alpha=alpha)     reg.fit(X, y)          # calculate mse and r2 score     y_pred = reg.predict(X)     mse = mean_squared_error(y, y_pred)     r2 = r2_score(y, y_pred)          # Log metrics     mlflow.log_metric(\"mse\", mse)     mlflow.log_metric(\"r2_score\", r2)          # Log model     mlflow.sklearn.log_model(reg, \"model\")          run_id = run.info.run_id  print(f\"Run ID: {run_id}\") <pre>2025/06/12 13:24:32 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n2025/06/12 13:24:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n</pre> <pre>\ud83c\udfc3 View run luminous-doe-45 at: http://localhost:5000/#/experiments/1/runs/75440d5793c9468ab0dd141bd7ec9220\n\ud83e\uddea View experiment at: http://localhost:5000/#/experiments/1\nRun ID: 75440d5793c9468ab0dd141bd7ec9220\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[5]: Copied! <pre>mlflow.register_model(\n    f\"runs:/{run_id}/model\",\n    \"ElasticNetModel\"\n)\n</pre> mlflow.register_model(     f\"runs:/{run_id}/model\",     \"ElasticNetModel\" ) <pre>Successfully registered model 'ElasticNetModel'.\n2025/06/12 13:24:34 WARNING mlflow.tracking._model_registry.fluent: Run with id 75440d5793c9468ab0dd141bd7ec9220 has no artifacts at artifact path 'model', registering model based on models:/m-4c3319e4ff534f99b425829ad8545e93 instead\n2025/06/12 13:24:34 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: ElasticNetModel, version 1\nCreated version '1' of model 'ElasticNetModel'.\n</pre> Out[5]: <pre>&lt;ModelVersion: aliases=[], creation_timestamp=1749727474914, current_stage='None', deployment_job_state=&lt;ModelVersionDeploymentJobState: current_task_name='', job_id='', job_state='DEPLOYMENT_JOB_CONNECTION_STATE_UNSPECIFIED', run_id='', run_state='DEPLOYMENT_JOB_RUN_STATE_UNSPECIFIED'&gt;, description='', last_updated_timestamp=1749727474914, metrics=None, model_id=None, name='ElasticNetModel', params=None, run_id='75440d5793c9468ab0dd141bd7ec9220', run_link='', source='models:/m-4c3319e4ff534f99b425829ad8545e93', status='READY', status_message=None, tags={}, user_id='', version='1'&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[6]: Copied! <pre># run in bash\n# MLFLOW_TRACKING_URI=http://localhost:5000 mlflow models serve -m models:/ElasticNetModel/1 --port 1234 --no-conda\n</pre> # run in bash # MLFLOW_TRACKING_URI=http://localhost:5000 mlflow models serve -m models:/ElasticNetModel/1 --port 1234 --no-conda In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[8]: Copied! <pre>data = {\n    \"instances\": [[3, 3], [10, 10]]\n}\nres = requests.post(\"http://localhost:1234/invocations\", json=data)\nprint(res.json())\n</pre> data = {     \"instances\": [[3, 3], [10, 10]] } res = requests.post(\"http://localhost:1234/invocations\", json=data) print(res.json()) <pre>{'predictions': [1.363638242683178, 2.6363720920743003]}\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"07-ml-tools/mlflow/notebook/#mlflow-toy-example","title":"MLflow - toy example\u00b6","text":""},{"location":"07-ml-tools/mlflow/notebook/#train-model","title":"Train model\u00b6","text":""},{"location":"07-ml-tools/mlflow/notebook/#register-the-model","title":"Register the model\u00b6","text":""},{"location":"07-ml-tools/mlflow/notebook/#serve-the-model","title":"Serve the model\u00b6","text":""},{"location":"07-ml-tools/mlflow/notebook/#call-the-model-via-rest-api","title":"Call the model via REST API\u00b6","text":""},{"location":"08-presentation/","title":"Prezentacje projekt\u00f3w","text":"<p>Rekomendacje i wymagania dotycz\u0105ce prezentacji przedstawione s\u0105 w link.</p>"}]}
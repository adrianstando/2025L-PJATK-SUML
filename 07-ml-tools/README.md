# NarzÄ™dzia ML

## AutoML

AutoML (Automated Machine Learning) to zestaw metod i narzÄ™dzi, ktÃ³rych celem jest zautomatyzowanie procesu tworzenia modeli ML â€“ od przygotowania danych po dobÃ³r algorytmu i jego strojenie. DziÄ™ki temu osoby bez gÅ‚Ä™bokiej wiedzy o ML mogÄ… tworzyÄ‡ modele o wysokiej jakoÅ›ci.

### Typowy cykl AutoML obejmuje:

| Etap  | Co siÄ™ dzieje? |
| ----- | -------------- |
| Przygotowanie danych  | Czyszczenie, brakujÄ…ce dane, skalowanie, kodowanie kategorii |
| InÅ¼ynieria cech   | Tworzenie nowych zmiennych (np. daty, interakcje) |
| DobÃ³r modelu | WybÃ³r algorytmu ML (np. Random Forest, SVM, itp.) |
| Strojenie parametrÃ³w | Optymalizacja hiperparametrÃ³w, walidacja krzyÅ¼owa (cross-validation) |
| Ensemble | ÅÄ…czenie wielu modeli |
| Ewaluacja i selekcja | Testowanie i wybÃ³r najlepszego modelu |

### AutoSklearn

AutoSklearn to otwartoÅºrÃ³dÅ‚owy framework AutoML w Pythonie oparty na scikit-learn.

Cechy:

* dziaÅ‚a na danych tabelarycznych (np. klasyfikacja, regresja),
* automatyzuje przygotowanie danych, wybÃ³r modelu, tuning,
* uÅ¼ywa technik:
    * Bayesian Optimization
    * Meta-learning
    * Ensemble construction

**Meta-learning**

Meta-learning pochodzi od *metadata learning*. AutoSklearn korzysta z bazy wiedzy o poprzednich zadaniach ML - posiada swÃ³j wÅ‚asny "meta-model", ktÃ³ry wspomaga i skraca czas znalezienia optymalnego modelu.

ğŸ§  "JeÅ›li wczeÅ›niej dane podobne do Twoich najlepiej dziaÅ‚aÅ‚y z XGBoostem, to zacznÄ™ od XGBoosta."

**Bayesian Optimization**

Zamiast Å›lepo testowaÄ‡ wszystkie kombinacje hiperparametrÃ³w, AutoSklearn inteligentnie wybiera kolejne eksperymenty, budujÄ…c model przewidujÄ…cy, ktÃ³re ustawienia mogÄ… dziaÅ‚aÄ‡ najlepiej.

ğŸ§  "W poprzednich krokach najlepiej dziaÅ‚aÅ‚y modele z n_estimators ~100 i max_depth ~5. SprawdÅºmy w tej okolicy!"

**Ensembling**

AutoSklearn automatycznie Å‚Ä…czy najlepsze modele w ensemble, co poprawia dokÅ‚adnoÅ›Ä‡ i stabilnoÅ›Ä‡.

### Zalety i wady

Zalety AutoML:

* OszczÄ™dnoÅ›Ä‡ czasu â€“ bez rÄ™cznego strojenia modeli
* NiÅ¼szy prÃ³g wejÅ›cia â€“ dziaÅ‚a nawet dla poczÄ…tkujÄ…cych
* Automatyczna optymalizacja â€“ model czÄ™sto dorÃ³wnuje rÄ™cznie budowanym

Wady AutoML:

* Czarna skrzynka â€“ nie wiadomo, co dokÅ‚adnie robi model (problem interpretowalnoÅ›ci)
* Ograniczona kontrola â€“ trudniej dopasowaÄ‡ model do specyficznego problemu
* WydajnoÅ›Ä‡ i koszt â€“ potrafi byÄ‡ wolne i zasoboÅ¼erne
* Nie zastÄ™puje wiedzy domenowej â€“ trzeba wiedzieÄ‡, co znaczÄ… dane

## MLflow

MLflow to open-source'owa platforma do zarzÄ…dzania caÅ‚ym cyklem Å¼ycia modeli ML.
Jest ona jÄ™zyk agnostyczna â€” wspiera Python, R, Java, REST API.
UÅ‚atwia Å›ledzenie eksperymentÃ³w, pakowanie modeli, wdraÅ¼anie i monitorowanie.

### Uruchomienie MLflow:

```bash
pip install mlflow
mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri sqlite:///07-ml-tools/mlflow/mlflow.db --default-artifact-root 07-ml-tools/mlflow/mlruns
```

### GÅ‚Ã³wne etapty cyklu Å¼ycia modelu ML

1. Zbieranie i przygotowanie danych
2. Trenowanie modelu
3. Åšledzenie eksperymentÃ³w (parametry, metryki, wyniki)
4. Pakowanie modeli do Å‚atwego wdroÅ¼enia (np. konetener)
5. WdraÅ¼anie modeli do produkcji
6. Monitorowanie modeli w dziaÅ‚aniu (np. spadek jakoÅ›ci)

### Kluczowe komponenty MLflow

1. MLflow Tracking: logowanie i porÃ³wnywanie eksperymentÃ³w (parametry, metryki, artefakty, wersje kodu).
2. MLflow Projects: standaryzowany sposÃ³b pakowania kodu do powtarzalnych eksperymentÃ³w.
3. MLflow Models: format pakowania modeli kompatybilny z rÃ³Å¼nymi platformami (TensorFlow, Spark, Scikit-learn).
4. MLflow Model Registry: centralne repozytorium modeli, wersjonowanie, kontrola etapÃ³w (np. staging, production).

### Podsumowanie

MLflow usprawnia caÅ‚y proces od eksperymentu po produkcjÄ™.

Zapewnia lepszÄ… reprodukowalnoÅ›Ä‡, skalowalnoÅ›Ä‡ i kontrolÄ™ nad modelami.

Idealne narzÄ™dzie dla data scientistÃ³w i inÅ¼ynierÃ³w ML do organizacji pracy i produkcyjnego wdraÅ¼ania modeli.

## Kwantyzacja sieci neuronowych

Kwantyzacja to proces redukcji precyzji liczb zmiennoprzecinkowych (np. float32) do mniejszych formatÃ³w, takich jak int8. Stosuje siÄ™ jÄ… w celu:

* zmniejszenia rozmiaru modelu,
* przyspieszenia dziaÅ‚ania (inference),
* umoÅ¼liwienia uruchamiania modeli na sÅ‚abszym sprzÄ™cie (CPU, edge, embedded).

Kiedy siÄ™ jÄ… stosuje?

* Po treningu (post-training quantization) â€“ najprostszy wariant.
* W trakcie treningu (quantization-aware training) â€“ dokÅ‚adniejsze wyniki.
* Po treningu z fine-tuningiem â€“ opcja poÅ›rednia, z niewielkÄ… iloÅ›ciÄ… dodatkowych danych.

Metody:

* K-Means Quantization - wagi z danej warstwy sÄ… klastrowane algorytmem k-means, a nastÄ™pnie zastÄ™powane przez centroid.
* Linear Quantization - wagi z danej warstwy sÄ… skalowane liniowo i przybliÅ¼ane do najbliÅ¼szych elementÃ³w w skwantyzowanej przestrzeni.

![K-Means Quantization (ÅºrÃ³dÅ‚o: https://www.geeksforgeeks.org/what-is-ci-cd/)](./img/k-means-quantization.png)

![Linear quantization (ÅºrÃ³dÅ‚o: https://www.geeksforgeeks.org/what-is-ci-cd/)](./img/linear-quantization-1.png)

![Linear quantization (ÅºrÃ³dÅ‚o: https://www.geeksforgeeks.org/what-is-ci-cd/)](./img/linear-quantization-2.png)

Zalety kwantyzacji:

* znacznie mniejszy rozmiar modeli,
* szybsze przetwarzanie na CPU / edge devices (smartfony, PC, Raspberry Pi, specjalne ukÅ‚ady jak Google Coral),
* niÅ¼sze zuÅ¼ycie energii,
* moÅ¼liwoÅ›Ä‡ dziaÅ‚ania na urzÄ…dzeniach mobilnych,
* prywatnoÅ›Ä‡ i bezpieczeÅ„stwo - dziÄ™ki mniejszym rozmiarom, dane mogÄ… byÄ‡ przetwarzane lokalnie, bez potrzeby zewnÄ…trznych dostawcÃ³w.

Wady:

* moÅ¼liwa utrata dokÅ‚adnoÅ›ci (jednak z zaawansowanymi technikami optymalizacyjnymi mogÄ… to byÄ‡ zmiany rzÄ™du kilka punktÃ³w procentowych w accuracy),
* bardziej skomplikowane debugowanie,
* czasem konieczny fine-tuning.

## Ollama: lokalne LLM

Ollama to narzÄ™dzie CLI i runtime do uruchamiania duÅ¼ych modeli jÄ™zykowych (LLM) lokalnie â€“ na lokalnym komputerze (CPU/GPU). ObsÅ‚uguje zoptymalizowane modele w formacie GGUF, ktÃ³re pozwalajÄ… na szybkie i efektywne wnioskowanie bez potrzeby uÅ¼ycia chmury.

* UÅ‚atwia pobieranie, uruchamianie i interakcjÄ™ z modelami (np. Mistral, LLaMA, Gemma, Phi-3).
* Nie wymaga dodatkowego kodu czy frameworkÃ³w.
* DziaÅ‚a na macOS, Windows, Linux.
* PeÅ‚ni teÅ¼ funkcjÄ™ serwera API i udostÄ™pnia udostÄ™pnia REST API do wysyÅ‚ania zapytaÅ„ - umoÅ¼liwia integracjÄ™ z frontendem, aplikacjami, bibliotekami takimi jak LangChain, LangGraph.

Instrukcja instalacji: [https://ollama.com/download/linux](https://ollama.com/download/linux)

DostÄ™pne modele: [https://ollama.com/search](https://ollama.com/search)

LLM moÅ¼na odpaliÄ‡ lokalnie komendÄ…:

```bash
ollama run llama3.2:latest
```
